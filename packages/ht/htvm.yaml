homepage: https://github.com/grwlf/htvm
changelog-type: markdown
hash: b7c10fc5c8645a7fdab744abd1aeb1af9130db6e8cfa12009116061108fcbf01
test-bench-deps:
  htvm: -any
  bytestring: -any
  base: ! '>=4.7 && <5'
  text: -any
  containers: -any
  tasty-quickcheck: -any
  tasty-hunit: -any
  temporary: -any
  tasty: -any
  recursion-schemes: -any
  QuickCheck: -any
  directory: -any
maintainer: grrwlf@gmail.com
synopsis: Bindings for TVM machine learning framework
changelog: ! '# Revision history for htvm


  ## 0.1.0.0  -- YYYY-mm-dd


  * First version. Released on an unsuspecting world.

'
basic-deps:
  bytestring: -any
  base: ! '>=4.11 && <4.12'
  deriving-compat: -any
  unordered-containers: -any
  text: -any
  filepath: -any
  process: -any
  array: -any
  containers: -any
  mtl: -any
  pretty-show: -any
  temporary: -any
  recursion-schemes: -any
  Earley: -any
  directory: -any
all-versions:
- '0.1.0.0'
- '0.1.1'
- '0.1.2'
author: Sergey Mironov
latest: '0.1.2'
description-type: markdown
description: ! "HTVM\n====\n\nHTVM is a library which provides Haskell runtime and
  experimental frontend for\n[TVM](https://tvm.ai/about) the Machine Learning framework.\n\n**Both
  HTVM and TVM are under development. While TVM is somewhat stable, we\ndon't recommend
  to use HTVM in applications currently**\n\n**[GitHub repository](https://github.com/grwlf/htvm)
  may contain newer version of HTVM**\n\nTVM in a nutshell\n-----------------\n\nTVM
  framework extends [Halide](https://halide-lang.org) principles to Machine\nLearning
  domain. It offers (a) EDSLs for defining and hand-optimizing ML models\n(b) export/import
  facilities for translating models from other frameworks such\nas TensorFlow and
  (c) compiler to binary code for a variety of supported\nplatforms, including LLVM
  (x86, arm), CUDA, OpenCL, Vulcan, ROCm, FPGAs and even\nWebAssembly (note: level
  of support may vary). DSLs for C++ and Python are best\nsupported and also there
  are some support for Java, Go and Rust languages.\n\n[Watch Halide introduction
  video](https://youtu.be/3uiEyEKji0M)\n\n[Read more on TVM site](https://tvm.ai/about)\n\nOriginally,
  TVM aimed at increasing speed of model's inference by providing a\nrich set of optimizing
  primitives called\n['schedules'](https://docs.tvm.ai/tutorials/language/schedule_primitives.html#sphx-glr-tutorials-language-schedule-primitives-py)).\nAt
  the same time it had little support for training models. Recently,\ntraining-related
  proposals were\n[added](https://sea-region.github.com/dmlc/tvm/issues/1996).\n\nTVM
  aims at compiling ML models in highly optimized binary code.\n\nImportant parts
  of TVM are:\n  * `tvm` is a core library providing `compute` interface.\n  * `topi`
  is a tensor operations collection. Most of the middle-layer\n    primitives such
  as `matmul`, `conv2d` and `softmax` are defined there.\n  * `relay` is a high-level
  library written in Python, providing\n    functional-style interface and its own
  typechecker. Currently, relay is\n    under active development and beyond the scope
  of HTVM.\n  * `nnvm` is another high-level wrapper in Python, now deprecated in
  favor of\n    `relay`.\n\nFeatures and goals\n------------------\n\nIn HTVM we are
  going to provide:\n\n 1. C Runtime, which makes it possible to run TVM models from
  Haskell.\n 2. Experimental EDSL for building TVM programs in Haskell.\n\nCombined
  TVM/HTVM-stack features are:\n\n### FFI\n\n  * Not many dependencies: TVM is much
  easier to build than other frameworks (hi\n    TensorFlow). Models are compiled
  to binary code, no interpreters required.\n  * Performance: HTVM uses TVM, which
  is designed with performace in mind.\n  * Simplicity of code.\n\n### EDSL\n\n  *
  Experimental status\n  * Simplicity again. Pure ADT-based design.\n  * Not much
  type-safety yet. Expect errors in runtime. Typechecker may be\n    implemented in
  future.\n\nInstall\n-------\n\n### Installing dependencies\n\n1. Make sure you have
  `g++` and `llvm` installed.\n\n2. Build tvm from development repository located
  at\n   https://github.com/grwlf/tvm, branch autodiff\n\n   ```\n   $ git clone https://github.com/grwlf/tvm\n
  \  $ cd tvm\n   $ git checkout origin/autodiff\n   .. follow up with the tvm build
  procedure\n   ```\n\n### Building HTVM\n\nWe use development environment specified
  in [Nix](https://nixos.org/nix)\nlanguage. In order to open it, please install the\n[Nix
  package manager](https://nixos.org/nix/download.html).\nHaving Nix manager and `NIX_PATH`
  set, enter the environment, by running Nix\ndevelopment shell from the project's
  root folder:\n\n    $ nix-shell\n\nIt should get all the Haskell dependencies upon
  the first run.  Alternatively,\nit should be possible to run Haskell distributions
  like [Haskell\nPlatform](https://www.haskell.org/platform/).\n\nAfter nix-shell
  or Haskell distibution is ready, run `cabal` to build the\nproject.\n\n    $ cabal
  configure --enable-tests\n    $ cabal build\n\nTo run tests, execute the test suite.
  At this point you will need `g++`, `clang`\nand `tvm` of the correct version (see
  above).\n\n    $ cabal test\n\nTo enter the interactive shell, type\n\n    $ cabal
  repl htvm\n    *HTVM.EDSL.Types> :lo Demo\n\nUsage examples may be found in [Tests](./test/Main.hs)
  and (possibly outdated)\n[Demo](./src/Demo.hs).\n\nTODO: Update demo, write more
  examples\n\nDesign notes\n------------\n\n### TVM C Runtime\n\nFFI for TVM C Runtime
  library is a Haskell package, linked to\n`libtvm_runtime.so`. This library contains
  functionality, required to load and\nrun ML code produced by TVM.\n\n 1. The module
  provide wrappers to `c_runtime_api.h` functions.\n 2. `TVMArray` is the main type
  describing Tensors in TVM. It is represented as\n    ForeignPtr to internal representation
  and a set of accessor functions.\n 3. Currently, HTVM can marshal data from Haskell
  lists. Support for\n    `Data.Array` is planned.\n 4. No backends besides LLVM are
  tested. Adding them should not be hard and is\n    on the TODO list.\n\n### TVM
  Haskell EDSL\n\nEDSL has a proof-of-concept status. It may be used to declare ML
  models in\nHaskell, convert them to TVM IR and finally compile.  Later, compiled
  model may be\nloaded and run with Haskell FFI or with any other runtime supported
  by TVM.\n\nContrary to usual practices, we don't manipulate TVM IR by calling TVM
  functions\ninternally. Instead, we build AST in Haskell and print it to C++ program.
  After\nthat we compile the program with common instruments. This approach has its
  pros and\ncons, which are described below.\n\n 1. `HTVM.EDSL.Types` module defines
  AST types which loosely corresponds to\n    `Stmt` and `Expr` class hierarchies
  of TVM.\n 2. `HTVM.EDSL.Monad` provides monadic interface to AST builders. We favored\n
  \   simplicity over type-safety. We belive that overuse of Haskell type system\n
  \   ruined many good libraries. The interface relies on simple ADTs whenever\n    possible.\n
  3. `HTVM.EDSL.Print` contain functions which print AST to C++ program of Model\n
  \   Generator.\n 4. `HTVM.EDSL.Build` provides instruments to compile and run the
  model\n    generator by executing `g++` and `clang` compilers:\n    * The Model
  Generator program builds TVM IR and generates LLVM assembly.\n      In HTVM, we
  support LLVM target, but more targets may be added later.\n    * We execute `clang`
  to compile LLVM into x86 '.so' library. Resulting\n      library may be loaded and
  executed by the Runtime code.\n\nThe whole data transformation pipeline goes as
  follows:\n\n```\n\nMonadic    --> AST --> C++ --> Model --> LLVM --> Model --> Runtime
  FFI\nInterface   .       .       .  Gen    .  asm   .  Library\n            .       .
  \      .         .        .\n            .     Print     .       Print      .\n
  \          Run    C++      g++               clang\n\n```\n\nKnown disadvantages
  of C++ printing approach are:\n- **Compilation speed is limited by the speed of
  `g++`, which is slow.** Gcc is\n  used to compile C++ to binary which may take as
  long as 5 seconds. Little may\n  be done about that without changing approaches.
  One possible way to overcome\n  this limitation would be to provide direct FFI to
  TVM IR like\n  [Halide-hs](https://github.com/cchalmers/halide-hs) does for Halide.\n
  \ Unfortunately, this approach has its own downsides:\n  * Low-level IR API is not
  as stable as its high-level counterpart\n  * TVM is in its early stages and sometimes
  crashes. FFI to IR would provide no\n    isolation from this.\n- **Calling construction-time
  procedures of TVM is non-trivial.** This is a\n  consequence of previous limitation.
  For example, TVM may calculate Tensor\n  shape in runtime and use it immediately
  to define new Tensors. In order to\n  that in Haskell we would need to compile and
  run C++ program which is possible\n  by slow. We try to avoid calling construction-time
  procedures.\n- **User may face weird C++ errors**. TVM is quite a low-level library
  which\n  offers little type-checking, so user may write bad programs easily. Other
  high\n  level TVM wrappers like Relay in Python, does provide their own typecheckers\n
  \ to catch errors earlier. HTVM offers no typechecker currently but it is\n  certainly
  possible to write one. Contributions are welcome!\n\nThe pros of this approach are:\n-
  C++ printer is implemented in less than 300 lines of code. Easy to maintain.\n-
  Easy to port to another TVM dialect such as Relay.\n- Isolation from TVM crashes.
  Memory problems of TVM IR will be translated to error\n  messages in Haskell.\n\nFuture
  plans\n------------\n\n * We aim at supporting basic `import tvm` and `import topi`
  functionality.\n * Support for Scheduling is minimal, but should be enhanced in
  future.\n * Support for TOPI is minimal, but should be enhanced in future.\n * No
  targets besides LLVM are supported. Adding them should be as simple as\n   adding
  them to C++ DSL.\n * We plan to support [Tensor-Level AD](https://sea-region.github.com/dmlc/tvm/issues/1996)\n
  * Adding support for [Relay](https://github.com/dmlc/tvm/issues/1673) is also\n
  \  possible but may require some efforts like writing Python printer.\n\n"
license-name: GPL-3
