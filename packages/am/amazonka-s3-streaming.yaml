homepage: https://github.com/Axman6/amazonka-s3-streaming#readme
changelog-type: markdown
hash: 18780926e9158faf085e3a0c549945a3489f2f44b65ef9beab7351dec855093a
test-bench-deps: {}
maintainer: Alex.Mason@data61.csiro.au
synopsis: Provides conduits to upload data to S3 using the Multipart API
changelog: "# Changelog - amazonka-s3-streaming\n\n## 0.2.0.5\n- Fix compatibility
  with \n\n## 0.2.0.4\n- Make building s3upload executable optional\n\n## 0.2.0.3\n
  * Make all library generated messages use Debug level not Info\n\n## 0.2.0.2\n *
  Update to mmorph < 1.2\n\n## 0.2.0.1\n * Fixed a bug with the printf format strings
  which would lead to a crash (Thanks @JakeOShannessy\n   for reporting).\n\n## 0.2.0.0\n
  * Fixed a potential bug with very large uploads where the chunksize might be too
  small\n   for the limit of 10,000 chunks per upload (#6).\n * Change API to allow
  the user to specify a chunk size for streaming if the user knows\n   more about
  the data than we do.\n * Allow the user to specify how many concurrent threads to
  use for `concurrentUpload` as\n   as well as chunk size (#4).\n * Better specify
  cabal dependency ranges."
basic-deps:
  http-client: ! '>=0.4 && <0.6'
  amazonka: ==1.6.*
  exceptions: ! '>=0.8.2.1 && <0.11'
  bytestring: ! '>=0.10.8.0 && <0.11'
  base: ! '>=4.6 && <5'
  text: -any
  amazonka-s3-streaming: -any
  async: ! '>=2 && <2.3'
  dlist: ! '>=0.8 && <0.9'
  conduit: ==1.3.*
  conduit-extra: -any
  lens: ! '>=4.13 && <5.0'
  amazonka-core: ==1.6.*
  mtl: ! '>=2.2.1 && <2.3'
  mmorph: ! '>=1.0.6 && <1.2'
  amazonka-s3: ==1.6.*
all-versions:
- 0.1.0.0
- 0.1.0.1
- 0.1.0.2
- 0.1.0.3
- 0.1.0.4
- 0.2.0.1
- 0.2.0.2
- 0.2.0.3
- 0.2.0.4
- 0.2.0.5
- 1.0.0.0
author: Alex Mason
latest: 1.0.0.0
description-type: markdown
description: |+
  # amazonka-s3-streaming [![Build Status](https://travis-ci.org/axman6/amazonka-s3-streaming.svg?branch=master)](https://travis-ci.org/axman6/amazonka-s3-streaming)

  Provides a conduit based streaming interface and a concurrent interface to uploading data to S3 using the Multipart API. Also provides method to upload files or bytestrings of known size in parallel.

  The documentation can be found on [Hackage](https://hackage.haskell.org/package/amazonka-s3-streaming).

license-name: BSD-3-Clause
