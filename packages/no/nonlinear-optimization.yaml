homepage: https://github.com/meteficha/nonlinear-optimization
changelog-type: ''
hash: 2b4efff8549256c2a301d28c7c21edddab0d30a16add8b9dbb02343901bf1212
test-bench-deps: {}
maintainer: Felipe A. Lessa <felipe.lessa@gmail.com>
synopsis: Various iterative algorithms for optimization of nonlinear functions.
changelog: ''
basic-deps:
  base: ! '>=3 && <5'
  primitive: ! '>=0.2 && <0.8'
  vector: ! '>=0.5 && <=0.13'
all-versions:
- '0.1'
- '0.2'
- '0.3'
- 0.3.1
- 0.3.2
- 0.3.3
- 0.3.4
- 0.3.5
- 0.3.5.1
- 0.3.5.2
- 0.3.6
- 0.3.7
- 0.3.8
- 0.3.9
- 0.3.10
- 0.3.11
- 0.3.12
- 0.3.12.1
author: Felipe A. Lessa (Haskell code), William W. Hager and Hongchao Zhang (CM_DESCENT
  code).
latest: 0.3.12.1
description-type: haddock
description: |-
  This library implements numerical algorithms to optimize
  nonlinear functions.  Optimization means that we try to find
  a minimum of the function.  Currently all algorithms
  guarantee only that local minima will be found, not global
  ones.

  Almost any continuosly differentiable function @f : R^n -> R@
  may be optimized by this library.  Any further restrictions
  are listed in the modules that need them.

  We use the @vector@ package to represent vectors and
  matrices, although it would be possible to use something like
  @hmatrix@ easily.

  Currently only CG_DESCENT method is implemented.

  If you want to use automatic differentiation to avoid hand-writing gradient functions,
  you can use
  <https://hackage.haskell.org/package/nonlinear-optimization-ad nonlinear-optimization-ad> package or
  <https://hackage.haskell.org/package/nonlinear-optimization-backprop nonlinear-optimization-backprop> package.
license-name: LicenseRef-GPL
