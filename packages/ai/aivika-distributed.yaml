homepage: http://www.aivikasoft.com
changelog-type: markdown
hash: 235306b39d66be837d351d315228a6a4d53526a2e188505d1cd7f5fecb1cd8a5
test-bench-deps: {}
maintainer: David Sorokin <david.sorokin@gmail.com>
synopsis: Parallel distributed discrete event simulation module for the Aivika library
changelog: ! "\nVersion 1.3\n-----\n\n* Fixed the implementation of the fault-tolerant
  mode.\n\nVersion 1.2\n-----\n\n* Fixed a leak of monitor references that are used
  in the fault tolerant mode only.\n\nVersion 1.1.2\n-----\n\n* Updated the main page
  documentation with new recommendations.\n\nVersion 1.1.1\n-----\n\n* Fixed the documentation.\n\nVersion
  1.1\n-----\n\n* Added the time horizon parameter.\n\n* Increased a frequency of
  the global virtual time synchronization.\n\nVersion 1.0\n-----\n\n* Optimized the
  rollback log.\n\n* Increased the default rollback log threshold.\n\n* Returned the
  size threshold for the output message queue.\n\nVersion 0.8\n-----\n\n* No more
  restriction on the number of output messages, which would lead to throttling.\n\nVersion
  0.7.4.2\n-----\n\n* Provided a more precise estimation of speed of simulation.\n\nVersion
  0.7.4.1\n-----\n\n* Updated the estimaton of speed in the description after recent
  changes in the sequential module.\n\nVersion 0.7.4\n-----\n\n* A more graceful termination
  of the time server in case of self-destruction by time-out.\n\nVersion 0.7.3\n-----\n\n*
  Updated so that external software tools could monitor the distributed simulation.\n\nVersion
  0.7.2\n-----\n\n* Improved the stopping of the logical processes in case of shutting
  the cluster down.\n\nVersion 0.7.1\n-----\n\n* Added the time server and logical
  process strategies to shutdown the cluster\n  in case of failure by the specified
  timeout intervals.\n\nVersion 0.7\n-----\n\n* Fixed the use of the LP abbreviation.\n\nVersion
  0.6\n-----\n\n* Using the mwc-random package for generating random numbers by default.\n\nVersion
  0.5.1\n-----\n\n* Added functions expectEvent and expectProcess.\n\n* Added the
  Guard module.\n\nVersion 0.5\n-----\n\n* Added an ability to restore the distributed
  simulation after temporary connection errors.\n\n* Better finalisation of the distributed
  simulation.\n\n* Implemented lazy references.\n\nVersion 0.3\n-----\n\n* Started
  using Samadi's algorithm to synchronize the global virtual time.\n\n* The logical
  processes must call registerDIO to connect to the time server.\n\n* Increased the
  default synchronization time-out and delay.\n\n* Increased the default log size
  threshold.\n"
basic-deps:
  exceptions: ! '>=0.8.0.2'
  mwc-random: ! '>=0.13.0.0'
  stm: ! '>=2.4.2'
  base: ! '>=4.6.0.0 && <6'
  time: ! '>=1.5.0.1'
  distributed-process: ! '>=0.6.1'
  aivika: ! '>=5.3.1'
  aivika-transformers: ! '>=5.3.1'
  array: ! '>=0.3.0.0'
  containers: ! '>=0.4.0.0'
  binary: ! '>=0.6.4.0'
  mtl: ! '>=2.1.1'
  random: ! '>=1.0.0.3'
all-versions:
- '0.1'
- '0.1.1'
- '0.1.3'
- '0.2'
- '0.3'
- '0.3.1'
- '0.5'
- '0.6'
- '0.7'
- '0.7.1.1'
- '0.7.2'
- '0.7.3'
- '0.7.4'
- '0.7.4.1'
- '0.7.4.2'
- '0.8'
- '1.0'
- '1.1'
- '1.1.1'
- '1.1.2'
- '1.2'
- '1.3'
author: David Sorokin
latest: '1.3'
description-type: haddock
description: ! 'This package extends the aivika-transformers [1] package and allows
  running parallel distributed simulations.

  It uses an optimistic strategy known as the Time Warp method. To synchronize the
  global virtual time,

  it uses Samadi''s algorithm.


  Moreover, this package uses the author''s modification that allows recovering the
  distributed

  simulation after temporary connection errors whenever possible. For that, you have
  to enable explicitly

  the recovering mode and enable the monitoring of all logical processes including
  the specialized Time Server process

  as it is shown in one of the test examples included in the distribution.


  With the recovering mode enabled, you can try to build a distributed simulation
  using ordinary computers connected

  via the ordinary net. For example, such a distributed model could even consist of
  computers located in different

  continents of the Earth, where the computers could be connected through the Internet.
  Here the most exciting thing

  is that this is the optimistic distributed simulation with possible rollbacks. It
  is assumed that optimistic methods

  tend to better support the parallelism inherited in the models.


  You can test the distributed simulation using your own laptop, although the package
  is still destined to be

  used with a multi-core computer, or computers connected in the distributed cluster.


  There are additional packages that allow you to run the distributed simulation experiments
  by using

  the Monte-Carlo method. They allow you to save the simulation results in SQL databases
  and then generate a report

  or a set of reports consisting of HTML pages with charts, histograms, links to CSV
  tables, summary statistics etc.

  Please consult the AivikaSoft [3] website for more details.


  Regarding the speed of simulation, the recent rough estimation is as follows. This
  estimation may change from

  version to version. For example, in version 1.0 the rollback log was rewritten,
  which had a significant effect.


  The distributed simulation module is slower up to 8-15 times in comparison with
  the sequential aivika [2] simulation

  library using the equivalent sequential models. The lower estimation in 8 times
  is likely to correspond to complex

  models. The upper estimation in 15 times will probably correspond to quite simple
  event-oriented and process-oriented

  models, where the sequential module can be exceptionally fast. At the same time,
  the message passing between

  the logical processes can dramatically decrease the speed of distributed simulation,
  especially if the messages cause

  rollbacks. Thus, much depends on the distributed model itself.


  When residing the logical processes in a computer with multi-core processor, you
  should follow the next recommendations.

  You should reserve at least 1 core for each logical process, or even reserve 2 cores
  if the logical process extensively

  sends and receives messages. Also you should additionally reserve at least 1 or
  2 cores for each computational node.

  These additional processor cores will be used by the GHC run-time system that includes
  the garbage collector as well.

  The Aivika distributed module creates a huge amount of short-living small objects.
  Therefore, the garbage collector

  needs at least one core to utilize efficiently these objects.


  You should compile your code with options -O2 and -threaded, but then launch it
  with run-time options +RTS -N.


  \[1] <http://hackage.haskell.org/package/aivika-transformers>


  \[2] <http://hackage.haskell.org/package/aivika>


  \[3] <http://www.aivikasoft.com>

'
license-name: BSD3
