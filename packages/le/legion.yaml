homepage: https://github.com/owensmurray/legion#readme
changelog-type: ''
hash: e123757479c5eb0b7910d8b2ae4bf283e543475d631d6f49e1acdb15cda17a6c
test-bench-deps: {}
maintainer: rick@owensmurray.com
synopsis: Distributed, stateful, homogeneous microservice framework.
changelog: ''
basic-deps:
  warp: ! '>=3.2 && <3.3'
  exceptions: ! '>=0.8 && <0.9'
  bytestring: ! '>=0.10.4.0 && <0.11'
  data-dword: ! '>=0.3 && <0.4'
  wai: ! '>=3.2.1.1 && <3.3'
  unix: ! '>=2.7 && <2.8'
  stm: ! '>=2.4.4.1 && <2.5'
  base: ! '>=4.8 && <4.10'
  data-default-class: ! '>=0.0.1 && <0.2'
  time: ! '>=1.6.0.1 && <1.7'
  canteven-http: ! '>=0.1.1.1 && <0.2'
  text: ! '>=1.2.2.0 && <1.3'
  uuid: ! '>=1.3.11 && <1.4'
  network: ! '>=2.6.2.1 && <2.7'
  conduit: ! '>=1.2.4 && <1.3'
  conduit-extra: ! '>=1.1.9 && <1.2'
  containers: ! '>=0.5.5.1 && <0.6'
  binary-conduit: ! '>=1.2.3 && <1.3'
  binary: ! '>=0.7.5 && <0.9'
  Ranged-sets: ! '>=0.3.0 && <0.4'
  monad-logger: ! '>=0.3.17 && <0.4'
  wai-extra: ! '>=3.0.16.1 && <3.1'
  transformers: ! '>=0.3.0.0 && <0.6'
  scotty: ! '>=0.11.0 && <0.12'
  http-types: ! '>=0.9.1 && <0.10'
  aeson: ! '>=0.11.2.0 && <1.1'
  scotty-resource: ! '>=0.1 && <0.3'
all-versions:
- 0.1.0.0
- 0.1.0.1
- 0.2.0.0
- 0.3.0.0
- 0.4.0.0
- 0.5.0.0
- 0.5.0.1
- 0.6.0.0
- 0.7.0.0
- 0.8.0.0
- 0.8.0.1
- 0.8.0.2
- 0.8.0.3
- 0.9.0.0
- 0.9.0.1
- 0.9.0.2
- 0.10.0.0
author: Rick Owens
latest: 0.10.0.0
description-type: markdown
description: ! "# legion\n\n- [Motivation](#motivation)\n    - [Disadvantages of Offloading
  State to the DB](#disadvantages-of-offloading-state-to-the-db)\n    - [Solutions](#solutions)\n-
  [Development Status](#development-status)\n    - [Examples](#examples)\n- [FAQ](#faq)\n
  \   - [How do a \"partition\" in my Legion application and a \"partition\" as a
  subset of records in a distributed database relate to one another?](#how-do-a-partition-in-my-legion-application-and-a-partition-as-a-subset-of-records-in-a-distributed-database-relate-to-one-another)\n
  \   - [Why Haskell?](#why-haskell)\n\n\nLegion is a framework for writing horizontally
  scalable stateful\napplications, particularly microservices.\n\n## Motivation\n\nWriting
  stateful microservices is hard. Typically, the way stateful\nservices are written
  to make them easy is they are written as stateless\nservices that offload state
  to a database, making the database the\nstateful service. This approach has several
  disadvantages, the most\nimportant of which is that it is not always possible in
  principal to\naccomplish what you need.\n\nWhy is it hard to write stateful microservices
  *without* resorting to\nthe DB?  Well, for the same reason it is hard to write a
  distributed\ndatabase in the first place. If you are storing state, you have to\nworry
  about scaling that state by distributing it across a cluster,\nensuring the durability
  by replicating the state, and routing requests\nto the location where the state
  is stored.  You have to worry about\nnodes entering an exiting the cluster, and
  how the state is repaired\nand rebalanced when the cluster topology changes.\n\nWouldn't
  it be nice if you could get all that for free and just focus\non logic of your microservice
  application?\n\n### Disadvantages of Offloading State to the DB\n\n- Data Transfer.\n\n
  \ Transfer costs are only trivial if the size of your state is trivial,\n  and probably
  not even then if you are dealing with frequently accessed\n  objects, or hot spots.
  It is difficult to offload state to the DB in\n  this way if the size of your state
  objects is large.\n\n- Consistency Is Still a Problem.\n\n  Distributed databases
  have gotten good at providing eventual consistency\n  for the semantics of database
  operations, but not for the semantics of\n  your application. Counters are a common
  example of this. Say a field\n  in your DB object represents some kind of counter
  keeping track of the\n  instances of some event or other. Two instances of the event
  happen\n  simultaneously on two different nodes. Node A reads the current value,\n
  \ which is 10. Node B reads the current value, which is 10. Node A adds\n  1, and
  stores the new value as 11. Node B adds 1, and stores the new\n  value as 11. Two
  events happened, but the countered only moved from 10\n  to 11. Your data is now
  inconsistent in relation to your application\n  semantics.\n\n  It is true that
  some databases are starting to provide tools to handle\n  this specific case, and
  others which are similar, but those tools are\n  not typically generalizable, or
  else require locking which may lead\n  to substandard performance, or break A or
  P in the CAP Theorem.\n\n  Another approach some people take to solve this problem
  is to store\n  CRDTs in the database layer (in fact, Legion relies heavily on\n
  \ CRDTs internally). This approach is limited by the support of your\n  database,
  and in any case using CRDTs this way is problematic because\n  the growth of most
  CRDTs is unbounded over time, causing the size of\n  the CRDT to become prohibitively
  large. It is very difficult to do\n  garbage collection on such CRDTs in a hybrid
  system.  One of the most\n  important things Legion does internally is implement
  asynchronous CRDT\n  garbage collection.\n\n### Solutions\n\nThe general philosophy
  that Legion takes to solving the problems of the\napplication/DB hybrid approach
  is not new. Instead of moving data to\nwhere a request is being handled, we move
  the request handling to where\nthe data lives.  What is interesting is the implementation,
  which has the\nfollowing characteristics:\n\n- Request Routing.\n\n  User's of the
  Legion framework supply a request handler which is used\n  to service application
  requests. Requests are routed by the Legion\n  runtime to a node in cluster where
  the data actually resides and the\n  request is executed by the user-provided request
  handler.\n\n- CAP Theorem.\n\n  Legion chooses A and P. In other words, Legion focuses
  on eventual\n  consistency while maintaining availability and fault tolerance.\n\n
  \ This is a little bit trickier than it seems at first glance. You are\n  probably
  used to this option being chosen by distributed databases;\n  in fact choosing A
  and P is basically the whole point of why many\n  distributed databases exist in
  the first place. However, distributed\n  DBs don't offer eventual consistency over
  **arbitrary user-defined\n  operations**. See \"Consistency Is Still a Problem\"
  above. Being\n  eventually consistent with arbitrary semantics is a lot harder than
  with\n  \"last write wins\".\n\n- Meet Semilattices.\n\n  Legion stores incoming
  requests as a set of (user-defined) events*\n  organized into a meet-semilattice,
  with monotonically increasing event\n  ids, and a monotonically increasing set of
  peer acknowledgements for\n  each event. This is important for two reasons. The
  first is because it\n  allows us to rewrite the order of events in the case of conflict
  while\n  maintaining the user-defined event semantics, giving us Strong Eventual\n
  \ Consistency. The second is because, unlike similar schemes layered on\n  top of
  an external database, it allows us to compute a Greatest Lower\n  Bound (or [infimum](https://en.wikipedia.org/wiki/Infimum_and_supremum))\n
  \ for the user-defined partition value (or \"object value\", or \"state\n  value\",
  as some people think of it) encapsulated implicitly in the\n  event semilattice.
  This is the same as saying that it allows us to\n  do garbage collections, because
  it is not possible for a new events\n  to arrive that fall below the infimum. In
  other words, while it is\n  possible for new events to arrive at a given peer out
  of their natural\n  order, it is guaranteed that all events arriving in the future
  must\n  be above the infimum, and that there are no possible events that fall\n
  \ below the infimum which the peer has not already seen. Therefore,\n  we are free
  to collapse and discard all events below the infimum.\n\n  \\* \"Events\" are user-defined
  pieces of code that accept the current\n  partition value as input, and produce
  some kind of response along with\n  a new partition value as output.\n\n- Pure Haskell
  Interface.\n\n  *Comming Soon.*\n\n- Automatic Rebalancing.\n\n  *Comming Soon.*\n\n-
  Replication.\n\n  *Comming Soon.*\n\n## Development Status\n\nThe Legion framework
  is still experimental.\n\n### Examples\n\nCheck out the\n[legion-discovery](https://github.com/owensmurray/legion-discovery)\nproject
  for an example of a stateful web services that takes advantage of\nLegion's ability
  to define your own operations on your data. Take a look at\n[`Network.Legion.Discovery.App`](https://github.com/owensmurray/legion-discovery/blob/master/src/Network/Legion/Discovery/App.hs)\nto
  see where the magic of defining a Legion application happens. The rest\nof the code
  is mostly just standard HTTP-interface-written-in-Haskell,\nand requests sent to
  the Legion runtime.\n\n## FAQ\n\n### How do a \"partition\" in my Legion application
  and a \"partition\" as a subset of records in a distributed database relate to one
  another?\n\nSome people find the term \"partition\" confusing because of the way\nit
  is typically used to describe subsets of a table in distributed\nrelational databases.
  That's ok. The term \"partition\" as used here\nhas a more general meaning, primarily
  because of the more generalized\nnature of Legion as compared to a distributed database.\n\nIn
  Legion, a partition is an abstract unit of state upon which user\nrequests operate.
  It is called a \"partition\" because it \"is separate\nfrom every other partition\",
  meaning that an individual request can only\noperate upon a single partition, and
  can never span multiple partitions.\nFurthermore, Legion can only guarantee consistency
  within the partition\nboundaries.\n\nAnother characteristic of a partition is that
  Legion cannot subdivide\nit.  All of the data on one partition is guaranteed to
  be located on the\nsame physical node. Legion treats partitions as the smallest
  unit of\ndata that can be rebalanced across the cluster.\n\nIn a relational database
  partition, it is sometimes the case that the\ntable can be \"repartitioned\", where
  rows from one partition move to\nthe other. This has no analog in Legion. In Legion,
  a partition is an\natomic unit of data which cannot be subdivided.\n\n\n### Why
  Haskell?\n\nDeveloping correct distributed systems is hard. One reason it is hard
  is\nbecause it comes with a large number of very subtle rules and constraints\nthat
  are not part of the average development process and require highly\nspecialized
  knowledge. Typically this knowledge is entirely unrelated\nto the business problem
  you are trying to solve. Violating any of those\nconstraints can lead to a nightmare
  of data corruption, scalability,\nor availability problems.\n\nMost languages are
  unable to enforce distributed constraints in the type\nsystem, forcing the developer
  to very carefully tread through a proverbial\nmine field. Making an error in even
  one step can have an associated cost\nthat is wildly disproportionate to the subtlety
  of the error.\n\nHaskell on the other hand, has a type system that can be used to
  express\nthese constraints. In addition to implementing the distributed runtime,\nproviding
  a distribution-safe API is a major part of what makes Legion\nawesome. It fences
  off the mines so you can run through the mine field\nfull tilt. If you hit one,
  the cost to your organization is a compile\ntime error, instead of a fundamentally
  broken and failing project.\n\n\n"
license-name: Apache-2.0
