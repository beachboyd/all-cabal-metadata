homepage: https://github.com/tweag/hyperion#readme
changelog-type: ''
hash: 4c99ca22024ce6254982c44cdfd702b616deb81d398652e5ffdd7fbad1edc1a7
test-bench-deps:
  base: -any
  unordered-containers: ! '>=0.2'
  hspec: ! '>=2.2'
  text: -any
  lens: -any
  QuickCheck: ! '>=2.8'
  hyperion: -any
maintainer: nicolas.mattia@tweag.io
synopsis: Reliable performance measurement with robust data export.
changelog: ''
basic-deps:
  exceptions: ! '>=0.8'
  bytestring: ! '>=0.10'
  ansi-wl-pprint: -any
  base: ! '>=4.9 && <5'
  time: ! '>=1.0'
  unordered-containers: ! '>=0.2'
  text: ! '>=1.2'
  clock: ! '>=0.7.2'
  generic-deriving: ! '>=1.11'
  filepath: -any
  process: -any
  random-shuffle: ! '>=0.0.4'
  containers: ! '>=0.5'
  lens: ! '>=4.0'
  mtl: ! '>=2.2'
  statistics: ! '>=0.13'
  hashable: -any
  optparse-applicative: ! '>=0.12'
  random: ! '>=1.1'
  deepseq: ! '>=1.4'
  aeson: ! '>=0.11'
  hyperion: -any
  vector: ! '>=0.11'
  directory: -any
all-versions:
- 0.1.0.0
author: Tweag I/O
latest: 0.1.0.0
description-type: markdown
description: ! "# Hyperion: Haskell-based systems benchmarking\n\n[![Build Status](https://travis-ci.org/tweag/hyperion.svg?branch=master)](https://travis-ci.org/tweag/hyperion)\n\nHyperion
  is a DSL for writing benchmarks to measure and analyze\nsoftware performance. It
  is a lab for future [Criterion][criterion]\nfeatures.\n\n## Getting started\n\n###
  Build\n\nYou can build the [`micro benchmark example`](examples/micro-benchmarks.hs)\nusing
  stack:\n\n``` shell\n$ stack build\n$ stack exec hyperion-micro-benchmark-example\n```\n\n###
  Example usage\n\nThe Hyperion DSL is a backwards compatible extension\nto [Criterion][criterion]'s
  DSL (except for the rarely used `env`\ncombinator, which has a safer type). Here
  is an example:\n\n``` haskell\nbenchmarks :: [Benchmark]\nbenchmarks =\n    [ bench
  \"id\" (nf id ())\n    , series [0,5..20] $ \\n ->\n        bgroup \"pure-functions\"\n
  \         [ bench \"fact\" (nf fact n)\n          , bench \"fib\" (nf fib n)\n          ]\n
  \   , series [1..4] $ \\n ->\n        series [1..n] $ \\k ->\n          bench \"n
  choose k\" $ nf (uncurry choose) (n, k)\n    ]\n\nmain :: IO ()\nmain = defaultMain
  \"hyperion-example-micro-benchmarks\" benchmarks\n```\n\nBy default Hyperion runs
  your benchmarks and pretty prints the\nresults. There are several command-line options
  that you can pass to\nthe executable, like printing the results to a JSON file or
  including\nindividual raw measurements. To see the full set of options run the\nexecutable
  with `--help`:\n\n``` shell\n$ stack exec hyperion-micro-benchmark-example -- --help\nUsage:
  hyperion-micro-benchmark-example ([--pretty] | [-j|--json PATH] |\n                                        [-f|--flat
  PATH]) ([-l|--list] | [--run]\n                                        | [--no-analyze])
  [--raw]\n                                        [--arg KEY:VAL] [NAME...]\n\nAvailable
  options:\n  -h,--help                Show this help text\n  --pretty                 Pretty
  prints the measurements on stdout.\n  -j,--json PATH           Where to write the
  json benchmarks output. Can be a\n                           file name, a directory
  name or '-' for stdout.\n  -f,--flat PATH           Where to write the json benchmarks
  output. Can be a\n                           file name, a directory name or '-'
  for stdout.\n  --version                Display version information\n  -l,--list
  \               List benchmark names\n  --run                    Run benchmarks
  and analyze them (default)\n  --no-analyze             Only run the benchmarks\n
  \ --raw                    Include raw measurement data in report.\n  --arg KEY:VAL
  \           Extra metadata to include in the report, in the\n                           format
  key:value.\n```\n"
license-name: BSD-3-Clause
