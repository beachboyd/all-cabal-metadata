homepage: ''
changelog-type: ''
hash: 8d87f19ff518c8b6cd3ecded5aeffcf438b534d8b4a13cbc6992302773a2b37e
test-bench-deps: {}
maintainer: Gwern <gwern@gwern.net>
synopsis: Archive supplied URLs in WebCite & Internet Archive
changelog: ''
basic-deps:
  bytestring: -any
  base: ==4.*
  curl: -any
  process: -any
  network: -any
  containers: -any
  HTTP: -any
  random: -any
all-versions:
- '0.1'
- '0.2'
- '0.3'
- 0.3.1
- '0.4'
- '0.5'
- 0.5.1
- 0.6.0
- 0.6.1
- 0.6.2
- 0.6.2.1
author: Gwern
latest: 0.6.2.1
description-type: haddock
description: ! '`archiver` is a daemon which will process a specified text file,

  each line of which is a URL, and will (randomly) one by one request that

  the URLs be archived or spidered by <http://www.webcitation.org>,

  <http://www.archive.org>, and <http://www.wikiwix.com> for future reference.

  (One may optionally specify an arbitrary `sh` command like `wget` to download URLs
  locally.)


  Because the interface is a simple text file, this can be combined

  with other scripts; for example, a script using Sqlite to extract

  visited URLs from Firefox, or a program extracting URLs from Pandoc

  documents. (See <http://www.gwern.net/Archiving%20URLs>.)


  For explanation of the derivation of the code in `Network.URL.Archiver`,

  see <http://www.gwern.net/haskell/Wikipedia%20Archive%20Bot>.'
license-name: BSD-3-Clause
