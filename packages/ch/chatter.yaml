homepage: http://github.com/creswick/chatter
changelog-type: markdown
hash: 2e13fdb6acc71cff7fdf0af4a87f3c7c24ec911be0056d871f542c8cf726221f
test-bench-deps:
  cereal: ! '>=0.4.0.1 && <0.5.4.0'
  base: ! '>=4.6 && <=6'
  unordered-containers: -any
  text: -any
  filepath: -any
  parsec: ! '>=3.1.5'
  HUnit: -any
  containers: -any
  quickcheck-instances: -any
  tasty-quickcheck: -any
  tokenize: -any
  chatter: -any
  tasty-hunit: -any
  tasty: -any
  QuickCheck: -any
  tasty-ant-xml: -any
maintainer: creswick@gmail.com
synopsis: A library of simple NLP algorithms.
changelog: ! "= HEAD =\n\n= 0.9.1.0 =\n\n - Updated dependency versions (cereal, specifically)
  to account for ghc-8.0.\n - Switched to using HashSet in VectorSim; gaining ~25%
  performance improvement (Thanks to @dgaw!)\n - Dropped support for ghc-7.6.\n\n=
  0.9.0.0 =\n\n - Regenerated the model files; changes to cereal caused the old\n
  \  Average Perceptron models to throw exceptions on load, and as such,\n   are no
  longer compatible.  The packaged models have been updated.\n\n - Minor updates to
  slack build script.\n\n= 0.8.0.2 =\n\n - Bounded the version of cereal to < 0.5
  because of breaking changes\n   with cereal's default instances.\n\n= 0.8.0.1 =\n\n
  - Added deriving clauses for Class and Feature to support GHC 7.10;\n   based on
  Rick Dzekman's pull request.  Adds an explicit NFData\n   instance for Perceptron.\n\n=
  0.8.0.0 =\n\n - Added a Document type to better store term count indexes\n   (essentially
  TermVectors with integer values).\n\n - Used the Document type for tf_idf; this
  changed the API, and is the\n   reason for the B-level version bump. (also done
  for performance)\n\n - A number of non-breaking performance improvements (e.g.,
  not using foldl in a few places.)\n\n - These changes, and much of 0.7.0.0 are due
  to work by Tristan Ravitch & Jonathan Daugherty.\n\n= 0.7.0.0 =\n - B-level version
  bump because we added test dependency on\n   unordered-containers, which could cause
  downstream issues.\n\n - TermVector: is a newtype now, and has its own arbitrary
  instance.\n\n - TermVector: adds addVectors, zeroVector, negate, and sum\n\n - DefaultMap:
  adds elems, map, and unionWith\n\n - Adds quickcheck properties for all of the above\n\n=
  0.6.0.0 =\n\n - Switched to using Hashmap for the DefaultMap implementation;\n   this
  *may* break compatibility with old binary stores.\n\n= 0.5.2.1 =\n\n - Removed an
  orphan instance for Text.\n\n= 0.5.2.0 =\n\n - Unceremoniously forced the Chunking
  model to do basic Named\n   Entity Recognition, and added a simple NER model based
  on Nothman\n   et al's WikiNER data set -- which is provided under a very\n   permissive
  CC license.\n   (http://schwa.org/projects/resources/wiki/Wikiner)\n\n= 0.5.1.0
  =\n\n - Moved to Tasty from test-framework. I'm treating this as a\n   c-level patch
  because it may have siginficant impact on the\n   transitive dependencies.\n\n=
  0.5.0.2 =\n - removed upper vesion bounds on many dependencies.\n\n= 0.5.0.1 =\n\n
  - Updates for mobx-0.3's changes to use Lazy Text.\n\n= 0.5.0.0 =\n\n - Added chunk,
  chunkText, and chunkStr functions to NLP.Chunk to\n   make it easier to experiment
  with the chunker.\n\n - Added a formatting function to show ChunkedSententences\n
  \  (NLP.Types.Tree.showChunkedSent) that formats chunks in bracket notation, eg:\n\n
  \  > chunkText tgr chk \"The dog jumped over the reluctant cat.\"\n   \"[NP The/DT
  dog/NN] [VP jumped/VBD] [NP over/IN the/DT reluctant/JJ cat/NN] ./.\"\n\n   Notice
  that the features still need some tuning for the chunker.\n\n - Moved the AveragePerceptron
  module into a NLP.ML (Machine Learning) module.\n\n - Added the 'tags-since-dt'
  feature to the AveragePerceptronChunker features\n   and retrained the Conll2000
  models.\n\n= 0.4.0.0 =\n\n - Added phrasal chunking via an Averaged Perceptron Chunker,\n
  \  trained on the Conll 2000 corpus.\n\n - Added a POS tagger trained on the Conll
  2000 corpus, because the\n   Conll chunker relies on that tagset.\n\n - Set the
  Conll 2000 POS tagger to the defaultTagger. Note that\n   the tagset is much smaller
  than the Brown tagset used by the\n   previous defaultTagger.  The Brown tagger
  is still available from\n   NLP.POS.brownTagger.\n\n= 0.3.0.1 =\n\n - Bumped dependency
  on base to >= 4.6 (from >= 4) because of\n   Text.Read.readMaybe.  This drops support
  for HP2012, and requires\n   GHC 7.6 or newer.\n\n - fixed lots of warnings and
  added documentation.\n\n= 0.3.0.0 =\n\n - Changed the Sentence and TaggedSentence
  data types to be actual\n   tree structures with real types at the respective\n
  \  layers. ChunkedSentence and ChunkOr were also added to facilitate\n   phrase
  and clause chunking.\n\n - Added a POS Tag data type for Brown corpus tags, and
  a Chunk type for\n   Chunks as well (in the Brown module, but that's probably not
  the best\n   place, given that the chunks have nothing to do with the actual Brown\n
  \  corpus.)\n\n - Updated the Parsec Examples to use the typed tags/chunks.\n\n
  - Regenerated the defaultTager, it uses the Brown tags now instead of\n   RawTag.\n\n=
  0.2.0.1 =\n\n - I realized immediately after the 0.2.0.0 release that I broke the\n
  \  defaultTagger by adding the protectTerms function to the\n   LiteralTagger.  Things
  broke because (i) there are bugs in that\n   functionality, which uses run-time
  assembled regexes, and (ii) the\n   UnambiguousTagger used in the defaultTagger
  delegates to an instance\n   of the LiteralTagger, which pulled in the (semi-broken)
  protectTerms\n   function.  This has been fixed by replacing the tokenizer when
  the\n   LiteralTagger is used as an UnambiguousTagger -- the later tager\n   doesn't
  need the functionality, and it should never have been used\n   there anyway.\n\n
  - Added a bevy of tests to cover the above fix.\n\n - Added tests (currently breaking)
  that exercise the broken bits of\n   the protectTerms function.\n\n= 0.2.0.0 =\n\n
  - Added support for expressing information extraciton patterns via Parsec.\n - Misc.
  bug fixes.\n"
basic-deps:
  cereal-text: ! '>=0.1 && <0.2'
  cereal: ! '>=0.4.0.1 && <0.5.4.0'
  regex-tdfa-text: -any
  bytestring: ! '>=0.10.0.0'
  MonadRandom: ! '>=0.1.2'
  base: ! '>=4.6 && <=6'
  unordered-containers: -any
  text: ! '>=0.11.3.0'
  fullstop: ! '>=0.1.3.1'
  filepath: ! '>=1.3.0.1'
  parsec: ! '>=3.1.5'
  array: -any
  random-shuffle: ! '>=0.0.4'
  containers: ! '>=0.5.0.0'
  quickcheck-instances: -any
  regex-tdfa: ! '>=1.2.0'
  zlib: ! '>=0.5.4.1'
  tokenize: ! '>=0.2.0'
  chatter: -any
  hashable: -any
  mbox: ! '>=0.3'
  transformers: -any
  deepseq: -any
  QuickCheck: -any
  directory: -any
all-versions:
- 0.0.0.1
- 0.0.0.2
- 0.0.0.3
- 0.0.0.4
- 0.1.0.4
- 0.1.0.5
- 0.2.0.1
- 0.3.0.0
- 0.3.0.1
- 0.4.0.0
- 0.5.0.0
- 0.5.0.1
- 0.5.0.2
- 0.5.1.0
- 0.5.2.0
- 0.5.2.1
- 0.6.0.0
- 0.7.0.0
- 0.8.0.0
- 0.8.0.1
- 0.8.0.2
- 0.9.0.0
- 0.9.1.0
author: Rogan Creswick
latest: 0.9.1.0
description-type: haddock
description: ! 'chatter is a collection of simple Natural Language

  Processing algorithms.


  Chatter supports:


  * Part of speech tagging with Averaged

  Perceptrons. Based on the Python implementation

  by Matthew Honnibal:

  (<http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/>)
  See ''NLP.POS'' for the details of part-of-speech tagging with chatter.


  * Phrasal Chunking (also with an Averaged Perceptron) to identify arbitrary chunks
  based on training data.


  * Document similarity; A cosine-based similarity measure, and TF-IDF calculations,

  are available in the ''NLP.Similarity.VectorSim'' module.


  * Information Extraction patterns via (<http://www.haskell.org/haskellwiki/Parsec/>)
  Parsec


  Chatter comes with models for POS tagging and

  Phrasal Chunking that have been trained on the

  Brown corpus (POS only) and the Conll2000 corpus

  (POS and Chunking)'
license-name: BSD-3-Clause
