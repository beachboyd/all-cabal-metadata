homepage: http://github.com/jfischoff/lagrangian
changelog-type: ''
hash: 18445d941f3ae9377869116bd71e37a28bfbcd25319caee99f6b811d6d34ba83
test-bench-deps:
  test-framework-hunit: ==0.3.*
  test-framework: ==0.8.*
  base: ! '>=4.5 && <5'
  ad: ! '>=4 && <5'
  test-framework-quickcheck2: ==0.3.*
  HUnit: ==1.2.*
  hmatrix: ! '>=0.14 && <0.17'
  nonlinear-optimization: ==0.3.*
  vector: ==0.10.*
maintainer: jonathangfischoff@gmail.com
synopsis: Solve Lagrange multiplier problems
changelog: ''
basic-deps:
  base: ! '>=4.5 && <5'
  ad: ! '>=4 && <5'
  hmatrix: ! '>=0.14 && <0.17'
  nonlinear-optimization: ==0.3.*
  vector: ==0.10.*
all-versions:
- 0.1.0.0
- 0.2.0.0
- 0.2.0.1
- 0.2.0.2
- 0.3.0.0
- 0.3.0.1
- 0.4.0.0
- 0.4.0.1
- 0.5.0.0
- 0.6.0.0
- 0.6.0.1
author: (c) Jonathan Fischoff 2012-2014, (c) Eric Pashman 2014
latest: 0.6.0.1
description-type: haddock
description: ! 'Numerically solve convex Lagrange multiplier problems with conjugate
  gradient descent.


  For some background on the method of Lagrange multipliers checkout the wikipedia
  page

  <http://en.wikipedia.org/wiki/Lagrange_multiplier>


  Here is an example from the Wikipedia page on Lagrange multipliers

  Maximize f(x, y) = x + y, subject to the constraint x^2 + y^2 = 1


  @

  \> maximize 0.00001 (\\[x, y] -> x + y) [(\\[x, y] -> x^2 + y^2) \<=\> 1] 2

  Right ([0.707,0.707], [-0.707])

  @


  For more information look here: <http://en.wikipedia.org/wiki/Lagrange_multiplier#Example_1>


  For example, to find the maximum entropy with the constraint that the probabilities
  sum

  to one.


  @

  \> maximize 0.00001 (negate . sum . map (\\x -> x * log x)) [sum \<=\> 1] 3

  Right ([0.33, 0.33, 0.33], [-0.09])

  @


  The first elements of the result pair are the arguments for the

  objective function at the maximum. The second elements are the Lagrange multipliers.

'
license-name: BSD-3-Clause
