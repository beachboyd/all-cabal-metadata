homepage: ''
changelog-type: text
hash: 2112f1a005c52d74cb34af60ff1ddb358ef7ce6393d1ef0175b7fde562e487c0
test-bench-deps: {}
maintainer: Joseph Barrow <jdb7hw@virginia.edu>
synopsis: A configurable and extensible neural network library
changelog: ! "0.2.0.0 - Introduced selection functions and a training function with
  a\n          stop condition.\n\n0.1.0.1 - Updated the Cabal file to fix failing
  builds\n\n0.1.0.0 - Initial build\n"
basic-deps:
  bytestring: -any
  split: -any
  base: ! '>=4 && <5'
  random-shuffle: ! '>=0.0.4'
  binary: -any
  random: -any
  hmatrix: ==0.16.0.6
all-versions:
- 0.1.0.0
- 0.1.0.1
- 0.2.0.0
author: Brent Baumgartner, Alex Thomas, Harang Ju, Joseph Barrow
latest: 0.2.0.0
description-type: markdown
description: ! "LambdaNet\n=====\n\nLambdaNet is an artificial neural network library
  written in Haskell\nthat abstracts network creation, training, and use as higher
  order\nfunctions. The benefit of this approach is that it provides a framework\nin
  which users can:\n  - quickly iterate through network designs by using different
  functional components\n  - experiment by writing small functional components to
  extend the library\n\nThe library comes with a pre-defined set of functions that
  can be composed\nin many ways to operate on real-world data. These will be enumerated
  later\nin the documentation.\n\n## Installation\n\nLambdaNet can be installed through
  Cabal:\n\n```\ncabal update\ncabal install LambdaNet\n```\n\n## Using LambdaNet\n\nUsing
  LambdaNet to rapidly prototype networks using built-in functions\nrequires only
  a minimal level of Haskell knowledge (although getting\nthe data into the right
  form may be more difficult). However, extending\nthe library may require a more
  in-depth knowledge of Haskell and\nfunctional programming techniques.\n\nYou can
  find a quick example of using the network in `XOR.hs`. Once LambdaNet\nis installed,
  download XOR.hs, and then you can run the file in your REPL to\nsee the results:\n\n```\nrunhaskell
  examples/XOR.hs\n```\n\nThe rest of this section dissects the XOR network in order
  to talk about\nthe design of LambdaNet.\n\n### Training Data\n\nBefore you can train
  or use a network, you must have training data. The\ntraining data is a tuple of
  vectors, the first value being the input\nto the network, and the second value being
  the expected output.\n\nFor the XOR network, the data is easily hardcoded:\n\n```\nlet
  trainData = [\n  (fromList [0.0, 0.0], fromList [0.0]),\n  (fromList [0.0, 1.0],
  fromList [1.0]),\n  (fromList [1.0, 0.0], fromList [1.0]),\n  (fromList [1.0, 1.0],
  fromList [0.0])\n]\n```\n\nHowever, for any non-trivial application the most difficult
  work will be\ngetting the data in this form. Unfortunately, LambdaNet does not currently\nhave
  tools to support data handling.\n\n### Layer Definitions\n\nThe first step in creating
  a network is to define a list of layer\ndefinitions. The type layer definition takes
  a neuron type, a count of\nneurons in the layer, and a connectivity function.\n\nCreating
  the layer definitions for a three-layer XOR network, with\n2 neurons in the input
  layer, 2 hidden neurons, and 1 output neuron\ncan be done as:\n\n```\nlet l = LayerDefinition
  sigmoidNeuron 2 connectFully\nlet l' = LayerDefinition sigmoidNeuron 2 connectFully\nlet
  l'' = LayerDefinition sigmoidNeuron 1 connectFully\n```\n\n#### Neuron Types\n\nA
  neuron is simply defined as an activation function and its derivative,\nand the
  LambdaNet library provides three built-in neuron types:\n  - `sigmoidNeuron` - A
  neuron with a sigmoid activation function\n  - `tanhNeuron` - A neuron with a hyperbolic
  tangent activation function\n  - `recluNeuron` - A neuron with a rectified linear
  activation function\n\nBy passing one of these functions into a LayerDefinition,
  you can\ncreate a layer with neurons of that type.\n\n#### Connectivity\n\nA connectivity
  function is a bit more opaque. Currently, the library\nonly provides `connectFully`,
  a function which creates a fully\nconnected feed-forward network.\n\nSimply, the
  connectivity function takes in the number of neurons in layer l\nand the number
  of neurons in layer l + 1, and returns a boolean matrix\nof integers (0/1) that
  represents the connectivity graph of the layers\n-- a 0 means two neurons are not
  connected and a 1 means they are. The\nstarting weights are defined later.\n\n###
  Creating the Network\n\nThe `createNetwork` function takes in a random transform,
  an entropy\ngenerator, and a list of layer definitions, and returns a network.\n\nFor
  the XOR network, the createNetwork function is:\n\n```\nlet n = createNetwork normals
  (mkStdGen 4) [l, l', l'']\n```\n\nOur source of entropy is the very random: `mkStdGen
  4`, which will\nalways result in the same generator.\n\n#### Random Transforms\n\nThe
  random transform function is a transform that operates on a\nstream of uniformly
  distributed random numbers and returns a stream\nof floating point numbers.\n\nCurrently,
  the two defined distributions are:\n  - `uniforms` - A trivial function that returns
  a stream of uniformly distributed random numbers\n  - `normals` - A slightly less-trivial
  function that uses the Box-Muller transform to create a stream of numbers ~ N(0,
  1)\n\nWork is being done to offer a student t-distribution, which would require\nsupport
  for a chi-squared distribution transformation.\n\n### Training the Network\n\nIn
  order to train a network, you must create a new trainer:\n\n```\nlet t = BackpropTrainer
  (3 :: Float) quadraticCost quadraticCost'\n```\n\nThe BackpropTrainer type takes
  in a learning rate, a cost function, and\nits derivative.\n\nThe actual training
  of the network, the `fit` function uses the trainer, a\nnetwork, and the training
  data, and returns a new, trained network.\nFor the XOR network, this is:\n\n```\nlet
  n' = trainUntilErrorLessThan n t online dat 0.01\n```\n\nLambdaNet provides three
  training methods:\n  - `trainUntil`\n  - `trainUntilErrorLessThan`\n  - `trainNTimes`\n\nThe
  `trainUntil` function takes a TrainCompletionPredicate (check Network/Trainer.hs)\nfor
  more information, and the last two are simply wrappers for the first one that\nprovide
  specific predicates.\n\nThe calculated error is what is returned by the cost function.\n\n####
  Cost Functions\n\nCurrently, the only provided cost function is the quadratic error
  cost function,\n`quadraticCost` and its derivative, `quadraticCost'`. I am about
  to add the\ncross-entropy cost function.\n\n#### Selection Functions\n\nSelection
  functions break up a dataset for each round of training. The currently provided\nselection
  functions are:\n  - `minibatch n` - You must provide an n and partially apply it
  to minibatch to get a valid selection function. This function updates the network
  after every n passes.\n  - `online` - Using this function means that the network
  updates after every training example.\n\nFor small data sets, it's better to use
  online, while for larger data sets, the training\ncan occur much faster if you use
  a reasonably sized minibatch.\n\n### Using the Network\n\nOnce the network is trained,
  you can use it with your test data or\nproduction data:\n\n```\npredict (fromList
  [1, 0]) n'\n```\n\nLambdaNet at least attempts to follow a Scikit-Learn style naming
  scheme\nwith `fit` and `predict` functions.\n\n### Storing and Loading\n\nOnce a
  network has been trained, the weights and biases can be stored in\na file:\n\n```\nsaveNetwork
  \"xor.ann\" n'\n```\n\nBy calling `saveNetwork` with a file path, you can save the
  state of the\nnetwork.\n\nLoading a network requires passing in a list of layer
  definitions\nfor the original network, but will load all the weights and biases
  of the\nsaved network:\n\n```\nn'' <- loadNetwork \"xor.ann\" [l, l', l'']\n```\n\nNote
  that the loadNetwork function returns an IO (Network), you can't simply\ncall predict
  or train on the object returned by loadNetwork. Using the\napproach in XOR.hs should
  allow you to work with the returned object.\n\n## Currently Under Development\n\nWhat
  has been outlined above is only the first stages of LambdaNet. I intend\nto support
  some additional features, such as:\n  - Regularization functions\n  - Additional
  trainer types (RProp, RMSProp)\n  - Additional cost functions\n\n### Regularization
  Functions and Momentum\n\nStandard backprop training is subject to overfitting and
  falling into local\nminima. By providing support for regularization and momentum,
  LambdaNet\nwill be able to provide more extensible and robust training.\n\n## Generating
  the Documentation Images\n\nAll the documentation for the network was generated
  in the following manner. In the docs folder, run:\n\n```\nrunhaskell docs.hs\npython
  analysis.py\n```\n\nNote that I am currently working on removing the Python image
  analysis\nfrom the library, and switching it with Haskell and gnuplot. I'm also\nworking
  on using the generated images in network documentation.\n"
license-name: MIT
