homepage: https://github.com/PatrickMaier/HdpH
changelog-type: ''
hash: 5279dc134962b3a11f747534b5dea426eb09ab823b505a5f433b6f220c173b5c
test-bench-deps: {}
maintainer: Patrick Maier <C.Patrick.Maier@gmail.com>
synopsis: Haskell distributed parallel Haskell
changelog: ''
basic-deps:
  cereal: ! '>=0.3.3 && <0.4'
  bytestring: ==0.10.*
  base: ==4.*
  time: ! '>=1.2 && <2'
  network: ==2.4.*
  containers: ! '>=0.1 && <0.6'
  network-multicast: ! '>=0.0.7 && <0.1'
  network-transport-tcp: ==0.3.*
  mtl: ==2.*
  network-info: ==0.2.*
  network-transport: ==0.3.*
  hdph-closure: ==0.0.1
  random: ==1.*
  deepseq: ! '>=1.1 && <2'
  template-haskell: -any
all-versions:
- 0.0.1
author: ! 'Patrick Maier <C.Patrick.Maier@gmail.com>,

  Rob Stewart <robstewart57@gmail.com>'
latest: 0.0.1
description-type: markdown
description: ! "Haskell Distributed Parallel Haskell\n====================================\n\n**Haskell
  distributed parallel Haskell (HdpH)** is a Haskell DSL for\nparallel computation
  on distributed-memory architectures. HdpH is\nimplemented entirely in Haskell but
  does make use of a few GHC extensions,\nmost notably TemplateHaskell.\n\nHdpH is
  described in some detail in the paper [Implementing a High-level Distributed-Memory
  Parallel Haskell in Haskell](http://www.macs.hw.ac.uk/~pm175/papers/Maier_Trinder_IFL2011_XT.pdf).\nThe
  paper on [The Design of SymGridPar2](http://www.macs.hw.ac.uk/~pm175/papers/Maier_Stewart_Trinder_SAC2013.pdf)
  paints the bigger picture of what HdpH was designed for and where future developments
  will lead.\n\nThis release is considered alpha stage.\n\n\nBuilding HdpH\n-------------\n\nShould
  be straightforward from the cabalised package `hdph`.\nNote that `hdph` depends
  on `hdph-closure`, an independent package that\nfactors out the closure representation
  of HdpH.\n\n\nRunning HdpH\n------------\n\n### Launch via `ssh`\n\nHdpH comes with
  a few sample applications. The simplest one,\na distributed _Hello World_, is good
  for testing whether HdpH works.\nFor example, the following Bourne shell command
  will run _Hello World_\non three nodes, distributed over two hosts, `bwlf01` and
  `bwlf02`.\n\n    $> for host in bwlf01 bwlf02 bwlf01; do ssh $host hdph/dist/build/hello/hello
  -numProcs=3 & done\n\nThis will launch three instances of the _Hello World_ binary
  `hello` (in\ndirectory `hdph/dist/build/hello` relative to the user's home directory),\ntwo
  on `bwlf01` and one on `bwlf02`.\nNote that the obligatory option `-numProcs` must
  match the number of instances;\nHdpH will likely hang otherwise.\n\nAssuming that
  `ssh` has been set up to enable password-less logins, the output\nshould be something
  like this:\n\n    Master Node:137.195.143.101:19754:0 wants to know: Who is here?\n
  \   Hello from Node:137.195.143.101:19754:0\n    Hello from Node:137.195.143.102:38939:0\n
  \   Hello from Node:137.195.143.101:30062:0\n\n\n### Launch via `mpiexec`\n\nShell
  scripts and `ssh` are quite a cumbersome way of launching HdpH.\nMore convenient
  are dedicated parallel job launchers such as `mpiexec`,\nwhich comes with any recent
  MPI distribution.\nEven though this version of HdpH does not use MPI, it can still
  be launched\nby `mpiexec`.\nAn MPICH installation, for example, might launch the
  _Hello World_ application\nwith the following command:\n\n    $> mpiexec -hosts
  bwlf01,bwlf02,bwlf01 ./hello -numProcs=3\n\nThis will launch the _Hello World_ binary
  `hello` (in the current working\ndirectory) 3 times, twice on `bwlf01` and once
  on `bwlf02`.\nThe expected output should look somewhat like this:\n\n    Hello from
  Node:137.195.143.102:12701:0\n    Hello from Node:137.195.143.101:15353:0\n    Master
  Node:137.195.143.101:14247:0 wants to know: Who is here?\n    Hello from Node:137.195.143.101:14247:0\n\n\n###
  Parallel launch via `mpiexec`\n\nTo test parallelism, there is a sample application
  computing the `n`th number\nof the _Fibonacci_ sequence using a naive parallel divide-and-conquer
  algorithm.\nFor example, the following command will compute the 45th Fibonacci number
  in\nparallel on 3 nodes, switching to sequential execution for subproblems below\na
  threshold of 30; a wall clock runtime of about 16 seconds is reported here.\n\n
  \   $> mpiexec -hosts bwlf01,bwlf02,bwlf03 ./fib -numProcs=3 v2 45 30\n    {v2,
  seqThreshold=30, parThreshold=30} fib 45 = 1836311903 {runtime=16.157814s}\n\nWhen
  passed the switch `-d1` HdpH will produce some summary output per\nnode relating
  to parallelism, eg. number of sparks generated per node,\nmaximum number of sparks
  residing on a node, number of times a node\nrequested work, and number of times
  work was scheduled in response.\nAn example run with `-d1` might look like this:\n\n
  \   $> mpiexec -hosts bwlf01,bwlf02,bwlf03 ./fib -numProcs=3 -d1 v2 45 30\n    137.195.143.101:9759:0
  #SPARK=1017   max_SPARK=154   max_THREAD=[3,1]\n    137.195.143.101:9759:0 #FISH_sent=1
  \  #SCHED_rcvd=0\n    {v2, seqThreshold=30, parThreshold=30} fib 45 = 1836311903
  {runtime=16.449183s}\n    137.195.143.102:12551:0 #SPARK=308   max_SPARK=4   max_THREAD=[1,1]\n
  \   137.195.143.102:12551:0 #FISH_sent=215   #SCHED_rcvd=214\n    137.195.143.103:27402:0
  #SPARK=271   max_SPARK=4   max_THREAD=[0,1]\n    137.195.143.103:27402:0 #FISH_sent=225
  \  #SCHED_rcvd=224\n\nThis shows that `bwlf01` (IP address `137.195.143.101`) was
  the master node,\ngenerating 1017 sparks in total, requesting work once (probably
  at the end\nof the computation) and not receiving any.\nThe other nodes did generate
  some sparks themselves (308 and 271, respectively)\nbut they also both received
  a substantial number of sparks (214 and 224,\nrespectively).\nMost importantly,
  each node requested work exactly once more often than\nreceiving it, which means
  no node was ever idling, except probably at the\nvery end of the computation.\n\n\n###
  Parallel launch via `mpiexec` on homogeneous multicores\n\nSo far HdpH ran in a
  single thread on each node.\nTo make use of multicores `mpiexec` can place several
  single-threaded nodes\non the same host, eg. the following examples will launch
  two and four nodes per\nhost, respectively; note that the number following the `-n`
  switch of `mpiexec`\nmust match the number following the `-numProcs` switch of `fib`.\n\n
  \   $> mpiexec -hosts bwlf01,bwlf02,bwlf03 -n 6 ./fib -numProcs=6 v2 45 30\n    {v2,
  seqThreshold=30, parThreshold=30} fib 45 = 1836311903 {runtime=8.026417s}\n\n    $>
  mpiexec -hosts bwlf01,bwlf02,bwlf03 -n 12 ./fib -numProcs=12 v2 45 30\n    {v2,
  seqThreshold=30, parThreshold=30} fib 45 = 1836311903 {runtime=4.845796s}\n\nThe
  other possibility is to run HdpH itself in a multi-threaded mode.\nThe following
  two examples run HdpH on two and four threads per node,\nrespectively, expecting
  that the OS will bind each thread to a core.\n\n    $> mpiexec -hosts bwlf01,bwlf02,bwlf03
  ./fib -numProcs=3 -scheds=2 v2 45 30 +RTS -N2\n    {v2, seqThreshold=30, parThreshold=30}
  fib 45 = 1836311903 {runtime=10.648305s}\n\n    $> mpiexec -hosts bwlf01,bwlf02,bwlf03
  ./fib -numProcs=3 -scheds=4 v2 45 30 +RTS -N4\n    {v2, seqThreshold=30, parThreshold=30}
  fib 45 = 1836311903 {runtime=6.507969s}\n\nNote that the GHC RTS switch `-N` determines
  the number of threads (or\nHECs in GHC terminology) whereas the `fib` switch `-scheds`
  determines\nhow many of these threads run the HdpH scheduler loop. Whether there
  should\nbe as many schedulers as threads or less depends on the GHC version and\nprobably
  on the OS. Using one scheduler less than the number of threads\nhas been observed
  to reduce variability, and sometimes even bring\nperformance gains:\n\n    $>  mpiexec
  -hosts bwlf01,bwlf02,bwlf03 ./fib -numProcs=3 -scheds=3 v2 45 30 +RTS -N4\n    {v2,
  seqThreshold=30, parThreshold=30} fib 45 = 1836311903 {runtime=5.831932s}\n\n\n###
  Parallel launch via `mpiexec` on heterogeneous multicores\n\nSo far all hosts were
  assumed to have the same number of cores.\nHowever, `mpiexec` can be used to launch
  on heterogeneous clusters.\nFor instance, the following call will launch HdpH with
  two threads each on\n`bwlf01` and `bwlf02`, and with four threads on `bwlf03`; please
  see the man\npage of `mpiexec` for an explanation of the command line format.\n\n
  \   $> mpiexec -hosts bwlf01,bwlf02,bwlf03 -n 2 ./fib -numProcs=3 -scheds=2 -d1
  v2 45 30 +RTS -N2 : -n 1 ./fib -numProcs=3 -scheds=4 -d1 v2 45 30 +RTS -N4\n    137.195.143.101:21394:0
  #SPARK=1114   max_SPARK=183   max_THREAD=[3,1,1]\n    137.195.143.101:21394:0 #FISH_sent=3
  \  #SCHED_rcvd=1\n    137.195.143.103:22027:0 #SPARK=261   max_SPARK=2   max_THREAD=[0,1,1,1,1]\n
  \   137.195.143.103:22027:0 #FISH_sent=209   #SCHED_rcvd=208\n    137.195.143.102:22374:0
  #SPARK=221   max_SPARK=3   max_THREAD=[1,1,1]\n    137.195.143.102:22374:0 #FISH_sent=173
  \  #SCHED_rcvd=172\n    {v2, seqThreshold=30, parThreshold=30} fib 45 = 1836311903
  {runtime=10.502205s}\n\nThe switch `-d1` reveals that `bwlf03` (IP address `137.195.143.103`)
  did\nindeed run 4 schedulers because its `max_THREAD` list was length 5, and\nthe
  length of the `max_THREAD` list is always 1 plus the number of schedulers.\n\n\n###
  Launch caveats\n\nAt startup HdpH nodes open random ports and rely on UDP multicasts
  to discover\neach other. This results in a number of limitations:\n\n+ UDP multicasts
  must be routed between all nodes (which may imply that all\n  hosts must reside
  in the same subnet).\n\n+ The obligatory `-numProcs` switch, which tells an application
  how many\n  nodes to expect, must match exactly the number of nodes launched.\n\n+
  Node discovery must be completed within a certain time frame (typically 10\n  seconds).\n\n+
  No second HdpH application may be launched during the node discovery phase.\n\nAny
  deviation from the above limitations is likely to cause HdpH to hang\nforever.\n\n\nRelated
  Projects\n----------------\n\n* [Cloud Haskell](http://haskell-distributed.github.com)\n\n*
  [Par Monad and Friends](https://github.com/simonmar/monad-par)\n\n\nReferences\n----------\n\n1.
  \ Patrick Maier, Phil Trinder.\n    [Implementing a High-level Distributed-Memory
  Parallel Haskell in Haskell](http://www.macs.hw.ac.uk/~pm175/papers/Maier_Trinder_IFL2011_XT.pdf).\n
  \   Proc. 2011 Symposium on Implementation and Application of Functional Languages
  (IFL 2011), Springer LNCS 7257, pp. 35-50.\n\n2.  Patrick Maier, Rob Stewart, Phil
  Trinder.\n    [Reliable Scalable Symbolic Computation: The Design of SymGridPar2](http://www.macs.hw.ac.uk/~pm175/papers/Maier_Stewart_Trinder_SAC2013.pdf).\n
  \   Proc. 28th ACM Symposium On Applied Computing (SAC 2013), pp. 1677-1684.\n\n3.
  \ [HdpH development repository](https://github.com/PatrickMaier/HdpH) on github.\n"
license-name: BSD-3-Clause
