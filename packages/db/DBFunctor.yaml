homepage: https://github.com/nkarag/DBFunctor#readme
changelog-type: markdown
hash: 895fb42812977752c50a415ad24b3f3118137f1ac7392225780770fc5a67b57f
test-bench-deps:
  MissingH: -any
  cereal: -any
  either: -any
  bytestring: -any
  base: ! '>=4.7 && <5'
  unordered-containers: -any
  text: -any
  containers: -any
  cassava: -any
  transformers: -any
  deepseq: -any
  DBFunctor: -any
  vector: -any
maintainer: nkarag@gmail.com
synopsis: DBFunctor - Functional Data Management =>  ETL/ELT Data Processing in Haskell
changelog: ! "\uFEFF# Changelog for DBFunctor\r\n\r\n### 0.1.0.0\r\n - Initial Version.
  Includes a full-working version of \r\n\t - Julius: A type-level Embedded Domain
  Specific (EDSL) Language for ETL\r\n\t - all common Relational Algebra operations,
  \r\n\t - the ETL Mapping and other typical ETL constructs and operations\r\n\t -
  operations applicable to all kinds of tabular data\r\n\t - In-memory, database-less
  data processing.\r\n"
basic-deps:
  MissingH: -any
  cereal: -any
  either: -any
  bytestring: -any
  base: ! '>=4.7 && <5'
  unordered-containers: -any
  text: -any
  containers: -any
  cassava: -any
  transformers: -any
  deepseq: -any
  DBFunctor: -any
  vector: -any
all-versions:
- '0.1.0.0'
author: Nikos Karagiannidis
latest: '0.1.0.0'
description-type: markdown
description: ! "\uFEFF![dbfunctor logo](./dbfunctor.png)\r\n# DBFunctor:  Functional
  Data Management\r\n## ETL/ELT* Data Processing in Haskell\r\n**DBFunctor** is a
  [Haskell](https://haskell-lang.org/) library for *ETL/ELT[^1]* data processing of
  tabular data. What does this mean?\r\nIt simply means that whenever you have a ***data
  analysis*, *data preparation*, or *data transformation* task** and you want to do
  it with Haskell type-safe code, that you enjoy, love and trust so much, now you
  can! \r\n### Main Features\r\n 1. **Julius: An Embedded Domain Specific (EDSL) Language
  for ETL**\r\nProvides an intuitive type-level Embedded Domain Specific (EDSL) Language
  called *Julius* for expressing complex data flows (i.e., ETL flows)  but also for
  performing SQL-like data analysis. For more info check this [Julius tutorial](https://github.com/nkarag/haskell-DBFunctor/blob/master/doc/JULIUS-TUTORIAL.md).\r\n
  2. **Supports all known relational operations**\r\nJulius supports all known relational
  operations (selection, projection, inner/outer join, grouping, ordering, aggregation,
  set operations etc.)\r\n 3. **Provides the ETL Mapping and other typical ETL constructs
  and operations**\r\nJulius implements typical ETL constructs such the *Column Mapping*
  and the *ETL Mapping*.\r\n 4. **Applicable to all kinds of tabular data**\r\nIt
  is applicable to all kinds of \"tabular data\" (see explanation below)\r\n 5. **In-memory,
  database-less data processing**\r\nData transformations or queries can run *in-memory*,
  within your Haskell code, without the need for a database to process your data.
  \ \r\n6. **Offloading to a database for heavy queries/data transformations**\r\nIn
  addition, a query or data transformation  can be *offloaded to a Database*, when
  data don't fit in memory, or heavy data processing over large volumes of data is
  required. The result can be fetched into the client's memory  (i.e., where your
  haskell code runs) in the `RTable` data structure  (see below), or stored in a database
  staging table.\r\n 7. **Workflow Operations**\r\nJulius provides common workflow
  operations. Workflows provide the ability to combine the evaluation of several different
  Julius Expressions (i.e., data pipelines) in an arbitrary logic. Examples of such
  operations include:\r\n - Ability to handle a failure of some operation in a Julius
  expression:\r\n\t - retry the failed operation (after corrective actions have taken
  place) and continue the evaluation of the Julius expression from this point onward.\r\n\t
  - skip the failed operation and move on with the rest operations in the pipeline.\r\n\t
  - restart the Julius expression from the beginning\r\n\t - terminate the Julius
  expression and skip all pending operations\r\n - Ability to start a Julius expression
  based on the success or failure result of another one\r\n - Ability to fork several
  different Julius expressions that will run concurrently\r\n - Conditional execution
  of Julius expressions and iteration functionality\r\n - Workflow hierarchy (i.e.,
  flows, subflows etc.)\r\n 8. **\"Declarative ETL\"**\r\nEnables *declarative ETL*
  implementation  in the same sense that SQL is declarative for querying data (see
  more below).\r\n### Typical examples of DBFunctor use-cases\r\n - **Build database-less
  Haskell apps.** Build your data processing haskell apps without the need to import
  your data in a database for querying functionality or any for executing any data
  transformations. Analyze your CSV files in-place with plain haskell code (*for Haskellers!*).\r\n
  - **Data Preparation.** I.e., clean-up data, calculate derived fields and variables,
  group by and aggregate etc., in order to feed some machine learning algorithm (*for
  Data Scientists*).\r\n - **Data Transformation.** in order to transform data from
  Data Model A to Data Model B (typical use-case *for Data Engineers* who perform
  ETL/ELT[^1] tasks for feeding Data Warehouses or Data Marts)\r\n - **Data Exploration.**
  Ad hoc data analysis tasks, in order to explore a data set for several purposes
  such as to find business insights and solve a specific business problem, or maybe
  to do data profiling in order to evaluate the quality of the data coming from a
  data source, etc (*for Data Analysts*).\r\n - **Business Intelligence.** Build reports,
  or dashboards in order to share business insights with others and drive decision
  making process (*for BI power-users*)\r\n\r\n[^1]:  **ETL** stands for **Extract
  Transform and Load** and is the standard technology for accomplishing data management
  tasks in Data Warehouses / Data Marts and in general for preparing data for any
  analytic purposes (Ad hoc queries, data exploration/data analysis, Reporting and
  Business Intelligence, feeding Machine Learning algorithms, etc.). **ELT** is a
  newer variation of ETL and means that the data are first Loaded into their final
  destination and then the data transformation runs in-place (as opposed to running
  at a separate staging area on possibly a different server)).\r\n\r\n### When to
  Use it?\r\nDBFunctor should be used whenever a data analysis, or data manipulation,
  or data transformation task, over *tabular data*, must be performed and we wish
  to perform it with Haskell code -yielding all the well-known (to Haskellers) benefits
  from doing that- without the need to use a database query engine for this task.\r\nDBFunctor
  provides an in-memory data structure called `RTable`, which implements the concept
  of a *Relational Table* (which -simply put- is a set of tuples) and all relevant
  relational algebra operations (Selection, Projection, Inner Join, Outer Joins, aggregations,
  Group By, Set Operations etc.).\r\nMoreover, it implements the concept of *Column
  Mapping* (for deriving new columns based on existing ones - by splitting , merging
  , or with any other possible combination using a lambda expression or a function
  to define the new value) and that of the *ETL Mapping*, which is the equivalent
  of a \"mapping\" in an ETL tool (like Informatica, Talend, Oracle Data Integrator,
  SSIS, Pentaho, etc.). With this powerful construct, one can **build arbitrary complex
  data pipelines**, which can enable any type of data transformations and all these
  **by writing Haskell code.**\r\n### What Kinds of Data?\r\nWith the term \"tabular
  data\" we mean any type of data that can be mapped to an RTable (e.g., CSV (or any
  other delimiter), DB Table/Query, JSON etc). Essentially, for a Haskell data type
  `a`to be \"tabular\", one must implement the following functions:\r\n```haskell\r\n
  \  toRTable :: RTableMData -> a -> RTable\r\n   fromRTable :: RTableMData -> RTable
  -> a\r\n```    \r\nThese two functions implement the \"logic\" of transforming data
  type `a` to/from an RTable based on specific RTable Metadata, which specify the
  column names and data types of the RTable, as well as (optionally) the primary key
  constraint, and/or alternative unique constraints (i.e., similar information provided
  with a CREATE TABLE statement in SQL) .\r\nBy implementing these two functions,
  data type `a` essentially becomes an instance of the type `class RTabular` and thus
  can be transformed with the  DBFunctor package. Currently, we have implemented a
  CSV data type (any delimeter allowed), based one the [Cassava](https://github.com/haskell-hvr/cassava)
  library, in order to enable data transformations over CSV files.\r\n### Current
  Status and Roadmap\r\nCurrently (version DBFunctor-0.1.0.0), the DBFunctor package
  **is stable for in-memory data transformation and queries of CSV files**  (any delimiter
  allowed), with the **Julius EDSL** (module Etl.Julius) , or directly via RTable
  functions (module RTable.Core). The use of the Julius language is strongly recommended
  because it simplifies greatly and standardizes the creation of complex ETL flows.\r\nAll
  in all, currently main features from #1 to #5 (from the list above) have been implemented
  and main features > #5  are future work that will be released in later versions.
  \r\n### Future Vision -> Declarative ETL\r\nOur ultimate goal is, eventually to
  make DBFunctor the **first *Declarative library for ETL/ELT, or data processing
  in general***, by exploiting the virtues of functional programming and Haskell strong
  type system in particular.\r\nHere we use \"declarative\"  in the same sense that
  SQL is a declarative language for querying data. (You only have to state what data
  you want to be returned and you don't care about how this will be accomplished -
  the DBMS engine does this for you behind the scenes).\r\nIn the same manner,  ideally,
  one should only need to code the desired data transformation from a *source schema*
  to a *target schema*, as well as all the *data integrity constraints* and *business
  rules* that should hold after the transformation and not having to define all the
  individual steps for implementing the transformation, as it is the common practice
  today. This will yield tremendous benefits compared to common ETL challenges faced
  today and change the way we build data transformation flows. Just to name a few:\r\n
  - Automated ETL coding driven by Source-to-Target mapping and business rules\r\n
  - ETL code correctness out-of-the-box\r\n - Data Integrity / Business Rules controls
  automatically embedded in your ETL code\r\n - Self-documented ETL code (Your documentation
  i.e., the Source-to-Target mapping and the business rules, is also the only code
  you need to write!)\r\n - Drastically minimize time-to-market for delivering Data
  Marts and Data Warehouses, or simply implementing Data Analysis tasks.\r\n\r\nThe
  above is inspired by the theoretical work on Categorical Databases by David Spivak,\r\n###
  Available Modules\r\n*DBFunctor* consists of the following set of Haskell modules:\r\n*
  **RTable.Core**: Implements the relational Table concept. Defines all necessary
  data types like `RTable` and `RTuple` as well as basic relational algebra operations
  on RTables.\r\n* **Etl.Julius**: A simple Embedded DSL for ETL/ELT data processing
  in Haskell\r\n* **RTable.Data.CSV**:  Implements `RTable` over CSV (TSV, or any
  other delimiter) files logic. It is based on the [Cassava](https://github.com/haskell-hvr/cassava)
  library.\r\n### A Very Simple Example\r\n In this example, we will load a CSV file,
  turn it into an RTable and then issue a very simple query on it and print the result,
  just to show the whole concept.\r\nSo  lets say we have a CSV file called test-data.csv.
  The file stores table metadata from an Oracle database. Each row represents a table
  stored in the database. Here is a small extract from the csv file:\r\n\r\n    $
  head test-data.csv\r\n        OWNER,TABLE_NAME,TABLESPACE_NAME,STATUS,NUM_ROWS,BLOCKS,LAST_ANALYZED\r\n
  \       APEX_030200,SYS_IOT_OVER_71833,SYSAUX,VALID,0,0,06/08/2012 16:22:36\r\n
  \       APEX_030200,WWV_COLUMN_EXCEPTIONS,SYSAUX,VALID,3,3,06/08/2012 16:22:33\r\n
  \       APEX_030200,WWV_FLOWS,SYSAUX,VALID,10,3,06/08/2012 22:01:21\r\n        APEX_030200,WWV_FLOWS_RESERVED,SYSAUX,VALID,0,0,06/08/2012
  16:22:33\r\n        APEX_030200,WWV_FLOW_ACTIVITY_LOG1$,SYSAUX,VALID,1,29,07/20/2012
  19:07:57\r\n        APEX_030200,WWV_FLOW_ACTIVITY_LOG2$,SYSAUX,VALID,14,29,07/20/2012
  19:07:57\r\n        APEX_030200,WWV_FLOW_ACTIVITY_LOG_NUMBER$,SYSAUX,VALID,1,3,07/20/2012
  19:08:00\r\n        APEX_030200,WWV_FLOW_ALTERNATE_CONFIG,SYSAUX,VALID,0,0,06/08/2012
  16:22:33\r\n        APEX_030200,WWV_FLOW_ALT_CONFIG_DETAIL,SYSAUX,VALID,0,0,06/08/2012
  16:22:33\r\n**1. Turn the CSV file into an RTable**\r\nThe first thing we want to
  do is to read the file and turn it into an RTable. In order to do this we need to
  define the RTable Metadata, which is the same information one can provide in an
  SQL CREATE TABLE statement, i,e, column names, column data types and integrity constraints
  (Primary Key, Unique Key only - no Foreign Keys). So lets see how this is done:
  \    \r\n```haskell\r\n    -- Define table metadata\r\n    src_DBTab_MData :: RTableMData\r\n
  \   src_DBTab_MData =\r\n\t    createRTableMData   (   \"sourceTab\"  -- table name\r\n
  \                                ,[  (\"OWNER\", Varchar)                                      --
  Owner of the table\r\n                                    ,(\"TABLE_NAME\", Varchar)
  \                               -- Name of the table\r\n                                    ,(\"TABLESPACE_NAME\",
  Varchar)                           -- Tablespace name\r\n                                    ,(\"STATUS\",Varchar)
  \                                    -- Status of the table object (VALID/IVALID)\r\n
  \                                   ,(\"NUM_ROWS\", Integer)                                  --
  Number of rows in the table\r\n                                    ,(\"BLOCKS\",
  Integer)                                    -- Number of Blocks allocated for this
  table\r\n                                    ,(\"LAST_ANALYZED\", Timestamp \"MM/DD/YYYY
  HH24:MI:SS\")   -- Timestamp of the last time the table was analyzed (i.e., gathered
  statistics)\r\n                                  ]\r\n                             )\r\n
  \                               [\"OWNER\", \"TABLE_NAME\"] -- primary key\r\n                                []
  -- (alternative) unique keys\r\n    main :: IO ()\r\n    main = do\r\n       --
  read source csv file\r\n       srcCSV <- readCSV \"./app/test-data.csv\"    \r\n
  \      let\r\n         -- turn source csv to an RTable\r\n         src_DBTab = toRTable
  src_DBTab_MData srcCSV\r\n    ...\r\n```\r\nWe have used the following functions:\r\n```haskell\r\n--
  | createRTableMData : creates RTableMData from input given in the form of a list\r\n--
  \  We assume that the column order of the input list defines the fixed column order
  of the RTuple.\r\ncreateRTableMData ::\r\n        (RTableName, [(ColumnName, ColumnDType)])\r\n
  \       -> [ColumnName]     -- ^ Primary Key. [] if no PK exists\r\n        -> [[ColumnName]]
  \  -- ^ list of unique keys. [] if no unique keys exists\r\n        -> RTableMData\r\n```\r\nin
  order to define the RTable metadata.\r\nFor reading the CSV file we have used:\r\n```haskell\r\n--
  | readCSV: reads a CSV file and returns a CSV data type (Treating CSV data as opaque
  byte strings)\r\nreadCSV ::\r\n    FilePath  -- ^ the CSV file\r\n    -> IO CSV
  \ -- ^ the output CSV type\r\n```\r\nFinally, in order to turn the CSV data type
  into an RTable, we have used function:\r\n```haskell\r\ntoRTable :: RTableMData
  -> CSV -> RTable\r\n```\r\nwhich comes from the `RTabular` type class instance of
  the `CSV` data type.\r\n **2. Query the RTable**\r\nOnce we have created an RTable,
  we can issue queries on it, or apply any type of data transformations. Note that
  due to immutability, each query or data transformation creates a new RTable.\r\n
  We will now issue the following query:\r\nWe return all the rows, which correspond
  to some filter predicate - in particular all rows where the table_name starts with
  a 'B'. For this we use the Julius EDSL, in order to express the query and then with
  the function `juliusToRTable :: ETLMappingExpr -> RTable `, we evaluate the expression
  into an RTable.\r\n```haskell\r\ntabs_with_B = juliusToRTable $\r\n   EtlMapStart\r\n
  \  :-> (EtlR $\r\n           ROpStart\r\n              -- apply a filter on RTable
  \"src_DBTab\" based on a predicate, expressed with a lambda expression\r\n           :.
  (Filter (From $ Tab src_DBTab) $                 \r\n                   FilterBy
  (\\t ->\tlet fstChar = Data.Text.take 1 $ fromJust $ toText (t <!> \"TABLE_NAME\")\r\n\t\t\t\t\t\t
  \           in fstChar == (pack \"B\"))\r\n           )\r\n              -- A simple
  column projection applied on the Previous result\r\n           :. (Select [\"OWNER\",
  \"TABLE_NAME\"] $  From Previous)\r\n   )\r\n```\r\nA Julius expression is a *data
  processing chain* consisting of various Relational Algebra operations `(EtlR $ ...)`
  \ and/or column mappings `(EtlC $ ...)` connected together via the `:->` data constructor,
  \ of the form (Julius expressions are read *from top-to-bottom  or from left-to-right*):\r\n```haskell\r\nmyJulExpression
  =\r\n\tEtlMapStart\r\n\t:-> (EtlC $ ...)  -- this is a Column Mapping\r\n\t:-> (EtlR
  $   -- this is a series of Relational Algebra Operations\r\n\t     ROpStart\r\n\t
  \ :. (Rel Operation 1) -- a relational algebra operation\r\n\t  :. (Rel Operation
  2))\r\n\t:-> (EtlC $ ...)  -- This is another Column Mapping\r\n\t:-> (EtlR $ --
  more relational algebra operations\r\n\t     ROpStart\r\n\t  :. (Rel Operation 3)\r\n\t
  \ :. (Rel Operation 4)\r\n\t  :. (Rel Operation 5))\r\n\t:-> (EtlC $ ...) -- This
  is Column Mapping 3\r\n\t:-> (EtlC $ ...) -- This is Column Mapping 4\r\n\t...\r\n```\r\nIn
  our example, the Julius expression consists only of two relational algebra operations:
  a `Filter` operation, which uses an RTuple  predicate of the form \t`RTuple -> Bool`
  to filter out RTuples (i.e., rows) that dont satisfy this predicate. The predicate
  is expressed as the lambda expression:\r\n```haskell\r\nFilterBy (\\t ->\tlet fstChar
  = Data.Text.take 1 $ fromJust $ toText (t <!> \"TABLE_NAME\")\r\n\t\t\t\tin fstChar
  == (pack \"B\"))\r\n```\r\nThe second relational operation is a simple Projection
  expressed with the node `(Select [\"OWNER\", \"TABLE_NAME\"] $ From Previous)`\r\nFinally,
  in order to print the result of the query on the screen, we use the `printfRTable
  :: RTupleFormat -> RTable -> IO()` function, which brings printf-like functionality
  into the printing of RTables\r\nAnd here is the output:\r\n```\r\n$ stack exec --
  dbfunctor-example\r\nThese are the tables that start with a \"B\":\r\n\r\n-------------------------------------\r\nOWNER
  \     TABLE_NAME                \r\n~~~~~      ~~~~~~~~~~                \r\nDBSNMP
  \    BSLN_BASELINES            \r\nDBSNMP     BSLN_METRIC_DEFAULTS      \r\nDBSNMP
  \    BSLN_STATISTICS           \r\nDBSNMP     BSLN_THRESHOLD_PARAMS     \r\nSYS
  \       BOOTSTRAP$                \r\n\r\n5 rows returned\r\n-------------------------------------\r\n```\r\nHere
  is the complete example.\r\n```haskell\r\nmodule Main where\r\n\r\nimport  RTable.Core
  \        (RTableMData ,ColumnDType (..) ,createRTableMData, printfRTable, genRTupleFormat,
  genDefaultColFormatMap, toText, (<!>))\r\nimport  RTable.Data.CSV     (CSV, readCSV,
  toRTable)\r\nimport  Etl.Julius          \r\n\r\nimport Data.Text            (take,
  pack)\r\nimport Data.Maybe           (fromJust)\r\n\r\n\r\n--  Define Source Schema
  (i.e., a set of tables)\r\n\r\n-- | This is the basic source table\r\n-- It includes
  the tables of an imaginary database\r\nsrc_DBTab_MData :: RTableMData\r\nsrc_DBTab_MData
  =\r\n    createRTableMData   (   \"sourceTab\"  -- table name\r\n                            ,[
  \ (\"OWNER\", Varchar)                                      -- Owner of the table\r\n
  \                               ,(\"TABLE_NAME\", Varchar)                                --
  Name of the table\r\n                                ,(\"TABLESPACE_NAME\", Varchar)
  \                          -- Tablespace name\r\n                                ,(\"STATUS\",Varchar)
  \                                    -- Status of the table object (VALID/IVALID)\r\n
  \                               ,(\"NUM_ROWS\", Integer)                                  --
  Number of rows in the table\r\n                                ,(\"BLOCKS\", Integer)
  \                                   -- Number of Blocks allocated for this table\r\n
  \                               ,(\"LAST_ANALYZED\", Timestamp \"MM/DD/YYYY HH24:MI:SS\")
  \  -- Timestamp of the last time the table was analyzed (i.e., gathered statistics)\r\n
  \                           ]\r\n                        )\r\n                        [\"OWNER\",
  \"TABLE_NAME\"] -- primary key\r\n                        [] -- (alternative) unique
  keys\r\n\r\n-- | Define Target Schema (i.e., a set of tables)\r\n\r\n\r\nmain ::
  IO ()\r\nmain = do\r\n    -- read source csv file\r\n    srcCSV <- readCSV \"./app/test-data.csv\"\r\n\r\n
  \   let\r\n        -- create source RTable from source csv\r\n        src_DBTab
  = toRTable src_DBTab_MData srcCSV      \r\n\r\n        -- select all tables starting
  with a B\r\n        tabs_with_B = juliusToRTable $\r\n                EtlMapStart\r\n
  \               :-> (EtlR $\r\n                        ROpStart                            \r\n
  \                       :. (Filter (From $ Tab src_DBTab) $                 \r\n
  \                               FilterBy (\\t ->  let fstChar = Data.Text.take 1
  $ fromJust $ toText (t <!> \"TABLE_NAME\") in fstChar == (pack \"B\"))\r\n                        )\r\n
  \                       :. (Select [\"OWNER\", \"TABLE_NAME\"] $\r\n                            From
  Previous\r\n                        )\r\n                )\r\n\r\n    putStrLn \"\\nThese
  are the tables that start with a \\\"B\\\":\\n\"\r\n\r\n    -- print source RTable
  first 100 rows\r\n    printfRTable (  \r\n                    -- this is the equivalent
  when pinting on the screen to a list of columns in a SELECT clause in SQL\r\n                    genRTupleFormat
  [\"OWNER\", \"TABLE_NAME\"] genDefaultColFormatMap\r\n                 ) $ tabs_with_B\r\n```\r\n\r\n###
  Julius Tutorial \r\nWe have written [a Julius tutorial](https://github.com/nkarag/haskell-DBFunctor/blob/master/doc/JULIUS-TUTORIAL.md)
  to help you get started with Julius DSL. \r\n<a  name=\"howtorun\"></a> \r\n###
  How to run\r\nDownload or clone DBFunctor in a new directory, \r\n```\r\n$ git clone
  https://github.com/nkarag/haskell-DBFunctor\r\n$ cd haskell-DBFunctor/\r\n```\r\nthen
  run \r\n```\r\n$ stack build --haddock\r\n```\r\nin order to build the code and
  generate the documentation with the [stack](https://docs.haskellstack.org/en/stable/README/)
  tool.\r\nIn order, to use it in you own haskell app, you only need to import the
  Julius module (you dont have to import RTable.Core, because it is exported by Julius)\r\n```Haskell\r\nimport
  Etl.Julius\r\n```\r\nFinally, for a successful build of your app, you must direct
  stack to treat it as a local package (Soon DBFunctor will be added to Hackage and
  you will not need to use it as a local package.). \r\nSo you have to include it
  in your stack.yaml file, as a local package that you want to link your code to.
  \r\n```\r\npackages:\r\n- location: .\r\n- location: <path where DBFunctor package
  has been cloned>\r\n  extra-dep: true\r\n```\r\nAnd of course, you must not forget
  to add the dependency of your app to the DBFunctor package in your .cabal file\r\n```\r\n
  \ build-depends:      ...\r\n                      , DBFunctor\r\n\r\n```\r\n"
license-name: BSD3
