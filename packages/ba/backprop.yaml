homepage: https://github.com/mstksg/backprop#readme
changelog-type: markdown
hash: f102c1fdcebd743d51c9a5dc722bfb7859dc8384130742a3d82175007c249e2c
test-bench-deps:
  mwc-random: -any
  base: ! '>=4.7 && <5'
  time: -any
  criterion: -any
  lens: -any
  bifunctors: -any
  mnist-idx: -any
  transformers: -any
  deepseq: -any
  hmatrix: ! '>=0.18'
  backprop: -any
  vector: -any
  directory: -any
maintainer: justin@jle.im
synopsis: Heterogeneous automatic differentation (backpropagation)
changelog: ! "Changelog\n=========\n\nVersion 0.1.5.1\n---------------\n\n*Apr 8,
  2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.5.1>\n\n*   Fixed
  `NFData` instance for `T`; before, was shallow.\n*   Added `Typeable` instances
  for all tuple types, and for `BVar`.\n*   Added `Eq`, `Ord`, `Show`, etc. instances
  for `T`.\n*   Added `Binary` instances for all tuple types.  Note that this does
  incur a\n    *binary* dependency only because of the tuple types; however, this
  will\n    hopefully be not too much of an issue because *binary* is a GHC library\n
  \   anyway.\n\nVersion 0.1.5.0\n---------------\n\n*Mar 30, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.5.0>\n\n*
  \  `T` added to *Numeric.Backprop.Tuple*: basically an `HList` with a `Num`\n    instance.\n*
  \  `Eq` and `Ord` instances for `BVar`.  Is this sound?\n\n*Internal*\n\n*   Refactored
  `Monoid` instances in *Numeric.Backprop.Tuple*\n\nVersion 0.1.4.0\n---------------\n\n*Mar
  25, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.4.0>\n\n*   `isoVar`,
  `isoVar2`, `isoVar3`, and `isoVarN`: convenient aliases for\n    applying isomorphisms
  to `BVar`s.  Helpful for use with constructors and\n    deconstructors.\n*   `opIso2`
  and `opIso3` added to *Numeric.Backprop.Op*, for convenience.\n*   `T0` (Unit with
  numeric instances) added to *Numeric.Backprop.Tuple\".\n\n*Internal*\n\n*   Completely
  decoupled the internal implementation from `Num`, which showed\n    some performance
  benefits.  Mostly just to make the code slightly cleaner,\n    and to prepare for
  some day potentially decoupling the external API from\n    `Num` as well.\n\nVersion
  0.1.3.0\n---------------\n\n*Feb 12, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.3.0>\n\n*
  \  *Preulude.Backprop* module added with lifted versions of several *Prelude*\n
  \   and base functions.\n*   `liftOpX` family of operators now have a more logical
  ordering for type\n    variables.  This change breaks backwards-compatibility.\n*
  \  `opIsoN` added to export list of *Numeric.Backprop*\n*   `noGrad` and `noGrad1`
  added to *Numeric.Backprop.Op*, for functions with\n    no defined gradient.\n\n*Internal*\n\n*
  \  Completely decoupled the internal implementation from `Num`, which showed\n    some
  performance benefits.\n\nVersion 0.1.2.0\n---------------\n\n*Feb 7, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.2.0>\n\n*
  \  Added currying and uncurrying functions for tuples in\n    *Numeric.Backprop.Tuple*.\n*
  \  `opIsoN`, for isomorphisms between a tuple of values and a value.\n*   (Internal)
  AD engine now using `Any` from *ghc-prim* instead of `Some I`\n    from *type-combinators*\n\nVersion
  0.1.1.0\n---------------\n\n*Feb 6, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.1.0>\n\n*
  \  Added canonical strict tuple types with `Num` instances, in the module\n    *Numeric.Backprop.Tuple*.
  \ This is meant to be a band-aid for the problem\n    of orphan instances and potential
  mismatched tuple types.\n*   Fixed bug in `collectVar` that occurs if container
  sizes change\n\n*Internal*\n\n*   Internal tweaks to the underlying automatic differentiation
  types that\n    decouple backpropagation from `Num`, internally.  `Num` is now just
  used\n    externally as a part of the API, which might someday be made optional.\n\nVersion
  0.1.0.0\n---------------\n\n*Feb 5, 2018*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.1.0.0>\n\n*
  \  First non-alpha release.\n*   More or less complete redesign of library.  The
  entire API is completely\n    changed, and there is no backwards compatibility!\n
  \   *   Everything is now \"implicit\" style, and there is no more `BP` monad.\n
  \   *   Accessing items in `BVar`s is now lens-, prism-, and traversal- based,\n
  \       instead of iso- and generics-based.\n    *   `Op` is no longer monadic\n
  \   *   *Mono* modules are removed.\n    *   *Implicit* modules are removed, since
  they are the default\n    *   *Iso* module is removed, since `Iso`s no longer play
  major role in the\n        implementation of the library.\n*   Removed dependency
  on *ad* and *ad*-based ops, which had been pulling in\n    the vast majority of
  dependencies.\n*   Moved from *.cabal* file to *hpack* system.\n\nVersion 0.0.3.0\n---------------\n\n*Alpha*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.0.3.0>\n\n*
  \  Removed samples as registered executables in the cabal file, to reduce\n    dependences
  to a bare minimum.  For convenience, build script now also\n    compiles the samples
  into the local directory if *stack* is installed.\n\n*   Added experimental (unsafe)
  combinators for working with GADTs with\n    existential types, `withGADT`, to *Numeric.Backprop*
  module.\n\n*   Fixed broken links in changelog.\n\nVersion 0.0.2.0\n---------------\n\n*Alpha*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.0.2.0>\n\n*
  \  Added optimized numeric `Op`s, and re-write `Num`/`Fractional`/`Floating`\n    instances
  in terms of them.\n\n*   Removed all traces of `Summer`/`Unity` from the library,
  eliminating a\n    whole swath of \"explicit-Summer\"/\"explicit-Unity\" versions
  of functions.\n    As a consequence, the library now only works with `Num` instances.
  \ The\n    API, however, is now much more simple.\n\n*   Benchmark suite added for
  MNIST example.\n\nVersion 0.0.1.0\n---------------\n\n*Alpha*\n\n<https://github.com/mstksg/backprop/releases/tag/v0.0.1.0>\n\n*
  \  Initial pre-release, as a request for comments.  API is in a usable form\n    and
  everything is fully documented, but there are definitely some things\n    left to
  be done. (See [README.md][readme-0.0.1.0])\n\n    [readme-0.0.1.0]: https://github.com/mstksg/backprop/tree/v0.0.1.0#readme\n\n"
basic-deps:
  reflection: -any
  type-combinators: -any
  base: ! '>=4.7 && <5'
  binary: -any
  transformers: -any
  deepseq: -any
  microlens: -any
  primitive: -any
  vector: -any
all-versions:
- '0.0.1.0'
- '0.0.2.0'
- '0.0.3.0'
- '0.1.0.0'
- '0.1.1.0'
- '0.1.2.0'
- '0.1.3.0'
- '0.1.4.0'
- '0.1.5.0'
- '0.1.5.1'
author: Justin Le
latest: '0.1.5.1'
description-type: markdown
description: ! "backprop\n========\n\n[![backprop on Hackage](https://img.shields.io/hackage/v/backprop.svg?maxAge=2592000)](https://hackage.haskell.org/package/backprop)\n[![Build
  Status](https://travis-ci.org/mstksg/backprop.svg?branch=master)](https://travis-ci.org/mstksg/backprop)\n\n[**Introductory
  blog post**][blog]\n\n[blog]: https://blog.jle.im/entry/introducing-the-backprop-library.html\n\nAutomatic
  *heterogeneous* back-propagation.\n\nWrite your functions to compute your result,
  and the library will automatically\ngenerate functions to compute your gradient.\n\nDiffers
  from [ad][] by offering full heterogeneity -- each intermediate step\nand the resulting
  value can have different types.  Mostly intended for usage\nwith gradient descent
  and other numeric optimization techniques.\n\n[ad]: http://hackage.haskell.org/package/ad\n\nCurrently
  up on [hackage][] (with 100% documentation coverage), but more\nup-to-date documentation
  is currently rendered [on github pages][docs]!\n\n[hackage]: http://hackage.haskell.org/package/backprop\n[docs]:
  https://mstksg.github.io/backprop\n\nMNIST Digit Classifier Example\n------------------------------\n\nMy
  [blog post][blog] introduces the concepts in this library in the context of\ntraining
  a handwritten digit classifier.  I recommend reading that first.\n\nThere are some
  [literate haskell examples][mnist-lhs] in the source, though\n([rendered as pdf
  here][mnist-pdf]), which can be built (if [stack][] is\ninstalled) using:\n\n[mnist-lhs]:
  https://github.com/mstksg/backprop/blob/master/samples/backprop-mnist.lhs\n[mnist-pdf]:
  https://github.com/mstksg/backprop/blob/master/renders/backprop-mnist.pdf\n[stack]:
  http://haskellstack.org/\n\n```bash\n$ ./Build.hs exe\n```\n\nThere is a follow-up
  tutorial on using the library with more advanced types,\nwith extensible neural
  networks a la [this blog post][blog], [available as\nliterate haskell][neural-lhs]
  and also [rendered as a PDF][neural-pdf].\n\n[blog]: https://blog.jle.im/entries/series/+practical-dependent-types-in-haskell.html\n[neural-lhs]:
  https://github.com/mstksg/backprop/blob/master/samples/extensible-neural.lhs\n[neural-pdf]:
  https://github.com/mstksg/backprop/blob/master/renders/extensible-neural.pdf\n\nBrief
  example\n-------------\n\n(This is a really brief version of my [blog post][blog])\n\nThe
  quick example below describes the running of a neural network with one\nhidden layer
  to calculate its squared error with respect to target `targ`,\nwhich is parameterized
  by two weight matrices and two bias vectors.\nVector/matrix types are from the *hmatrix*
  package.\n\nLet's make a data type to store our parameters, with convenient accessors
  using\n*[lens][]*:\n\n[lens]: http://hackage.haskell.org/package/lens\n\n```haskell\ndata
  Network i h o = Net { _weight1 :: L h i\n                         , _bias1   ::
  R h\n                         , _weight2 :: L o h\n                         , _bias2
  \  :: R o\n                         }\n\nmakeLenses ''Network\n```\n\nNormally,
  we might write code to \"run\" a neural network on an input like this:\n\n```haskell\nneuralNet\n
  \   :: R i\n    -> Network i h o\n    -> R h\nneuralNet x n = z\n  where\n    y
  = logistic $ (n ^. weight1) #> x + (n ^. bias1)\n    z = logistic $ (n ^. weight2)
  #> y + (n ^. bias2)\n\nlogistic :: Floating a => a -> a\nlogistic x = 1 / (1 + exp
  (-x))\n```\n\n(`R i` is an i-length vector, `L h i` is an h-by-i matrix, etc., `#>`
  is\nmatrix-vector multiplication, and `^.` is access to a field via lens.)\n\nWhen
  given an input vector and the network, we compute the result of the neural\nnetwork
  ran on the input vector.\n\nWe can write it, instead, using *backprop*:\n\n```haskell\nneuralNet\n
  \   :: Reifies s W\n    => BVar s (R i)\n    -> BVar s (Network i h o)\n    -> BVar
  s (R o)\nneuralNet x n = z\n  where\n    y = logistic $ (n ^^. weight1) #> x + (n
  ^^. bias1)\n    z = logistic $ (n ^^. weight2) #> y + (n ^^. bias2)\n\nlogistic
  :: Floating a => a -> a\nlogistic x = 1 / (1 + exp (-x))\n```\n\n(`#>!` is a backprop-aware
  version of `#>`, and `^^.` is access to a field via\nlens in a `BVar`)\n\nAnd that's
  it!  `neuralNet` is now backpropagatable!\n\nWe can \"run\" it using `evalBP`:\n\n```haskell\nevalBP
  (neuralNet (constVar x)) :: Network i h o -> R o\n```\n\nAnd we can find the gradient
  using `gradBP`:\n\n```haskell\ngradBP (neuralNet (constVar x)) :: Network i h o
  -> Network i h o\n```\n\nIf we write a function to compute errors:\n\n```haskell\nnetError\n
  \   :: Reifies s W\n    => BVar s (R i)\n    -> BVar s (R o)\n    -> BVar s (Network
  i h o)\n    -> BVar s Double\nnetError x targ n = norm_2 (neuralNet x - t)\n```\n\n(`norm_2`
  is a backprop-aware euclidean norm)\n\nNow, we can perform gradient descent!\n\n```haskell\ngradDescent\n
  \   :: R i\n    -> R o\n    -> Network i h o\n    -> Network i h o\ngradDescent
  x targ n0 = n0 - 0.1 * gradient\n  where\n    gradient = gradBP (netError (constVar
  x) (constVar targ)) n0\n```\n\nTa dah!  We were able to compute the gradient of
  our error function, just by\nonly saying how to compute *the error itself*.\n\nFor
  a more fleshed out example, see my [blog post][blog] and the [MNIST\ntutorial][mnist-lhs]
  (also [rendered as a pdf][mnist-pdf])\n\nLens Access\n-----------\n\nA lot of the
  friction of dealing with `BVar s a`s instead of `a`s directly is\nalleviated with
  the lens interface.\n\nWith a lens, you can \"view\" and \"set\" items inside a
  `BVar`, as if they were\nthe actual values:\n\n```haskell\n(^.)  ::        a ->
  Lens' a b ->        b\n(^^.) :: BVar s a -> Lens' a b -> BVar s b\n\n(.~)  :: Lens'
  a b ->        b ->        a ->        a\n(.~~) :: Lens' a b -> BVar s b -> BVar
  s a -> BVar s a\n```\n\nAnd you can also extract multiple potential targets, as
  well, using\n`Traversal`s and `Prism`s:\n\n```haskell\n-- | Actually takes a Traversal,
  to be more general.\n-- Can be used to implement \"pattern matching\" on BVars\n(^?)
  \ ::        a -> Prism' a b -> Maybe (       b)\n(^^?) :: BVar s a -> Prism' a b
  -> Maybe (BVar s b)\n\n(^..)  ::        a -> Traversal' a b -> [       b]\n(^^..)
  :: BVar s a -> Traversal' a b -> [BVar s b]\n```\n\nNote that the library itself
  has no *lens* dependency, using *[microlens][]*\ninstead.\n\n[microlens]: http://hackage.haskell.org/package/microlens\n\nBenchmarks\n----------\n\nHere
  are some basic benchmarks comparing the library's automatic\ndifferentiation process
  to \"manual\" differentiation by hand.  When using the\n[MNIST tutorial][bench]
  as an example:\n\n[bench]: https://github.com/mstksg/backprop/blob/master/bench/MNISTBench.hs\n\n![benchmarks](https://i.imgur.com/9DXUaOI.png)\n\n*
  \  For computing the gradient, there is about a 2.5ms overhead (or about 3.5x)\n
  \   compared to computing the gradients by hand.  Some more profiling and\n    investigation
  can be done, since there are two main sources of potential\n    slow-downs:\n\n
  \   1.  \"Inefficient\" gradient computations, because of automated\n        differentiation
  not being as efficient as what you might get from doing\n        things by hand
  and simplifying.  This sort of cost is probably not\n        avoidable.\n    2.
  \ Overhead incurred by the book-keeping and actual automatic\n        differentiating
  system, which involves keeping track of a dependency\n        graph and propagating
  gradients backwards in memory.  This sort of\n        overhead is what we would
  be aiming to reduce.\n\n    It is unclear which one dominates the current slowdown.\n\n*
  \  However, it may be worth noting that this isn't necessarily a significant\n    bottleneck.
  \ *Updating* the networks using *hmatrix* actually dominates the\n    runtime of
  the training.  Manual gradient descent takes 3.2ms, so the extra\n    overhead is
  about 60%-70%.\n\n*   Running the network (and the backprop-aware functions) incurs
  virtually\n    zero overhead (about 4%), meaning that library authors could actually\n
  \   export backprop-aware functions by default and not lose any performance.\n\nTodo\n----\n\n1.
  \ Benchmark against competing back-propagation libraries like *ad*, and\n    auto-differentiating
  tensor libraries like *[grenade][]*\n\n    [grenade]: https://github.com/HuwCampbell/grenade\n\n2.
  \ Write tests!\n\n3.  Explore potentially ditching `Num` for another typeclass that
  only has `+`,\n    `0`, and `1`.  Currently, `Num` is required for all backpropagated
  types,\n    but only `+`, `fromInteger 0`, and `fromInteger 1` are ever used.\n\n
  \   The main upside to using `Num` is that it integrates well with the rest of\n
  \   the Haskell ecosystem, and many things already have useful `Num` instances.\n\n
  \   There are two downsides -- one minor and one major.\n\n    *   It requires more
  work to make a type backpropagatable.  Instead of\n        writing only `+`, `0`
  and `1`, users must also define `*`, `-` or\n        `negate`, `abs`, `signum`,
  and all of `fromInteger`.  However, I don't\n        see this being a big issue
  in practice, since most values that will be\n        used with *backprop* would
  presumably also benefit from having a full\n        `Num` instance even without
  the need to backprop.\n\n    *   Automatically generated prisms (used with `^^?`)
  work with tuples, and\n        so cannot work out-of-the-box without a `Num` instance
  for tuples.  In\n        addition, it's often useful to have anonymous products
  and tuples in\n        general.\n\n        This is bandaided-over by having *backprop*
  provide canonical\n        tuple-with-`Num` types for different libraries to use,
  but it's not a\n        perfect solution.\n\n        This can be resolved by using
  the orphan instances in the\n        *[NumInstances][]* package.  Still, there might
  be some headache for\n        application developers if different libraries using
  *backprop*\n        accidentally pull in their orphan instances from different places.\n\n
  \       [NumInstances]: https://hackage.haskell.org/package/NumInstances\n\n        Alternatively,
  one day we can get `Num` instances for tuples into\n        *base*!\n\n    The extra
  complexity that would come from adding a custom typeclass just\n    for `+` / `0`
  / `1`, though, I feel, might not be worth the benefit.  The\n    entire numeric
  Haskell ecosystem, at the time, revolves around `Num`.\n\n    However, it is worth
  noting that it wouldn't be too hard to add \"Additive\n    Typeclass\" instances
  for any custom types -- one would just need to define\n    `(<+>) = (+)`, `zero
  = fromInteger 0`, and `one = fromInteger 1` (a\n    three-liner), so it might not
  be too bad.\n\n    But really, a lot of this would all resolve itself if we got
  `Num`\n    instances for tuples in base :)\n\n3.  Explore opportunities for parallelization.
  \ There are some naive ways of\n    directly parallelizing right now, but potential
  overhead should be\n    investigated.\n\n4.  Some open questions:\n\n    a. Is it
  possible to support constructors with existential types?\n"
license-name: BSD3
