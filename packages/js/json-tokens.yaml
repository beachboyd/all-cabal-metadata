homepage: https://github.com/andrewthad/json-tokens
changelog-type: markdown
hash: 534f7f5422492bbf179bd939dd82439160e00bcac7f81cb618d3db9d1610327e
test-bench-deps:
  bytestring: -any
  base: ! '>=4.12.0.0 && <5'
  json-tokens: -any
  small-bytearray-builder: -any
  text: ! '>=1.2'
  gauge: -any
  byteslice: -any
  tasty-hunit: ! '>=0.10.0.2 && <0.11'
  tasty: ! '>=1.2.3 && <1.3'
  QuickCheck: ! '>=2.13.1 && <2.14'
  scientific-notation: ! '>=0.1'
  primitive: -any
  vector: -any
  neat-interpolation: ! '>=0.3.2'
maintainer: andrew.thaddeus@gmail.com
synopsis: Tokenize JSON
changelog: |
  # Revision history for json-tokenize

  ## 0.1.0.0 -- 2019-09-27

  * First version. Released on an unsuspecting world.
basic-deps:
  bytestring: ! '>=0.10.8 && <0.11'
  text-short: ! '>=0.1.3 && <0.2'
  base: ! '>=4.12 && <5'
  array-chunks: ! '>=0.1.1 && <0.2'
  byteslice: ! '>=0.1.3 && <0.2'
  array-builder: ! '>=0.1 && <0.2'
  scientific-notation: ! '>=0.1 && <0.2'
  primitive: ! '>=0.7 && <0.8'
  bytesmith: ! '>=0.2 && <0.3'
all-versions:
- 0.1.0.0
author: Andrew Martin
latest: 0.1.0.0
description-type: haddock
description: |-
  Convert JSON to a token stream. This libary focuses on
  high performance and minimal allocations. This library
  is distinguished from `aeson` in the following ways:

  * In `aeson`, `decode` parses JSON by building an AST
  that resembles the ABNF given in RFC 7159. Notably,
  this converts every JSON `object` to a `HashMap`.
  (This choice of intermediate data structure may not
  be appropritae depending on how the user wants to
  interpret the `object`). By constrast, `json-tokens`
  converts a document to a token sequence.

  * For numbers, `aeson` uses `scientific`, but `json-tokens`
  uses `scientific-notation`. Although `scientific` and
  `scientific-notation` have similar APIs, `scientific-notation`
  includes a parser that is about 4x faster and conversion
  functions that are 10x faster than those found in
  `scientific` and `aeson`.

  * For text, `aeson` uses the UTF-16-backed `text` library,
  but `json-tokens` uses the UTF-8-backed `text-short`
  library.

  * Parsing is resumable in `aeson`, which uses `attoparsec`,
  but not in `json-tokens`, which uses `bytesmith`.

  * In `aeson`, all batteries are included. In particular,
  the combination of typeclasses and GHC Generics
  (or Template Haskell) make it possible to elide lots of
  boilerplate. None of these are included in `json-tokens`.

  The difference in design decisions means that solutions using
  `json-tokens` are able to decode JSON twice as fast as
  solutions with `aeson. In the `zeek-json` benchmark suite,
  a `json-tokens`-based decoding outperforms `aeson`'s `decode`
  by a factor of two. This speed comes at a cost. Users must
  write more code to use `json-tokens` than they do for `aeson`.
  If high-throughput parsing of small JSON documents is paramount,
  this cost may be worth bearing. It is always possible to go a
  step further and forego tokenization entirely, parsing the
  desired Haskell data type directly from a byte sequence. Doing this
  in a low-allocation way while retaining both the ability the
  handle JSON `object` keys in any order and the ability to handle
  escape sequences in `object` keys is fiendishly difficult. Kudos
  to the brave soul that goes down that path. For the rest of us,
  `json-tokens` is a compromise worth considering.
license-name: BSD-3-Clause
