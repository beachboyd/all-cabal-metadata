homepage: https://github.com/fimad/scalpel
changelog-type: markdown
hash: a6b100207b79df9b9a644f8638a60199747d9eb370f09a0beb806b0d50327662
test-bench-deps: {}
maintainer: willcoster@gmail.com
synopsis: A high level web scraping library for Haskell.
changelog: ! "# Change Log\n\n## HEAD\n\n## 0.5.1\n\n- Fix bug (#59, #54) in DFS traversal
  order.\n\n## 0.5.0\n\n- Split `scalpel` into two packages: `scalpel` and `scalpel-core`.
  The latter\n  does not provide networking support and does not depend on curl.\n\n##
  0.4.1\n\n- Added `notP` attribute predicate.\n\n## 0.4.0\n\n- Add the `chroot` tricks
  (#23 and #25) to README.md and added examples.\n- Fix backtracking that occurs when
  using `guard` and `chroot`.\n- Fix bug where the same tag may appear in the result
  set multiple times.\n- Performance optimizations when using the (//) operator.\n-
  Make Scraper an instance of MonadFail. Practically this means that failed\n  pattern
  matches in `<-` expressions within a do block will evaluate to mzero\n  instead
  of throwing an error and bringing down the entire script.\n- Pluralized scrapers
  will now return the empty list instead mzero when there\n  are no matches.\n- Add
  the `position` scraper which provides the index of the current sub-tree\n  within
  the context of a `chroots`'s do-block.\n\n## 0.3.1\n\n- Added the `innerHTML` and
  `innerHTMLs` scraper.\n- Added the `match` function which allows for the creation
  of arbitrary\n  attribute predicates.\n- Fixed build breakage with GHC 8.0.1.\n\n##
  0.3.0.1\n\n- Make tag and attribute matching case-insensitive.\n\n## 0.3.0\n\n-
  Added benchmarks and many optimizations.\n- The `select` method is removed from
  the public API.\n- Many methods now have a constraint that the string type parametrizing\n
  \ TagSoup's tag type now must be order-able.\n- Added `scrapeUrlWithConfig` that
  will hopefully put an end to multiplying\n  `scrapeUrlWith*` methods.\n- The default
  behaviour of the `scrapeUrl*` methods is to attempt to infer the\n  character encoding
  from the `Content-Type` header.\n\n## 0.2.1.1\n\n- Cleanup stale instance references
  in documentation of TagName and\n  AttributeName.\n\n## 0.2.1\n\n- Made Scraper
  an instance of MonadPlus.\n\n## 0.2.0.1\n\n- Fixed examples in documentation and
  added an examples folder for ready to\n  compile examples. Added travis tests to
  ensures that examples remain\n  compilable.\n\n## 0.2.0\n\n- Removed the StringLike
  parameter from the Selector, Selectable,\n  AttributePredicate, AttributeName, and
  TagName types. Instead they are now\n  agnostic to the underlying string type, and
  are only constructable with\n  Strings and the Any type.\n\n## 0.1.3.1\n\n- Tighten
  dependencies and drop download-curl all together.\n\n## 0.1.3\n\n- Add the html
  and html scraper primitives for extracting raw HTML.\n\n## 0.1.2\n\n- Make scrapeURL
  follow redirects by default.\n- Expose a new function scrapeURLWithOpts that takes
  a list of curl options.\n- Fix bug (#2) where image tags that do not have a trailing
  \"/\" are not\n  selectable.\n\n## 0.1.1\n\n- Tighten dependencies on download-curl.\n\n##
  0.1.0\n\n- First version!\n"
basic-deps:
  bytestring: -any
  scalpel-core: ==0.5.1
  base: ! '>=4.6 && <5'
  text: -any
  curl: ! '>=1.3.4'
  data-default: -any
  tagsoup: ! '>=0.12.2'
all-versions:
- 0.1.0
- 0.1.1
- 0.1.2
- 0.1.3
- 0.1.3.1
- 0.2.0
- 0.2.0.1
- 0.2.1
- 0.2.1.1
- 0.3.0
- 0.3.0.1
- 0.3.1
- 0.4.0
- 0.4.1
- 0.5.0
- 0.5.1
author: Will Coster
latest: 0.5.1
description-type: markdown
description: ! "Scalpel [![Build Status](https://travis-ci.org/fimad/scalpel.svg?branch=master)](https://travis-ci.org/fimad/scalpel)
  [![Hackage](https://img.shields.io/hackage/v/scalpel.svg)](https://hackage.haskell.org/package/scalpel)\n=======\n\nScalpel
  is a web scraping library inspired by libraries like\n[Parsec](http://hackage.haskell.org/package/parsec-3.1.7/docs/Text-Parsec.html)\nand
  Perl's [Web::Scraper](http://search.cpan.org/~miyagawa/Web-Scraper-0.38/).\nScalpel
  builds on top of [TagSoup](http://hackage.haskell.org/package/tagsoup)\nto provide
  a declarative and monadic interface.\n\nThere are two general mechanisms provided
  by this library that are used to build\nweb scrapers: Selectors and Scrapers.\n\nSelectors\n---------\n\nSelectors
  describe a location within an HTML DOM tree. The simplest selector,\nthat can be
  written is a simple string value. For example, the selector\n`\"div\"` matches every
  single div node in a DOM. Selectors can be combined\nusing tag combinators. The
  `//` operator to define nested relationships within a\nDOM tree. For example, the
  selector `\"div\" // \"a\"` matches all anchor tags\nnested arbitrarily deep within
  a div tag.\n\nIn addition to describing the nested relationships between tags, selectors
  can\nalso include predicates on the attributes of a tag. The `@:` operator creates
  a\nselector that matches a tag based on the name and various conditions on the\ntag's
  attributes. An attribute predicate is just a function that takes an\nattribute and
  returns a boolean indicating if the attribute matches a criteria.\nThere are several
  attribute operators that can be used to generate common\npredicates. The `@=` operator
  creates a predicate that matches the name and\nvalue of an attribute exactly. For
  example, the selector `\"div\" @: [\"id\" @=\n\"article\"]` matches div tags where
  the id attribute is equal to `\"article\"`.\n\nScrapers\n--------\n\nScrapers are
  values that are parameterized over a selector and produce a value\nfrom an HTML
  DOM tree. The `Scraper` type takes two type parameters. The first\nis the string
  like type that is used to store the text values within a DOM tree.\nAny string like
  type supported by `Text.StringLike` is valid. The second type\nis the type of value
  that the scraper produces.\n\nThere are several scraper primitives that take selectors
  and extract content\nfrom the DOM. Each primitive defined by this library comes
  in two variants:\nsingular and plural. The singular variants extract the first instance
  matching\nthe given selector, while the plural variants match every instance.\n\nExample\n-------\n\nComplete
  examples can be found in the\n[examples](https://github.com/fimad/scalpel/tree/master/examples)
  folder in the\nscalpel git repository.\n\nThe following is an example that demonstrates
  most of the features provided by\nthis library. Supposed you have the following
  hypothetical HTML located at\n`\"http://example.com/article.html\"` and you would
  like to extract a list of all\nof the comments.\n\n```html\n<html>\n  <body>\n    <div
  class='comments'>\n      <div class='comment container'>\n        <span class='comment
  author'>Sally</span>\n        <div class='comment text'>Woo hoo!</div>\n      </div>\n
  \     <div class='comment container'>\n        <span class='comment author'>Bill</span>\n
  \       <img class='comment image' src='http://example.com/cat.gif' />\n      </div>\n
  \     <div class='comment container'>\n        <span class='comment author'>Susan</span>\n
  \       <div class='comment text'>WTF!?!</div>\n      </div>\n    </div>\n  </body>\n</html>\n```\n\nThe
  following snippet defines a function, `allComments`, that will download\nthe web
  page, and extract all of the comments into a list:\n\n```haskell\ntype Author =
  String\n\ndata Comment\n    = TextComment Author String\n    | ImageComment Author
  URL\n    deriving (Show, Eq)\n\nallComments :: IO (Maybe [Comment])\nallComments
  = scrapeURL \"http://example.com/article.html\" comments\n   where\n       comments
  :: Scraper String [Comment]\n       comments = chroots (\"div\" @: [hasClass \"container\"])
  comment\n\n       comment :: Scraper String Comment\n       comment = textComment
  <|> imageComment\n\n       textComment :: Scraper String Comment\n       textComment
  = do\n           author      <- text $ \"span\" @: [hasClass \"author\"]\n           commentText
  <- text $ \"div\"  @: [hasClass \"text\"]\n           return $ TextComment author
  commentText\n\n       imageComment :: Scraper String Comment\n       imageComment
  = do\n           author   <- text       $ \"span\" @: [hasClass \"author\"]\n           imageURL
  <- attr \"src\" $ \"img\"  @: [hasClass \"image\"]\n           return $ ImageComment
  author imageURL\n```\n\nTips & Tricks\n-------------\n\nThe primitives provided
  by scalpel are intentionally minimalistic with the\nassumption being that users
  will be able to build up complex functionality by\ncombining them with functions
  that work on existing type classes (Monad,\nApplicative, Alternative, etc.).\n\nThis
  section gives examples of common tricks for building up more complex\nbehavior from
  the simple primitives provided by this library.\n\n### OverloadedStrings\n\n`Selector`,
  `TagName` and `AttributeName` are all `IsString` instances, and\nthus it is convenient
  to use scalpel with `OverloadedStrings` enabled. If not\nusing `OverloadedStrings`,
  all tag names must be wrapped with `tagSelector`.\n\n### Matching Wildcards\n\nScalpel
  has 3 different wildcard values each corresponding to a distinct use case.\n\n-
  `anySelector` is used to match all tags:\n\n    `textOfAllTags = texts anySelector`\n\n-
  `AnyTag` is used when matching all tags with some attribute constraint. For\n  example,
  to match all tags with the attribute `class` equal to `\"button\"`:\n\n    `textOfTagsWithClassButton
  = texts $ AnyTag @: [hasClass \"button\"]`\n\n- `AnyAttribute` is used when matching
  tags with some arbitrary attribute equal\n   to a particular value. For example,
  to match all tags with some attribute\n   equal to `\"button\"`:\n\n    `textOfTagsWithAnAttributeWhoseValueIsButton
  = texts $ AnyTag @: [AnyAttribute @= \"button\"]`\n\n### Complex Predicates\n\nIt
  is possible to run into scenarios where the name and attributes of a tag are\nnot
  sufficient to isolate interesting tags and properties of child tags need to\nbe
  considered.\n\nIn these cases the `guard` function of the `Alternative` type class
  can be\ncombined with `chroot` and `anySelector` to implement predicates of arbitrary\ncomplexity.\n\nBuilding
  off the above example, consider a use case where we would like find the\nhtml contents
  of a comment that mentions the word \"cat\".\n\nThe strategy will be the following:\n\n1.
  Isolate the comment div using `chroot`.\n\n2. Then within the context of that div
  the textual contents can be retrieved\n   with `text anySelector`. This works because
  the first tag within the current context\n   is the div tag selected by chroot,
  and the `anySelector` selector will match the\n   first tag within the current context.\n\n3.
  Then the predicate that `\"cat\"` appear in the text of the comment will be\n   enforced
  using `guard`. If the predicate fails, scalpel will backtrack and\n   continue the
  search for divs until one is found that matches the predicate.\n\n4. Return the
  desired HTML content of the comment div.\n\n```haskell\ncatComment :: Scraper String
  String\ncatComment =\n    -- 1. First narrow the current context to the div containing
  the comment's\n    --    textual content.\n    chroot (\"div\" @: [hasClass \"comment\",
  hasClass \"text\"]) $ do\n        -- 2. anySelector can be used to access the root
  tag of the current context.\n        contents <- text anySelector\n        -- 3.
  Skip comment divs that do not contain \"cat\".\n        guard (\"cat\" `isInfixOf`
  contents)\n        -- 4. Generate the desired value.\n        html anySelector\n```\n\nFor
  the full source of this example, see\n[complex-predicates](https://github.com/fimad/scalpel/tree/master/examples/complex-predicates/)\nin
  the examples directory.\n\n### Generalized Repetition\n\nThe pluralized versions
  of the primitive scrapers (`texts`, `attrs`, `htmls`)\nallow the user to extract
  content from all of the tags matching a given\nselector. For more complex scraping
  tasks it will at times be desirable to be\nable to extract multiple values from
  the same tag.\n\nLike the previous example, the trick here is to use a combination
  of the\n`chroots` function and the `anySelector` selector.\n\nConsider an extension
  to the original example where image comments may contain\nsome alt text and the
  desire is to return a tuple of the alt text and the URLs\nof the images.\n\nThe
  strategy will be the following:\n\n1. to isolate each img tag using `chroots`.\n\n2.
  Then within the context of each img tag, use the `anySelector` selector to extract\n
  \  the alt and src attributes from the current tag.\n\n3. Create and return a tuple
  of the extracted attributes.\n\n```haskell\naltTextAndImages :: Scraper String [(String,
  URL)]\naltTextAndImages =\n    -- 1. First narrow the current context to each img
  tag.\n    chroots \"img\" $ do\n        -- 2. Use anySelector to access all the
  relevant content from the the currently\n        -- selected img tag.\n        altText
  <- attr \"alt\" anySelector\n        srcUrl  <- attr \"src\" anySelector\n        --
  3. Combine the retrieved content into the desired final result.\n        return
  (altText, srcUrl)\n```\n\nFor the full source of this example, see\n[generalized-repetition](https://github.com/fimad/scalpel/tree/master/examples/generalized-repetition/)\nin
  the examples directory.\n\n### scalpel-core\n\nThe `scalpel` package relies on curl
  to provide networking support. For small\nprojects and one off scraping tasks this
  is likely sufficient. However when\nusing scalpel in existing projects or on platforms
  without curl this dependency\ncan be a hindrance.\n\nFor these scenarios users can
  instead depend on\n[scalpel-core](https://hackage.haskell.org/package/scalpel-core)
  which does not\nprovide networking support and does not depend on curl.\n\nTroubleshooting\n---------------\n\n###
  My Scraping Target Doesn't Return The Markup I Expected\n\nSome websites return
  different markup depending on the user agent sent along\nwith the request. In some
  cases, this even means returning no markup at all in\nan effort to prevent scraping.\n\nTo
  work around this, you can add your own user agent string with a curl option.\n\n```haskell\n#!/usr/local/bin/stack\n--
  stack runghc --resolver lts-6.24 --install-ghc --package scalpel-0.4.0\n\nimport
  Network.Curl\nimport Text.HTML.Scalpel\n\nmain = do\n    html <- scrapeURLWithOpts
  opts url $ htmls anySelector\n    maybe printError printHtml html\n  where\n    url
  = \"https://www.google.com\"\n    opts = [ CurlUserAgent \"some user agent string\"
  ]\n    printError = putStrLn \"Failed\"\n    printHtml = mapM_ putStrLn\n```\n\nA
  list of user agent strings can be found\n[here](http://www.useragentstring.com/pages/useragentstring.php).\n\n###
  Building on Windows\n\nBuilding scalpel on Windows can be a challenge because of
  the dependency on\ncurl. In order to successfully build scalpel you must download\n[curl](http://curl.haxx.se/download.html)
  and add the following to your\nstack.yaml file.\n\n```yaml\nextra-lib-dirs: [\"C:/Program
  Files/cURL/dlls\"]\nextra-include-dirs: [\"C:/Program Files/cURL/dlls\"]\n```\n\nIf
  you do not require network support, you can instead depend on\n[scalpel-core](https://hackage.haskell.org/package/scalpel-core)
  which does not\ndoes not depend on curl.\n"
license-name: Apache-2.0
