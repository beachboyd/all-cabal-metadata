homepage: https://bitbucket.org/gchrupala/sequor
changelog-type: ''
hash: aa9bf1050ef1b1188945033da39e6f9f97961049c4037ed9d02166a03ae48f90
test-bench-deps: {}
maintainer: grzegorz.chrupala@gmail.com
synopsis: A sequence labeler based on Collins's sequence perceptron.
changelog: ''
basic-deps:
  nlp-scores: ! '>=0.6.0'
  bytestring: ! '>=0.9.2'
  split: ! '>=0.2'
  base: ! '>=3 && <5'
  text: ! '>=0.10 && <1.0'
  array: ! '>=0.2'
  containers: ! '>=0.2'
  binary: ! '>=0.5'
  mtl: ! '>=1.1'
  pretty: ! '>=1.0'
  vector: ! '>=0.5'
all-versions:
- '0.1'
- '0.2'
- 0.2.2
- 0.2.3
- 0.3.0
- 0.3.1
- 0.3.2
- 0.4.2
- 0.7.0
- 0.7.1
- 0.7.2
- 0.7.5
author: Grzegorz Chrupała
latest: 0.7.5
description-type: text
description: ! "Sequor\n======\n\nSequor is a sequence labeler based on Collins's
  (2002)\nperceptron. Sequor has a flexible feature template language and is\nmeant
  mainly for NLP applications such as Named Entity labeling, Part\nof Speech tagging
  or syntactic chunking. It includes the SemiNER named\nentity recognizer, with pre-trained
  models for German and English (see\n`Named Entity Recognition (SemiNER)`_).\n\nSequor
  is especially useful if your dataset has a large label set. In\nthis case it is
  likely to run faster and allow you to use much less\nRAM than a sequence labeler
  based on Conditional Random\nFields. Additionally sequor implements options which
  allow you to\ncontrol the size of model and tradeoff speed against accuracy:\n\n-
  size of the beam\n- label dictionary\n- feature hashing \n\nSee https://bitbucket.org/gchrupala/sequor/wiki/Options
  for details.\n\nInstallation\n------------\n\nThe easiest way to compile and install
  sequor is to \n\n1. Install the `Haskell platform <http://www.haskell.org/platform/>`_\n2.
  Run::\n\n    cabal update\n    cabal install sequor --prefix=`pwd`\n\nCabal should
  then download and install the necessary packages, and\ninstall the sequor binary
  in ./bin, and the data files in ./share\n\n\n\nUsage\n-----\nWith Sequor you can
  learn a model from sequences manually annotated\nwith labels, and then apply this
  model to new data in order to add\nlabels. Sequor is meant to be used mainly with
  linguistic data, for\nexample to learn Part of Speech tagging, syntactic chunking
  or Named\nEntity labeling::\n \n    Usage: sequor command [OPTION...] [ARG...]\n
  \   train:    train model\n    train [OPTION...] TEMPLATE-FILE TRAIN-FILE MODEL-FILE
  \n      --rate=NUM (0.01)         learning rate\n      --beam=INT (10)           beam
  size\n      --iter=INT (10)           number of iterations\n      --min-count=INT
  (100)     minimum feature frequency for label dictionary\n      --heldout=FILE            path
  to heldout data\n      --hash                    use hashing instead of feature
  dictionary\n      --hash-sample=INT (1000)  sample size to estimate number of features
  when hashing\n      --hash-max-size=INT       maximum size of parameter vector when
  hashing\n\n::\n\nSee https://bitbucket.org/gchrupala/sequor/wiki/Options for more\ndetails
  about the training options.\n\n   predict:  predict using model\n   predict  MODEL-FILE
  \n\n   version:  print version\n   version  \n\n   help:     print usage information\n
  \  help  \n\nData files should be in the UTF-8 encoding.\n\nAs an example we can
  use data annotated with syntactic chunk labels in\nthe data directory. For example::\n\n
  \ ./bin/sequor train data/all.features data/train.conll  model\\\n\t     --rate
  0.1 --beam 10 --iter 5 --hash\\\n             --heldout data/devel.conll\n\n  ./bin/sequor
  predict model < data/test.conll > data/test.labels\n\nFeature template syntax\n-----------------------\n\nSequor
  uses a mini language to specify which features to extract from\ndata. For details
  see https://bitbucket.org/gchrupala/sequor/wiki/Templates\n\n\nNamed Entity Recognition
  (SemiNER)\n----------------------------------\n\nSequor includes the SemiNER named
  entity recognizer, with pre-trained\nmodels for German and English.\n\nThe German
  model recognizer is trained on the CoNLL 2003 data and\nrecognizes the following
  labels:\n\n- PER - people\n- ORG - organizations\n- LOC - locations such as cities
  and countries\n- MISC - miscellaneous entities such as nationalities   \n\nThe German
  model is described in [Chrupala_and_Klakow_2010]_.\n\nThe English model is trained
  on the BBN Wall Street Journal data and\nrecognizes the following labels:\n\n* CARDINAL
  - cardinal number\n* DATE - calendar date\n* GPE:CITY - city\n* GPE:COUNTRY - country\n*
  GPE:STATE_PROVINCE - state or province\n* MONEY - currency\n* NORP:NATIONALITY -
  nationality \n* NORP:OTHER - \n* NORP:POLITICAL - political affiliation \n* ORDINAL
  - ordinal number\n* ORGANIZATION - organization\n* PERCENT - percentage \n* PERSON
  - people\n* QUANTITY - numerical quantity\n\nSee https://bitbucket.org/gchrupala/sequor/wiki/SemiNER
  for usage\ninformation.\n\n\nSequence perceptron\n-------------------\n\nCompared
  to the commonly used Conditional Random Field model, the\nSequence Perceptron algorithm
  is simpler, more efficient and often has\nsimilar performance.\n\nThe sequence perceptron
  was introduced in [Collins_2002]_.\n\n.. [Collins_2002] Collins, Michael. 2002.
  Discriminative training\n   methods for Hidden Markov Models: Theory and experiments
  with\n   perceptron\n   algorithms. EMNLP 2002. http://www.clic.cs.columbia.edu/~mcollins/papers/tagperc.pdf\n\n\n..
  [Chrupala_and_Klakow_2010] Grzegorz Chrupała and Dietrich\n   Klakow. 2010. A Named
  Entity Labeler for German: exploiting\n   Wikipedia and distributional\n   clusters.
  LREC. http://grzegorz.chrupala.me/papers/lrec-2010.pdf"
license-name: BSD-3-Clause
