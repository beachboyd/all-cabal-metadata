homepage: https://github.com/composewell/bench-show
changelog-type: markdown
hash: 2fa4fc497f45bcba3d1fd72848a02c77e0274201c5f0642c94bfcaadaf8c80ef
test-bench-deps:
  bench-show: -any
  split: ==0.2.*
  base: ! '>=4.8 && <5'
  text: ! '>=1.1.1 && <1.3'
maintainer: harendra.kumar@gmail.com
synopsis: Show, plot and compare benchmark results
changelog: ! "## 0.2.1\n\n* Use new version of `statistics` package.\n\n## 0.2.0\n\n###
  Release Notes\n\n* Due to a bug in the `statistics` package, reporting may crash
  on certain\n  inputs with a `vector index out of bounds` message. The bug has been
  fixed\n  and will be available in an upcoming release.\n\n### Breaking Changes\n\n*
  The package `bench-graph` has been renamed to `bench-show` to reflect the\n  fact
  that it now includes text reports as well. This includes the change of\n  module
  name `BenchGraph` to `BenchShow`.\n* The `bgraph` API has been removed and replaced
  by `graph`\n* The way output file is generated has changed. Now field name or group
  name\n  being plotted or both may be suffixed to the output file name automatically.\n
  \ The estimator type (e.g. mean or median) is also suffixed to the filename.\n*
  Changes to `Config` record:\n    * `chartTitle` field has been renamed to `title`.\n
  \   * The type of `outputDir` is now a `Maybe`.\n    * `comparisonStyle` has been
  replaced by `presentation`\n    * `ComparisonStyle` has been replaced by `Presentation`\n
  \   * `sortBenchmarks` has been replaced by `selectBenchmarks`. The new\n      function
  can be defined as follows in terms of an older definition:\n        `selectBenchmarks
  = \\g ->\n            sortBenchmarks $ either error (map fst) $ f (ColumnIndex 0)`\n
  \   * `sortBenchGroups` has been replaced by `selectGroups`\n    * `setYScale` field
  has been broken down into two fields `fieldRanges` and\n      `fieldTicks`. Now
  you also need to specify which fields' scale\n      you want to set.\n\n### Enhancements\n\n*
  A `report` API has been added to generate textual reports\n* More ways to compare
  groups have been added, including percent and percent\n  difference\n* Now we can
  show multiple fields as columns in a single benchmark group report\n* Field units
  are now automatically selected based on the range of values\n* Additions to `Config`
  record type:\n  * `selectFields` added to select the fields to be plotted and to
  change\n    their presentation order.\n  * `selectBenchmarks` can now sort the results
  based on values corresponding to\n    any field or benchmark group.\n  * new fields
  added: `diffStrategy`, `verbose`, `estimator`, `threshold`\n\n## 0.1.4\n\n* Fix
  a bug resulting in a bogus error, something like \"Field [time] found at\n  different
  indexes..\" even though the field has exactly the same index at all\n  places.\n\n##
  0.1.3\n\n* Add maxrss plotting support\n\n## 0.1.2\n\n* Fixed a bug that caused
  missing graphs in some cases when multiple iterations\n  of a benchmark are present
  in the bechmark results file.\n\n* Better error reporting to pinpoint errors when
  a problem occurs.\n\n## 0.1.1\n\n* Support GHC 8.4\n\n## 0.1.0\n\n* Initial release\n"
basic-deps:
  Chart: ! '>=1.6 && <2'
  mwc-random: ! '>=0.13 && <0.15'
  Chart-diagrams: ! '>=1.6 && <2'
  ansi-wl-pprint: ! '>=0.6 && <0.7'
  split: ! '>=0.2 && <0.3'
  base: ! '>=4.8 && <5'
  filepath: ! '>=1.3 && <1.5'
  csv: ! '>=0.1 && <0.2'
  statistics: ! '>=0.15 && <0.16'
  transformers: ! '>=0.4 && <0.6'
  vector: ! '>=0.10 && <0.13'
  directory: ! '>=1.2 && <1.4'
all-versions:
- '0.2.0'
- '0.2.1'
author: Harendra Kumar
latest: '0.2.1'
description-type: markdown
description: ! "# bench-show\n\nGenerate text reports and graphical charts from the
  benchmark results\ngenerated by `gauge` or `criterion`, showing or comparing benchmarks
  in\nmany useful ways. In a few lines of code, we can report time taken, peak\nmemory
  usage, allocations, among many other fields; we can group benchmarks\nand compare
  the groups; we can compare benchmarks before and after a change;\nwe can show absolute
  or percentage difference from the baseline; we can sort\nthe results to get the
  worst affected benchmarks by percentage change.\n\nIt can help us in answering questions
  like the following, visually or\ntextually:\n\n* Across two benchmark runs, show
  all the operations that resulted in a\n  regression of more than 10%, so that we
  can quickly identify and fix\n  performance problems in our application.\n* Across
  two (or more) packages providing similar functionality, show all the\n  operations
  where the performance differs by more than 10%, so that we can\n  critically analyze
  the packages and choose the right one.\n\n## Quick Start\n\nUse `gauge` or `criterion`
  to generate a `results.csv` file, and then use the\nfollowing code to generate a
  textual report or a graph:\n\n```\nreport \"results.csv\"  Nothing defaultConfig\ngraph
  \ \"results.csv\" \"output\" defaultConfig\n```\n\nFor advanced usage, control the
  generated report by modifying the\n`defaultConfig`.\n\n## Reports and Charts\n\n`report`
  with `Fields` presentation style generates a multi-column report.  We\ncan select
  many fields from a `gauge` raw report.  Units of the fields are\nautomatically determined
  based on the range of values:\n\n```haskell\nreport \"results.csv\" Nothing defaultConfig
  { presentation = Fields }\n```\n\n```\nBenchmark     time(μs) maxrss(MiB)\n-------------
  -------- -----------\nvector/fold     641.62        2.75\nstreamly/fold   639.96
  \       2.75\nvector/map      638.89        2.72\nstreamly/map    653.36        2.66\nvector/zip
  \     651.42        2.58\nstreamly/zip    644.33        2.59\n```\n\n`graph` generates
  one bar chart per field:\n\n```\ngraph \"results.csv\" \"output\" defaultConfig\n```\n\nWhen
  the input file contains results from a single benchmark run, by default\nall the
  benchmarks are placed in a single benchmark group named \"default\".\n\n[![Median
  Time Grouped](https://github.com/composewell/bench-show/blob/master/docs/full-median-time.svg)](https://github.com/composewell/bench-show/blob/master/docs/full-median-time.svg)\n\n##
  Grouping\n\nLet's write a benchmark classifier to put the `streamly` and `vector`\nbenchmarks
  in their own groups:\n\n```haskell\n   classifier name =\n       case splitOn \"/\"
  name of\n           grp : bench -> Just (grp, concat bench)\n           _          ->
  Nothing\n```\n\nNow we can show the two benchmark groups as separate columns. We
  can\ngenerate reports comparing different benchmark fields (e.g. `time` and\n`maxrss`)
  for all the groups:\n\n```haskell\n   report \"results.csv\" Nothing\n     defaultConfig
  { classifyBenchmark = classifier }\n```\n\n```\n(time)(Median)\nBenchmark streamly(μs)
  vector(μs)\n--------- ------------ ----------\nfold            639.96     641.62\nmap
  \            653.36     638.89\nzip             644.33     651.42\n```\n\nWe can
  do the same graphically as well, just replace `report` with `graph`\nin the code
  above.  Each group is placed as a cluster on the graph. Multiple\nclusters are placed
  side by side (i.e. on the same scale) for easy\ncomparison. For example:\n\n[![Median
  Time Grouped](https://github.com/composewell/bench-show/blob/master/docs/grouped-median-time.svg)](https://github.com/composewell/bench-show/blob/master/docs/grouped-median-time.svg)\n\n##
  Regression, Percentage Difference and Sorting\n\nWe can append benchmarks results
  from multiple runs to the same file. These\nruns can then be compared. We can run
  benchmarks before and after a change\nand then report the regressions by percentage
  change in a sorted order:\n\nGiven a results file with two runs, this code generates
  the report that\nfollows:\n\n```haskell\n   report \"results.csv\" Nothing\n     defaultConfig\n
  \        { classifyBenchmark = classifier\n         , presentation = Groups PercentDiff\n
  \        , selectBenchmarks = \\f ->\n              reverse\n              $ map
  fst\n              $ sortBy (comparing snd)\n              $ either error id $ f
  $ ColumnIndex 1\n         }\n```\n\n```\n(time)(Median)(Diff using min estimator)\nBenchmark
  streamly(0)(μs)(base) streamly(1)(%)(-base)\n--------- --------------------- ---------------------\nzip
  \                     644.33                +23.28\nmap                      653.36
  \                +7.65\nfold                     639.96                -15.63\n```\n\nIt
  tells us that in the second run the worst affected benchmark is zip\ntaking 23.28
  percent more time compared to the baseline.\n\nGraphically:\n\n[![Median Time Regression](https://github.com/composewell/bench-show/blob/master/docs/regression-percent-descending-median-time.svg)](https://github.com/composewell/bench-show/blob/master/docs/regression-percent-descending-median-time.svg)\n\n##
  Full Documentation and examples\n\n* See the [haddock documentation](http://hackage.haskell.org/package/bench-show)
  on Hackage\n* See the [comprehensive tutorial](http://hackage.haskell.org/package/bench-show)
  module in the haddock docs\n* For examples see the [test directory](https://github.com/composewell/bench-show/tree/master/test)
  in the package\n\n## Contributions and Feedback\n\nContributions are welcome! Please
  see the [TODO.md](TODO.md) file or the\nexisting [issues](https://github.com/composewell/bench-show/issues)
  if you want\nto pick up something to work on.\n\nAny feedback on improvements or
  the direction of the package is welcome. You\ncan always send an email to the\n[maintainer](https://github.com/composewell/bench-show/blob/master/bench-show.cabal)\nor
  [raise an issue](https://github.com/composewell/bench-show/issues/new) for\nanything
  you want to suggest or discuss, or send a PR for any change that you\nwould like
  to make.\n"
license-name: BSD3
