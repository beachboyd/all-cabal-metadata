homepage: http://allmydata.org/source/zfec
changelog-type: ''
hash: 916d302856260e975d778d23b8369a6267bca561e5614852044958caa89954dd
test-bench-deps: {}
maintainer: Adam Langley <agl@imperialviolet.org>
synopsis: Forward error correction of ByteStrings
changelog: ''
basic-deps:
  bytestring: ! '>=0.9'
  base: -any
all-versions:
- '0.1'
- 0.1.1
author: Adam Langley <agl@imperialviolet.org>
latest: 0.1.1
description-type: text
description: ! " * Intro and Licence\n\nThis package implements an \"erasure code\",
  or \"forward error correction code\".\n\nYou may use this package under the GNU
  General Public License, version 2 or, at\nyour option, any later version.  You may
  use this package under the Transitive\nGrace Period Public Licence, version 1.0.
  \ (You may choose to use this package\nunder the terms of either licence, at your
  option.)  See the file COPYING.GPL\nfor the terms of the GNU General Public License,
  version 2.  See the file\nCOPYING.TGPPL.html for the terms of the Transitive Grace
  Period Public Licence,\nversion 1.0.\n\nThe most widely known example of an erasure
  code is the RAID-5 algorithm which\nmakes it so that in the event of the loss of
  any one hard drive, the stored data\ncan be completely recovered.  The algorithm
  in the zfec package has a similar\neffect, but instead of recovering from the loss
  of only a single element, it can\nbe parameterized to choose in advance the number
  of elements whose loss it can\ntolerate.\n\nThis package is largely based on the
  old \"fec\" library by Luigi Rizzo et al.,\nwhich is a mature and optimized implementation
  of erasure coding.  The zfec\npackage makes several changes from the original \"fec\"
  package, including\naddition of the Python API, refactoring of the C API to support
  zero-copy\noperation, a few clean-ups and optimizations of the core code itself,
  and the\naddition of a command-line tool named \"zfec\".\n\n\n * Installation\n\nThis
  package is managed with the \"setuptools\" package management tool.  To build\nand
  install the package directly into your system, just run \"python ./setup.py\ninstall\".
  \ If you prefer to keep the package limited to a specific directory so\nthat you
  can manage it yourself (perhaps by using the \"GNU stow\") tool, then\ngive it these
  arguments: \"python ./setup.py install\n--single-version-externally-managed\n--record=${specificdirectory}/zfec-install.log
  --prefix=${specificdirectory}\"\n\nTo run the self-tests, execute \"python ./setup.py
  test\" (or if you have Twisted\nPython installed, you can run \"trial zfec\" for
  nicer output and test options.)\nThis will run the tests of the C API, the Python
  API, and the command-line\ntools.\n\nTo run the tests of the Haskell API:\n  % runhaskell
  haskell/test/FECTest.hs\n\nNote that you must have installed the library first in
  order for this to work\ndue to the fact that the interpreter cannot process FEC.hs
  as it takes a\nreference to an FFI function.\n\n\n\n * Community\n\nThe source is
  currently available via darcs on the web with the command:\n\ndarcs get http://allmydata.org/source/zfec\n\nMore
  information on darcs is available at http://darcs.net\n\nPlease join the zfec mailing
  list and submit patches:\n\n<http://allmydata.org/cgi-bin/mailman/listinfo/zfec-dev>\n\n\n
  * Overview\n\nThis package performs two operations, encoding and decoding.  Encoding
  takes\nsome input data and expands its size by producing extra \"check blocks\",
  also\ncalled \"secondary blocks\".  Decoding takes some data -- any combination
  of\nblocks of the original data (called \"primary blocks\") and \"secondary blocks\",\nand
  produces the original data.\n\nThe encoding is parameterized by two integers, k
  and m.  m is the total number\nof blocks produced, and k is how many of those blocks
  are necessary to\nreconstruct the original data.  m is required to be at least 1
  and at most 256,\nand k is required to be at least 1 and at most m.\n\n(Note that
  when k == m then there is no point in doing erasure coding -- it\ndegenerates to
  the equivalent of the Unix \"split\" utility which simply splits\nthe input into
  successive segments.  Similarly, when k == 1 it degenerates to\nthe equivalent of
  the unix \"cp\" utility -- each block is a complete copy of the\ninput data.  The
  \"zfec\" command-line tool does not implement these degenerate\ncases.)\n\nNote
  that each \"primary block\" is a segment of the original data, so its size is\n1/k'th
  of the size of original data, and each \"secondary block\" is of the same\nsize,
  so the total space used by all the blocks is m/k times the size of the\noriginal
  data (plus some padding to fill out the last primary block to be the\nsame size
  as all the others).  In addition to the data contained in the blocks\nthemselves
  there are also a few pieces of metadata which are necessary for later\nreconstruction.
  \ Those pieces are: 1.  the value of K, 2.  the value of M, 3.\nthe sharenum of
  each block, 4.  the number of bytes of padding that were used.\nThe \"zfec\" command-line
  tool compresses these pieces of data and prepends them\nto the beginning of each
  share, so each the sharefile produced by the \"zfec\"\ncommand-line tool is between
  one and four bytes larger than the share data\nalone.\n\nThe decoding step requires
  as input k of the blocks which were produced by the\nencoding step.  The decoding
  step produces as output the data that was earlier\ninput to the encoding step.\n\n\n
  * Command-Line Tool\n\nNOTE: the format of the sharefiles was changed in zfec v1.1
  to allow K == 1 and\nK == M.  This change of the format of sharefiles means that
  zfec >= v1.1 cannot\nread sharefiles produced by zfec < v1.1.\n\nThe bin/ directory
  contains two Unix-style, command-line tools \"zfec\" and\n\"zunfec\".  Execute \"zfec
  --help\" or \"zunfec --help\" for usage instructions.\n\nNote: a Unix-style tool
  like \"zfec\" does only one thing -- in this case erasure\ncoding -- and leaves
  other tasks to other tools.  Other Unix-style tools that go\nwell with zfec include
  \"GNU tar\" for archiving multiple files and directories\ninto one file, \"rzip\"
  or \"lrzip\" for compression, and \"GNU Privacy Guard\" for\nencryption or \"sha256sum\"
  for integrity.  It is important to do things in order:\nfirst archive, then compress,
  then either encrypt or sha256sum, then erasure\ncode.  Note that if GNU Privacy
  Guard is used for privacy, then it will also\nensure integrity, so the use of sha256sum
  is unnecessary in that case.\n\n\n * Performance Measurements\n\nOn my Athlon 64
  2.4 GHz workstation (running Linux), the \"zfec\" command-line\ntool encoded a 160
  MB file with m=100, k=94 (about 6% redundancy) in 3.9\nseconds, where the \"par2\"
  tool encoded the file with about 6% redundancy in 27\nseconds.  zfec encoded the
  same file with m=12, k=6 (100% redundancy) in 4.1\nseconds, where par2 encoded it
  with about 100% redundancy in 7 minutes and 56\nseconds.\n\nThe underlying C library
  in benchmark mode encoded from a file at about 4.9\nmillion bytes per second and
  decoded at about 5.8 million bytes per second.\n\nOn Peter's fancy Intel Mac laptop
  (2.16 GHz Core Duo), it encoded from a file at\nabout 6.2 million bytes per second.\n\nOn
  my even fancier Intel Mac laptop (2.33 GHz Core Duo), it encoded from a file\nat
  about 6.8 million bytes per second.\n\nOn my old PowerPC G4 867 MHz Mac laptop,
  it encoded from a file at about 1.3\nmillion bytes per second.\n\n\n * API\n\nEach
  block is associated with \"blocknum\".  The blocknum of each primary block is\nits
  index (starting from zero), so the 0'th block is the first primary block,\nwhich
  is the first few bytes of the file, the 1'st block is the next primary\nblock, which
  is the next few bytes of the file, and so on.  The last primary\nblock has blocknum
  k-1.  The blocknum of each secondary block is an arbitrary\ninteger between k and
  255 inclusive.  (When using the Python API, if you don't\nspecify which blocknums
  you want for your secondary blocks when invoking\nencode(), then it will by default
  provide the blocks with ids from k to m-1\ninclusive.)\n\n ** C API\n\nfec_encode()
  takes as input an array of k pointers, where each pointer points to\na memory buffer
  containing the input data (i.e., the i'th buffer contains the\ni'th primary block).
  \ There is also a second parameter which is an array of the\nblocknums of the secondary
  blocks which are to be produced.  (Each element in\nthat array is required to be
  the blocknum of a secondary block, i.e. it is\nrequired to be >= k and < m.)\n\nThe
  output from fec_encode() is the requested set of secondary blocks which are\nwritten
  into output buffers provided by the caller.\n\nfec_decode() takes as input an array
  of k pointers, where each pointer points to\na buffer containing a block.  There
  is also a separate input parameter which is\nan array of blocknums, indicating the
  blocknum of each of the blocks which is\nbeing passed in.\n\nThe output from fec_decode()
  is the set of primary blocks which were missing\nfrom the input and had to be reconstructed.
  \ These reconstructed blocks are\nwritten into output buffers provided by the caller.\n\n
  ** Python API\n\nencode() and decode() take as input a sequence of k buffers, where
  a \"sequence\"\nis any object that implements the Python sequence protocol (such
  as a list or\ntuple) and a \"buffer\" is any object that implements the Python buffer
  protocol\n(such as a string or array).  The contents that are required to be present
  in\nthese buffers are the same as for the C API.\n\nencode() also takes a list of
  desired blocknums.  Unlike the C API, the Python\nAPI accepts blocknums of primary
  blocks as well as secondary blocks in its list\nof desired blocknums.  encode()
  returns a list of buffer objects which contain\nthe blocks requested.  For each
  requested block which is a primary block, the\nresulting list contains a reference
  to the apppropriate primary block from the\ninput list.  For each requested block
  which is a secondary block, the list\ncontains a newly created string object containing
  that block.\n\ndecode() also takes a list of integers indicating the blocknums of
  the blocks\nbeing passed int.  decode() returns a list of buffer objects which contain
  all\nof the primary blocks of the original data (in order).  For each primary block\nwhich
  was present in the input list, then the result list simply contains a\nreference
  to the object that was passed in the input list.  For each primary\nblock which
  was not present in the input, the result list contains a newly\ncreated string object
  containing that primary block.\n\nBeware of a \"gotcha\" that can result from the
  combination of mutable data and\nthe fact that the Python API returns references
  to inputs when possible.\n\nReturning references to its inputs is efficient since
  it avoids making an\nunnecessary copy of the data, but if the object which was passed
  as input is\nmutable and if that object is mutated after the call to zfec returns,
  then the\nresult from zfec -- which is just a reference to that same object -- will
  also\nbe mutated.  This subtlety is the price you pay for avoiding data copying.
  \ If\nyou don't want to have to worry about this then you can simply use immutable\nobjects
  (e.g. Python strings) to hold the data that you pass to zfec.\n\n ** Haskell API\n\nThe
  Haskell code is fully Haddocked, to generate the documentation, run\n  % runhaskell
  Setup.lhs haddock\n\n\n * Utilities\n\nThe filefec.py module has a utility function
  for efficiently reading a file and\nencoding it piece by piece.  This module is
  used by the \"zfec\" and \"zunfec\"\ncommand-line tools from the bin/ directory.\n\n\n
  * Dependencies\n\nA C compiler is required.  To use the Python API or the command-line
  tools a\nPython interpreter is also required.  We have tested it with Python v2.4
  and\nv2.5.  For the Haskell interface, GHC >= 6.8.1 is required.\n\n\n * Acknowledgements\n\nThanks
  to the author of the original fec lib, Luigi Rizzo, and the folks that\ncontributed
  to it: Phil Karn, Robert Morelos-Zaragoza, Hari Thirumoorthy, and\nDan Rubenstein.
  \ Thanks to the Mnet hackers who wrote an earlier Python wrapper,\nespecially Myers
  Carpenter and Hauke Johannknecht.  Thanks to Brian Warner and\nAmber O'Whielacronx
  for help with the API, documentation, debugging,\ncompression, and unit tests.  Thanks
  to Adam Langley for improving the C API and\ncontributing the Haskell API.  Thanks
  to the creators of GCC (starting with\nRichard M. Stallman) and Valgrind (starting
  with Julian Seward) for a pair of\nexcellent tools.  Thanks to my coworkers at Allmydata
  -- http://allmydata.com --\nFabrice Grinda, Peter Secor, Rob Kinninmont, Brian Warner,
  Zandr Milewski,\nJustin Boreta, Mark Meras for sponsoring this work and releasing
  it under a Free\nSoftware licence.\n\n\nEnjoy!\n\nZooko Wilcox-O'Hearn\n2008-01-20\nBoulder,
  Colorado\n"
license-name: LicenseRef-GPL
