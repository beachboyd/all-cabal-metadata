homepage: ''
changelog-type: markdown
hash: dd4ccabc4ed3e9130b1c400d628f3e1d9b9965f52b461cddf6020fe258aa7701
test-bench-deps: {}
maintainer: Ivan.Miljenovic@gmail.com
synopsis: Create tests and benchmarks together
changelog: ! "0.2.1.2 (23 May, 2018)\n======================\n\n* Allow building with
  criterion-0.13.* and 0.14.*\n\n* Allow building with temporary-1.3.*\n\n0.2.1.1
  (6 February, 2018)\n==========================\n\n* Allow building with GHC 8.2.*\n\n*
  Now requires streaming-0.2.*\n\n0.2.1.0 (13 July, 2017)\n=======================\n\n*
  Add `compareFuncAllWith`\n\n* Documentation improvements (especially on `baselineWith`).\n\n0.2.0.1
  (12 July, 2017)\n=======================\n\n* Accidentally deleted the type signatures
  of `normalForm` and\n  `normalFormIO`, breaking compilation.\n\n0.2.0.0 (12 July,
  2017)\n=======================\n\n* Benchmarking results are now printed out incrementally
  rather than\n  requiring all of them to be completed first.\n\n* Support for using
  the [weigh] library to measure memory usage.\n\n    [weigh]: http://hackage.haskell.org/package/weigh\n\n*
  Can now optionally provide a list of `CompParam` values to\n  `compareFunc` and
  related functions rather than needing to use\n  `mappend` or `<>` to manually combine
  them all.\n\n* Some of Criterion's command-line options are now available,\n  including
  the ability to output a CSV file (albeit with a different\n  format).\n\n* Can now
  evaluate `IO`-based benchmarks.\n\n* Add `compareFuncList`, `compareFuncAll` (as
  well as primed and\n  `IO`-based variants) and `normalForm` (and `normalFormIO`).\n\n*
  Remove `compareFuncConstraint` as it was found to not be very\n  helpful and complicated
  the code base.  The same functionality can\n  be achieved using `compareFuncList`
  and related functions.\n\n    - This includes removing `SameAs` and `CUnion` as
  they are now no\n      longer needed.\n\n    - The type of `compareFunc` is now
  different, but existing code\n      shouldn't needed to change.\n\n* The `LabelTree`
  data structure now includes the depth of each node.\n\n0.1.0.0 (22 May, 2016)\n======================\n\n*
  Initial release\n"
basic-deps:
  streaming: ==0.2.*
  testbench: -any
  bytestring: -any
  base: ==4.*
  streaming-cassava: ==0.1.*
  streaming-with: ! '>=0.1.0.0 && <0.3'
  criterion: ! '>=1.2.1.0 && <1.5'
  process: ! '>=1.1.0.0 && <1.7'
  dlist: ==0.8.*
  HUnit: ! '>=1.1 && <1.7'
  containers: -any
  weigh: ! '>=0.0.4 && <0.1'
  cassava: ==0.5.*
  statistics: ==0.14.*
  transformers: -any
  temporary: ! '>=1.1 && <1.4'
  optparse-applicative: ! '>=0.11.0.0 && <0.15'
  deepseq: ! '>=1.1.0.0 && <1.5'
all-versions:
- '0.1.0.0'
- '0.2.0.0'
- '0.2.0.1'
- '0.2.1.0'
- '0.2.1.1'
- '0.2.1.2'
author: Ivan Lazar Miljenovic
latest: '0.2.1.2'
description-type: markdown
description: ! "testbench\n=========\n\n[![Hackage](https://img.shields.io/hackage/v/testbench.svg)](https://hackage.haskell.org/package/testbench)
  [![Build Status](https://travis-ci.org/ivan-m/testbench.svg)](https://travis-ci.org/ivan-m/testbench)\n\n>
  Test your benchmarks!\n\n> Benchmark your tests!\n\nIt's too easy to accidentally
  try and benchmark apples and oranges\ntogether.  Wouldn't it be nice if you could
  somehow guarantee that\nyour benchmarks satisfy some simple tests (e.g. a group
  of comparisons\nall return the same value)?\n\nFurthermore, trying to compare multiple
  inputs/functions against each\nother requires a lot of boilerplate, making it even
  easier to\naccidentally compare the wrong things (e.g. using `whnf` instead of\n`nf`).\n\n_testbench_
  aims to help solve these problems and more by making it\neasier to write unit tests
  and benchmarks together by stating up-front\nwhat requirements are needed and then
  using simple functions to state\nthe next parameter to be tested/benchmarked.\n\nThis
  uses [HUnit] and [criterion] to create the tests and benchmarks\nrespectively, and
  it's possible to obtain these explicitly to embed\nthem within existing test- or
  benchmark-suites.  Alternatively, you\ncan use the provided `testBench` function
  directly to first run the\ntests and then -- if the tests all succeeded -- run the
  benchmarks.\n\n[HUnit]: https://hackage.haskell.org/package/HUnit\n[criterion]:
  https://hackage.haskell.org/package/criterion\n\nExamples\n--------\n\nPlease see
  the provided `examples/` directory.\n\nLimitations\n-----------\n\n* No availability
  of specifying an environment to run benchmarks in.\n\n* To be able to display the
  tree-like structure more readily for\n  comparisons, the following limitations (currently)
  have to be made:\n\n    - No detailed output, including no reports.  In practice
  however,\n      the detailed outputs produced by _criterion_ don't lend\n      themselves
  well to comparisons.\n\nFortuitously Anticipated Queries\n--------------------------------\n\n###
  Why write this library?\n\nThe idea behind _testbench_ came about because of two
  related\ndissatisfactions with _criterion_ that I found:\n\n1. Even when the `bcompare`
  function was still available, it still\n   seemed very difficult/clumsy to write
  comparison benchmarks since\n   so much needed to be duplicated for each comparison.\n\n2.
  When trying to find examples of benchmarks that performed\n   comparisons between
  different implementations, I came across some\n   that seemingly did the same calculation
  on different\n   inputs/implementations, but upon closer analysis the implementation\n
  \  that \"won\" was actually doing less work than the others (not by a\n   large
  amount, but the difference was non-negligible in my opinion).\n   This would have
  been easy to pick up if even a simple test was\n   performed (e.g. using `==` would
  have led rise to a type mis-match,\n   making it obvious they did different things).\n\n_testbench_
  aims to solve these problems by making it easier to write\ncomparisons up-front:
  by using the `compareFunc` function to specify\nwhat you are benchmarking and how,
  then using `comp` just to specify\nthe input (without needing to also re-specify
  the function,\nevaluationg type, etc.).\n\n### Do I need to know HUnit or criterion
  to be able to use this?\n\nNo, for basic/default usage this library handles all
  that for you.\n\nThere are two overall hints for good benchmarks though:\n\n* Use
  the `NFData` variants (e.g. `normalForm`) where possible: this\n  ensures the calculation
  is actually completed rather than laziness\n  biting you.\n\n* If the variance is
  high, make the benchmark do more work to decrease\n  it.\n\n### Why not use hspec/tasty/some-other-testing-framework?\n\nHopefully
  by the nature of this question it is obvious why I did not\npick one over the others.
  \ HUnit is low-level enough that it can be\nutilised by any of the others if so
  required whilst keeping the\ndependencies required minimal.\n\nNot to mention that
  these tests are more aimed at checking that the\n_benchmarks_ are valid and are
  thus typically equality/predicate-based\ntests on the result from a simple function;
  as such it is more\nintended that they are quickly run as a verification stage rather
  than\nthe basis for a large test-suite.\n\n### Why not use criterion directly for
  running benchmarks?\n\n_criterion_ currently does not lend itself well to visualising
  the\nresults from comparison-style benchmarks:\n\n* A very limited internal tree-like
  structure which is not really\n  apparent when results are displayed.\n\n* No easy
  way to actually _compare_ benchmark values: there used to be\n  a `bcompare` function
  but it hasn't been available since version\n  1.0.0.0 came out in August 2014.  As
  such, comparisons must be done\n  by hand by comparing the results visually.\n\n*
  Having more than a few benchmarks together produces a lot of output\n  (either to
  the terminal or a resulting report): combined with the\n  previous point, having
  more than a few benchmarks is discouraged.\n\nNote that if however you wish to use
  _criterion_ more directly (either\nfor configurability or to be able to have reports),
  a combination of\n`getTestBenches` and `flattenBenchForest` will provide you with
  a\n`Benchmark` value that is accepted by _criterion_.\n"
license-name: MIT
