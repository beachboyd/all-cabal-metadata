homepage: http://github.com/composewell/streaming-benchmarks
changelog-type: markdown
hash: cf0097337778062c12a6319231fb4c8978e6782fbfbbb4cba1356127aeac5a29
test-bench-deps:
  streaming: ! '>=0.1.4 && <0.3'
  base: ==4.*
  conduit: ! '>=1.3 && <1.4'
  gauge: ! '>=0.2.3 && <0.3'
  drinkery: ! '>=0.3 && <0.4'
  pipes: ! '>=4 && <4.4'
  mtl: ! '>=2 && <2.3'
  transformers: ! '>=0.4 && <0.6'
  random: ! '>=1.0 && <2.0'
  deepseq: ! '>=1.4.0 && <1.5'
  machines: ! '>=0.6.0 && <0.7'
  template-haskell: ! '>=2.10 && <2.14'
  streamly: ! '>=0.2.1 && <0.5'
  vector: ! '>=0.12 && <0.13'
maintainer: Harendra Kumar
synopsis: Benchmarks to compare streaming packages
changelog: ! "## 0.2.0\n\n* Added benchmarks for pure lists\n* Added benchmarks for
  pure `vector`\n* Added benchmarks for `vector` monadic streaming library\n* Added
  `drinkery` streaming library\n* The code is modular now, package specific ops for
  each benchmarked package\n  are contained in a separate own module. It is much easier
  to add a new\n  package now.\n* The benchmarking code now works for `IO` as well
  as `Identity` monad.\n* Used the same stream generation method for all libraries
  for a fair\n  comparison.\n* Use a monadic API (`unfoldrM`) for generating the stream.\n*
  conduit-1.3.0 has a performance issue with `mapM_`. Avoided using `mapM_` and\n
  \ used `sinkNull` instead. See https://github.com/snoyberg/conduit/issues/363.\n
  \ This workaround improves the performance of all conduit benchmarks that drain\n
  \ the stream.\n* pipes also had an issue similar to that of conduit. The code was
  using\n  `mapM_` which was very inefficient, used `discard` instead and got a\n
  \ significant boost in numbers.\n\n## 0.1.0\n\n* Initial release\n"
basic-deps:
  Chart: ! '>=1.6 && <2'
  bytestring: ! '>=0.9 && <0.11'
  Chart-diagrams: ! '>=1.6 && <2'
  split: ! '>=0.2 && <0.3'
  base: ==4.*
  text: ! '>=1.1.1 && <1.3'
  bench-graph: ! '>=0.1 && <0.2'
  csv: ! '>=0.1 && <0.2'
  typed-process: ! '>=0.1.0.0 && <0.3'
  getopt-generics: ! '>=0.11 && <0.14'
  transformers: ! '>=0.4 && <0.6'
  directory: ! '>=1.2 && <1.4'
all-versions:
- 0.1.0
- 0.2.0
author: Harendra Kumar
latest: 0.2.0
description-type: text
description: ! "Streaming Benchmarks\n====================\n\n.. image:: https://badges.gitter.im/composewell/gitter.svg?\n
  \ :target: https://gitter.im/composewell/streamly\n  :alt: Gitter chat\n\n.. image::
  https://img.shields.io/hackage/v/streaming-benchmarks.svg?style=flat\n  :target:
  https://hackage.haskell.org/package/streaming-benchmarks\n  :alt: Hackage\n\n..
  image:: https://travis-ci.org/composewell/streaming-benchmarks.svg?branch=master\n
  \ :target: https://travis-ci.org/composewell/streaming-benchmarks\n  :alt: Unix
  Build Status\n\n.. image:: https://ci.appveyor.com/api/projects/status/8d1kgrrw9mmxv5xt?svg=true\n
  \ :target: https://ci.appveyor.com/project/harendra-kumar/streaming-benchmarks\n
  \ :alt: Windows Build status\n\n.. contents:: Table of Contents\n   :depth: 1\n\nThis
  package compares `streamly <https://github.com/composewell/streamly>`_, a\nblazing
  fast streaming library providing native high level, declarative and\ncomposable
  concurrency support, with popular streaming libraries e.g. vector,\nstreaming, pipes
  and conduit.  This package has been motivated by `streamly\n<https://github.com/composewell/streamly>`_,
  however, it is general purpose and\ncompares more libraries and benchmarks than
  shown here. Please send an email or\na pull request if the benchmarking code has
  a problem or is unfair to some\nlibrary in any way.\n\nBenchmarks & Results\n--------------------\n\nA
  stream of one million consecutive numbers is generated using monadic unfold\nAPI
  ``unfoldrM``, these elements are then processed using a streaming\ncombinator under
  test (e.g. ``map``). The total time to process all one million\noperations, and
  the maximum resident set size (rss) is measured and plotted for\neach library. The
  underlying monad for each stream is the IO Monad. All the\nlibraries are compiled
  with GHC-8.4.3. All the benchmarks were run on an Apple\nMacBook Pro computer with
  a single 2.2 GHz Intel Core i7 processor with 4 cores\nand 16GB RAM.\n\nHighlights\n~~~~~~~~~~\n\n*
  ``streamly`` shows the best overall performance in terms of time as well as\n  space.
  ``streamly`` and ``vector`` show similar performance except\n  for the ``append``
  operation where ``streamly`` is much better, and the\n  ``filter`` operation where
  vector is faster.\n* The ``append`` operation scales well only for ``streamly``
  and ``conduit``.\n  All other libraries show quadratic complexity on this operation.\n*
  ``streaming`` performs slightly better than ``conduit`` when multiple\n  operations
  are composed together even though in terms of individual\n  operations it is slightly
  worse than ``conduit``.\n* ``conduit`` and ``pipes`` show unusually large space
  utilization for\n  ``take`` and ``drop`` operations (more than 100-150 MiB vs 3
  MiB).\n* ``drinkery`` shows very good performance too though not plotted here because\n
  \ of a small issue in measurement and lack of space.\n* ``machines`` is roughly
  2x slower than the slowest library here, and its\n  maximum resident set size is
  close to 100 MiB for all operations (touching\n  300 MiB for ``take``) compared
  to the 3MiB for all other libraries.  I am not\n  sure if there is something wrong
  with the measurements or the benchmarking\n  code, majority of the code is common
  to all libraries, any improvements in\n  the machines benchmarking code are welcome.\n\nKey
  Operations\n~~~~~~~~~~~~~~\n\nThe following diagram plots the time taken by key
  streaming operations to\nprocess a million stream elements.\n*Note: the time for
  streamly and vector is very low (600-700 microseconds) and\ntherefore can barely
  be seen in this graph.*\n\n.. |keyoperations-time| image:: charts-0/KeyOperations-time.svg\n
  \ :width: 75%\n  :target: charts-0/KeyOperations-time.svg\n  :alt: Time Cost of
  Key Streaming Operations\n\n|keyoperations-time|\n\nFor those interested in the
  heap allocations, the following diagram\nplots the overall heap allocations during
  each measurement period i.e. the\ntotal allocations for processing one million stream
  elements.\n\n.. |keyoperations-allocated| image:: charts-0/KeyOperations-allocated.svg\n
  \ :width: 75%\n  :target: charts-0/KeyOperations-allocated.svg\n  :alt: Heap allocations
  for Key Streaming Operations\n\n|keyoperations-allocated|\n\nThe following diagram
  plots the maximum resident set size (rss) during the\nmeasurement of each operation.
  In plain terms, it is the maximum amount of\nphysical memory that is utilized at
  any point during the measurement.\n\n.. |keyoperations-maxrss| image:: charts-0/KeyOperations-maxrss.svg\n
  \ :width: 75 %\n  :target: charts-0/KeyOperations-maxrss.svg\n  :alt: Maximum rss
  for Key Streaming Operations\n\n|keyoperations-maxrss|\n\n+------------------------+----------------------------------------------------+\n|
  Benchmark              | Description                                        |\n+========================+====================================================+\n|
  drain                  | Just discards all the elements in the stream       |\n+------------------------+----------------------------------------------------+\n|
  drop-all               | drops all element using the ``drop`` operation     |\n+------------------------+----------------------------------------------------+\n|
  last                   | extract the last element of the stream             |\n+------------------------+----------------------------------------------------+\n|
  fold                   | sum all the numbers in the stream                  |\n+------------------------+----------------------------------------------------+\n|
  map                    | increments each number in the stream by 1          |\n+------------------------+----------------------------------------------------+\n|
  take-all               | Use ``take`` to retain all the elements in the     |\n|
  \                       | stream                                             |\n+------------------------+----------------------------------------------------+\n|
  filter-even            | Use ``filter`` to keep even numbers and discard    |\n|
  \                       | odd numbers in the stream.                         |\n+------------------------+----------------------------------------------------+\n|
  scan                   | scans the stream using ``+`` operation             |\n+------------------------+----------------------------------------------------+\n|
  mapM                   | transform the stream using a monadic action        |\n+------------------------+----------------------------------------------------+\n|
  zip                    | combines corresponding elements of the two streams |\n|
  \                       | together                                           |\n+------------------------+----------------------------------------------------+\n\nAppend
  Operation\n~~~~~~~~~~~~~~~~\n\nA million streams of single elements are created
  and appended together to\ncreate a stream of million elements. The total time taken
  in this operation is\nmeasured. *Note that vector, streaming and pipes show a quadratic\ncomplexity
  (O(n^2)) on this benchmark and do not finish in a reasonable time*.\nThe time shown
  in the graph for these libraries is just\nindicative, the actual time taken is much
  higher.\n\n.. |append| image:: charts-0/AppendOperation-time.svg\n  :width: 60 %\n
  \ :target: charts-0/AppendOperation-time.svg\n  :alt: Cost of appending a million
  streams of single elements\n\n|append|\n\ntoList Operation\n~~~~~~~~~~~~~~~~\n\nA
  stream of a million elements is generated using ``unfoldrM`` and then\nconverted
  to a list.\n\n.. |toList| image:: charts-0/toListOperation-time.svg\n  :width: 60
  %\n  :target: charts-0/toListOperation-time.svg\n  :alt: Cost of converting a stream
  of million elements to a list\n\n|toList|\n\nComposing Multiple Operations\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA
  stream operation or a combination of stream operations are performed four\ntimes
  in a row to measure how the composition scales for each library. A\nmillion elements
  are passed through this composition.\n\n*Note: the time for streamly and vector
  is very low (600-700 microseconds) and\ntherefore can barely be seen in this graph.*\n\n..
  |composed| image:: charts-0/ComposedOperations%3A4times-time.svg\n  :width: 60 %\n
  \ :target: charts-0/ComposedOperations%3A4times-time.svg\n  :alt: Cost when operations
  are composed\n\n|composed|\n\n+------------------------+----------------------------------------------------+\n|
  Benchmark              | Description                                        |\n+========================+====================================================+\n|
  mapM                   | ``mapM`` four times in a row                       |\n+------------------------+----------------------------------------------------+\n|
  all-in-filters         | four filters in a row,                             |\n|
  \                       | each allowing all elements in                      |\n+------------------------+----------------------------------------------------+\n|
  map-with-all-in-filter | ``map`` followed by ``filter`` composed four times |\n|
  \                       | serially                                           |\n+------------------------+----------------------------------------------------+\n\nHow
  to Run\n----------\n\nTo quickly compare packages:\n\n::\n\n  # Chart all the default
  packages\n  ./run.sh --quick\n\n  # Compare a given list of packages\n  # Available
  package names are: streamly, vector, streaming, pipes,\n  # conduit, machines, drinkery,
  list, pure-vector\n  ./run.sh --quick --select \"streamly,vector\"\n\n  # Show full
  results for the first packages and delta from that for\n  # the rest of the packages.\n
  \ ./run.sh --quick --select \"streamly,vector\" --delta\n\nAfter running you can
  find the charts generated in the ``charts`` directory.\nIf you have the patience
  to wait longer for the results remove the ``--quick``\noption, the results are likely
  to be a tiny bit more accurate.\n\nThe ``list`` package above is the standard haskell
  lists in the base package,\nand ``pure-vector`` is the vector package using pure
  API instead of the monadic\nAPI.\n\nPedantic Mode\n~~~~~~~~~~~~~\n\nNote that if
  different optimization flags are used on different packages,\nperformance can sometimes
  badly suffer because of GHC inlining and\nspecialization not working optimally.
  \ If you  want to be absolutely sure that\nall packages and dependencies are compiled
  with the same optimization flags\n(``-O2``) use ``run.sh --pedantic``, it will install
  the stack snapshot in a\nprivate directory under the current directory and build
  them fresh with the ghc\nflags specified in ``stack-pedantic.yaml``. Be aware that
  this will require 1-2\nGB extra disk space.\n\nAdding New Libraries\n~~~~~~~~~~~~~~~~~~~~\n\nIt
  is trivial to add a new package. This is how `a\nbenchmark file\n<https://github.com/composewell/streaming-benchmarks/blob/master/Benchmarks/Streamly.hs>`_\nfor
  a streaming package looks like. Pull requests are welcome, I will be happy\nto help,
  `just join the gitter chat\n<https://github.com/composewell/streaming-benchmarks/blob/master/Benchmarks/Streamly.hs>`_\nand
  ask!\n\nBenchmarking Notes\n------------------\n\nBenchmarking is a tricky business.
  Though the benchmarks have been carefully\ndesigned there may still be issues with
  the way benchmarking is being done or\nthe way they have been coded. If you find
  that something is being measured\nunfairly or incorrectly please bring it to our
  notice by raising an issue or\nsending an email or via gitter chat.\n\nMeasurement\n~~~~~~~~~~~\n\n``Benchmarking
  Tool:`` We use the `gauge\n<https://github.com/vincenthz/hs-gauge>`_ package for
  measurements instead of\ncriterion.  There were several issues with criterion that
  we fixed in gauge to\nget correct results. Each benchmark is run in a separate process
  to avoid any\ninteraction between benchmarks.\n\nBenchmarking Code\n~~~~~~~~~~~~~~~~~\n\n*
  ``IO Monad:`` We run the benchmarks in the IO monad so that they are close to\n
  \ real life usage. Note that most existing streaming benchmarks use pure code\n
  \ or Identity monad which may produce entirely different results.\n\n* ``unfoldrM``
  is used to generate the stream for two reasons, (1) it is\n  monadic, (2) it reduces
  the generation overhead so that the actual streaming\n  operation cost is amplified.
  If we use generation from a list there is a\n  significant overhead in the generation
  itself because of the intermediate\n  list structure.\n\n* Unless we perform some
  real IO operation, the operation being benchmarked can\n  get completely optimized
  out in some cases. We use a random number generation\n  in the IO monad and feed
  it to the operation being benchmarked to avoid that\n  issue.\n\nGHC Inlining\n------------\n\n*
  ``Inlining:`` GHC simplifier is very fragile and inlining may affect the\n  results
  in unpredictable ways unless you have spent enough time scrutinizing\n  and optimizing
  everything carefully.  Inlining is the biggest source of\n  fragility in performance
  benchmarking. It can easily result in an order of\n  magnitude drop in performance
  just because some operation is not correctly\n  inlined. Note that this applies
  very well to the benchmarking code as well.\n\n* ``GHC Optimization Flags:`` To
  make sure we are comparing fairly we make sure\n  that we compile the benchmarking
  code, the library code as well as all\n  dependencies using exactly the same GHC
  flags. GHC inlining and\n  specialization optimizations can make the code unpredictable
  if mixed flags\n  are used. See the ``--pedantic`` option of the ``run.sh`` script.\n\n*
  ``Single file vs multiple files`` The best way to avoid issues is to have all\n
  \ the benchmarking code in a single file. However, in real life that is not the\n
  \ case and we also needed some modularity to scale the benchmarks to arbitrary\n
  \ number of libraries so we split it into per package file. As soon as the code\n
  \ was split into multiple files, performance of some libraries dropped, in some\n
  \ cases by 3-4x.  Careful sprinkling of INLINE pragmas was required to bring it\n
  \ back to original. Even functions that seemed just 2 lines of code were not\n  automatically
  inlined.\n\n* When all the code was in a single file, not a single INLINE pragma
  was\n  needed. But when split in multiple files even functions that were not\n  exported
  from that file needed an INLINE pragma for equivalent performance.\n  This is something
  that GHC may have to look at.\n\n* The effect of inlining varied depending on the
  library.  To make sure that we\n  are using the fully optimized combination of inline
  or non-inline for each\n  library we carefully studied the impact of inlining individual
  operations for\n  each package. The current code is the best we could get for each
  package.\n\n* There is something magical about streamly, not sure what it is. Even
  though\n  all other libraries were impacted significantly for many ops, streamly
  seemed\n  almost unaffected by splitting the benchmarking ops into a separate file!
  If\n  we can find out why is it so, we could perhaps understand and use GHC\n  inlining
  in a more predictable manner. Edit - CPS seems to be more immune to\n  inlining,
  as soon as streamly started using direct style, it too became\n  sensitive to inlining.\n\n*
  This kind of unpredictable non-uniform impact of moving functions in\n  different
  files shows that we are at the mercy of the GHC simplifier and\n  always need to
  tune performance carefully after refactoring, to be sure that\n  everything is fine.
  In other words, benchmarking and optimizing is crucial\n  not just for the libraries
  `but for the users of the libraries as well`.\n\nStreaming Libraries\n-------------------\n\nThere
  are two dual paradigms for stream processing in Haskell. In the first\nparadigm
  we represent a stream as a data type and use functions to work on it.\nIn the second
  paradigm we represent *stream processors* as data types and\nprovide them individual
  data elements to process, there is no explicit\nrepresentation of the stream as
  a data type. In the first paradigm we work with\ndata representation and in the
  second paradigm we work with function\nrepresentations. Both of these paradigms
  have equal expressive power. The\nlatter uses the monadic composition for data flow
  whereas the former does not\nneed monadic composition for straight line stream processing
  and therefore can\nuse it for higher level composition e.g.  to compose streams
  in a product\nstyle.\n\nTo see an example of the first paradigm, let us use the
  ``vector`` package to\nrepresent a monadic stream of integers as ``Stream IO Int``.
  This data\nrepresentation of stream is passed explicitly to the stream processing\nfunctions
  like ``filter`` and ``drop`` to manipulate it::\n\n  import qualified Data.Vector.Fusion.Stream.Monadic
  as S\n\n  stream :: S.Stream IO Int\n  stream = S.fromList [1..100]\n\n  main =
  \ do\n    let str = (S.filter even . S.drop 10) stream\n    toList str >>= putStrLn
  . show\n\nPure lists and vectors are the most basic examples of streams in this
  paradigm.\nThe streaming IO libraries just extend the same paradigm to monadic streaming.\nThe
  API of these libraries is very much similar to lists with a monad parameter\nadded.\n\nThe
  second paradigm is direct opposite of the first one, there is no stream\nrepresentation
  in this paradigm, instead we represent *stream processors* as\ndata types. A stream
  processor represents a particular process rather than\ndata, and we compose them
  together to create composite processors. We can call\nthem stream transducers or
  simply pipes. Using the ``machines`` package::\n\n  import qualified Data.Machine
  as S\n\n  producer :: S.SourceT IO Int\n  producer = S.enumerateFromTo 1 100\n\n
  \ main =  do\n    let processor = producer S.~> S.dropping 10 S.~> S.filtered even\n
  \   S.runT processor >>= putStrLn . show\n\nBoth of these paradigms look almost
  the same, right? To see the difference\nlet's take a look at some types. In the
  first paradigm we have an explicit\nstream type and the processing functions take
  the stream as input and produce\nthe transformed stream::\n\n  stream :: S.Stream
  IO Int\n  filter :: Monad m => (a -> Bool) -> Stream m a -> Stream m a\n\nIn the
  second paradigm, there is no stream data type, there are stream\nprocessors, let's
  call them boxes that represent a process.  We have a\n*SourceT* box that represents
  a singled ended producer and a *Process* box or a\npipe that has two ends, an input
  end and an output end, a ``MachineT``\nrepresents any kind of box. We put these
  boxes together using the ``~>``\noperator and then run the resulting machine using
  ``runT``::\n\n  producer :: S.SourceT IO Int\n  filtered :: (a -> Bool) -> Process
  a a\n  dropping :: Int -> Process a a\n  (~>) :: Monad m => MachineT m k b -> ProcessT
  m b c -> MachineT m k c\n\nCustom pipes can be created using a Monadic composition
  and primitives to\nreceive and send data usually called ``await`` and ``yield``.\n\n..
  |str| replace:: `streamly <https://github.com/composewell/streamly>`__\n\n+-----------------------------------------------------------------------------+\n|
  Streaming libraries using the direct paradigm.                              |\n+------------------------+----------------------------------------------------+\n|
  Library                | Remarks                                            |\n+========================+====================================================+\n|
  vector                 | The simplest in this category, provides            |\n|
  \                       | transformation and combining of monadic            |\n|
  \                       | streams but no monadic composition of streams.     |\n|
  \                       | Provides a very simple list like API.              |\n+------------------------+----------------------------------------------------+\n|
  streaming              | * Encodes a return value to be supplied when the   |\n|
  \                       |   stream ends. The monad instance passes on the    |\n|
  \                       |   streams and combines the return values.          |\n|
  \                       | * Functor general                                  |\n|
  \                       | * The API is more complicated than vector because  |\n|
  \                       |   of the return value and the functor layer.       |\n+------------------------+----------------------------------------------------+\n|
  list-t                 | Provides straight line composition of streams      |\n|
  \                       | as well as a list like monadic composition.        |\n|
  \                       | The API is simple, just like ``vector``.           |\n+------------------------+----------------------------------------------------+\n|
  \                       | Like list-t, in addition to straight line          |\n|
  \                       | composition it provides a list like monadic        |\n|
  \                       | composition of streams, supports combining streams |\n|
  \                       | concurrently supports concurrent applicative and   |\n|
  \                       | monadic composition.                               |\n|
  |str|                  | The basic API is very much like lists and          |\n|
  \                       | almost identical to ``vector`` streams.            |\n+------------------------+----------------------------------------------------+\n\n+-----------------------------------------------------------------------------+\n|
  Streaming libraries using the pipes paradigm.                               |\n+------------------------+----------------------------------------------------+\n|
  Library                | Remarks                                            |\n+========================+====================================================+\n|
  conduit                | ``await`` and ``yield`` data to upstream or        |\n|
  \                       | downstream pipes; supports pushing leftovers back. |\n+------------------------+----------------------------------------------------+\n|
  pipes                  | ``await`` and ``yield`` data to upstream or        |\n|
  \                       | downstream pipes                                   |\n+------------------------+----------------------------------------------------+\n|
  machines               | Can await from two sources, left and right.        |\n+------------------------+----------------------------------------------------+\n\n"
license-name: MIT
