homepage: http://github.com/composewell/streaming-benchmarks
changelog-type: ''
hash: 66412112f25ccd1aee9f91f42a8322a9e1781ef7dbd8039fc3b86d3937da6a8a
test-bench-deps:
  list-transformer: ! '>=1.0.2 && <1.1'
  streaming: ! '>=0.1.4 && <0.3'
  logict: ! '>=0.5.0 && <0.7'
  base: ==4.*
  list-t: ! '>=0.4.6 && <1.1'
  conduit: ! '>=1.3 && <1.4'
  gauge: ! '>=0.2.1 && <0.3'
  pipes: ! '>=4 && <4.4'
  mtl: ! '>=2 && <2.3'
  transformers: ! '>=0.4 && <0.6'
  random: ! '>=1.0 && <2.0'
  deepseq: ! '>=1.4.0 && <1.5'
  machines: ! '>=0.6.0 && <0.7'
  streamly: ! '>=0.1.1 && <0.2'
  vector: ! '>=0.12 && <0.13'
maintainer: Harendra Kumar
synopsis: Benchmarks to compare streaming packages
changelog: ''
basic-deps:
  Chart: ! '>=1.6 && <1.9'
  bytestring: ! '>=0.9 && <0.11'
  Chart-diagrams: ! '>=1.6 && <1.9'
  split: ! '>=0.2 && <0.3'
  base: ==4.*
  text: ! '>=1.1.1 && <1.3'
  csv: ! '>=0.1 && <0.2'
  typed-process: ! '>=0.1.0.0 && <0.3'
  directory: ! '>=1.2 && <1.4'
all-versions:
- '0.1.0'
author: Harendra Kumar
latest: '0.1.0'
description-type: text
description: ! "Streaming Benchmarks\n--------------------\n\nComprehensive, carefully
  crafted benchmarks for streaming operations and their\ncomparisons across notable
  Haskell streaming libraries including `streaming`,\n`machines`, `pipes`, `conduit`
  and `streamly`. `Streamly\n<https://github.com/composewell/streamly>`_ is a brand
  new streaming library\nwith beautiful high level and composable concurrency built-in,
  it is the\nprimary motivation for these benchmarks. We go to great lengths to make
  sure\nthat the benchmarks are correct, fair and reproducible. Please report if you\nfind
  something that is not right.\n\nBenchmarks & Results\n--------------------\n\nIn
  all the benchmarks we work on a stream of a million consecutive numbers. We\nstart
  the sequence using a random number between 1 and 1000 and enumerate it to\nmake
  a total of a million elements using the streaming library's native\nsequence enumeration
  API. Note that the efficiency of this sequence generation\nmay affect all performance
  numbers of the library because this is a constant\ncost involved in all the benchmarks.\n\nNote
  that, these benchmarks show results for conduit-1.3.0 which is a recently\nreleased
  major version, it perhaps requires some work to get at par with the\nearlier version
  i.e.\nconduit-1.2.13.1 `which showed significantly better performance\n<https://github.com/composewell/streaming-benchmarks/blob/269ac94fc59c76267b89b07690d9ea290096b95b/charts/AllOperationsataGlance.svg>`_\ncompared
  to the newer version.\n\nWhen choosing a streaming library to use we should not
  be over obsessed about\nthe performance numbers as long as the performance is within
  reasonable bounds.\nWhether the absolute performance or the differential among various
  libraries matters\nor not may depend on your workload. If the cost of processing
  the data is\nsignificantly higher then the streaming operations' overhead will just
  pale in\ncomparison and may not matter at all. Unless you are performing huge number
  of\ntiny operations, performance difference may not be significant.\n\nComposing
  Pipeline Stages\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThese benchmarks compare the performance
  when multiple operations are composed\nserially in a pipeline. This is how the streaming
  libraries are supposed to be\nused in real applications.\n\nThe `mapM` benchmark
  introduces four stages of `mapM` between the source and\nthe sink.\n\n`all-in-filters`
  composes four stages of a `filter` operation that passes all\nthe items through.
  \ Note that passing or blocking nature of the filter may\nimpact the results. Some
  libraries can do blocking more optimally by short\ncircuiting.\n\n`all-out-filters`
  composes four stages of a `filter` operation that `blocks`\nall the items i.e. does
  not let anything pass through.\n\nThe `map-with-all-in-filter` benchmark introduces
  four identical stages between\nthe source and the sink where each stage performs
  a simple `map` operation\nfollowed by a `filter` operation that passes all the items
  through.\n\n.. image:: charts/Composing Pipeline Stages.svg\n  :alt: Composing Pipeline
  Stages\n\nIndividual Operations\n~~~~~~~~~~~~~~~~~~~~~\n\nThis chart shows microbenchmarks
  for all individual streaming operations for a\nquick comparison. Operations are
  ordered more or less by increasing cost for\nbetter visualization. If an operation
  is not present in a library then an empty\nspace is displayed instead of a colored
  bar in its slot. See the following\nsections for details about what the benchmarks
  do.\n\n.. image:: charts/All Operations at a Glance.svg\n  :alt: All Operations
  at a Glance\n\nDiscarding and Folding\n^^^^^^^^^^^^^^^^^^^^^^\n\nThis chart shows
  the cheapest of all operations, they include operations that\niterate over the stream
  and either discard all the elements or fold them to a\nsingle value. They all do
  similar stuff and are generally expected to have\nsimilar cost.  Benchmarks include:\n\n*
  `toNull:` Just discards all the elements in the stream.\n* `drop-all`: drops ``n``
  elements from the stream where ``n`` is set to the\n  length of the stream.\n* `last`:
  drops all the elements except the last one.\n* `fold`: adds all the elements in
  the stream to produces the sum.\n\n.. image:: charts/Discarding and Folding.svg\n
  \ :alt: Discarding and Folding\n\nPure Transformation and Filtering\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis
  is the next category which is a bit costlier than the previous one. Unlike\nprevious
  category these operations inspect the elements in the stream and\nform a transformed
  stream based on a function on the value. Benchmarks include:\n\n* `filter-all-out`:
  A filter that discards all the elements in the stream.\n* `filter-all-in`: A filter
  that retains all the elements in the stream.\n* `take-all`: take `n` elements from
  the stream where `n` is set to the length\n  of the stream. Effectively iterates
  through the stream and retains all of it.\n* `takeWhile-true`: retains all elements
  of the stream using a condition that\n  always wvaluates to true.\n* `map`: A pure
  transformation that increments each element by 1.\n* `filter-even`: A filter that
  passes even elements in the stream i.e. half the\n  elements are kept and the other
  half discarded.\n* `scan`: scans the stream using ``+`` operation.\n\n.. image::
  charts/Pure Transformation and Filtering.svg\n  :alt: Pure Transformation and Filtering\n\nMonadic
  Transformation\n^^^^^^^^^^^^^^^^^^^^^^\n\nThis benchmark compares the monadic transformation
  of the stream using\n``mapM``.\n\n.. image:: charts/Monadic Transformation.svg\n
  \ :alt: Monadic Transformation\n\nFolding to List\n^^^^^^^^^^^^^^^\n\nThis benchmark
  compares folding the stream to a list.\n\n.. image:: charts/Folding to List.svg\n
  \ :alt: Folding to List\n\nZip and Concat\n^^^^^^^^^^^^^^\n\nZip combines corresponding
  elements of the two streams together. Concat turns a\nstream of containers into
  a stream of their elements.\n\n.. image:: charts/Zipping and Concating Streams.svg\n
  \ :alt: Zipping and Concating Streams\n\nStudying the Scaling of Composition\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis
  category of benchmarks studies the effect of adding more stages in a\ncomposition
  pipeline. For each library it displays the results when 1, 2, 3 or\n4 pipeline stages
  are used. There are no graphs you can see the results in the\nbenchmark output.\n\nHow
  to Run\n----------\n\n::\n\n  ./run.sh\n\nAfter running you can find the charts
  generated in the ``charts`` directory. If\nyou are impatient use ``./run.sh --quick``
  and you will get the results much\nsooner though a tiny bit less precise. Note that
  quick mode won't generate the\ngraphs unless the latest ``gauge`` is used from github
  repo.\n\nNote that if different optimization flags are used on different packages,\nperformance
  can sometimes badly suffer because of GHC inlining and\nspecialization not working
  optimally.  If you  want to be aboslutely sure that\nall packages and dependencies
  are compiled with the same optimization flags\n(``-O2``) use ``run.sh --pedantic``,
  it will install the stack snapshot in a\nprivate directory under the current directory
  and build them fresh with the ghc\nflags specified in ``stack-pedantic.yaml``. Be
  aware that this will require 1-2\nGB extra disk space.\n\nImportant Points about
  Benchmarking Methodology\n-----------------------------------------------\n\n``IO
  Monad:`` We run the benchmarks in the IO monad so that they are close to\nreal life
  usage. Note that most existing streaming benchmarks use pure code or\nIdentity monad
  which may produce entirely different results.\n\n``Benchmarking Tool:`` We use the
  `gauge\n<https://github.com/vincenthz/hs-gauge>`_ package instead of criterion.
  \ We\nspent a lot of time figuring out why benchmarking was not producing accurate\nresults.
  Criterion had several bugs due to which results were not reliable. We\nfixed those
  bugs in ``gauge``. For example due to GC or CAF evaluation\ninteraction across benchmarks,
  the results of benchmarks running later in the\nsequence were sometimes totally
  off the mark. We fixed that by running each\nbenchmark in a separate process in
  gauge. Another bug caused criterion to\nreport wrong mean.\n\n``Iterations:`` We
  pass a million elements through the streaming pipelines. We\ndo not rely on the
  benchmarking tool for this, it is explicitly done by the\nbenchmarking code and
  the benchmarking tool is asked to perform just one\niteration. We added fine grained
  control in `gauge\n<https://github.com/vincenthz/hs-gauge>`_ to be able to do this.\n\n``Effects
  of Optimizations:`` In some cases fusion or other optimizations can\njust optimize
  out everything and produce ridiculously low results. To avoid\nthat we generate
  random numbers in the IO monad and pass those through the\npipeline rather than
  using some constant or predictable source.\n\n``GHC Optimization Flags:`` To make
  sure we are comparing fairly we make sure\nthat we compile the benchmarking code,
  the library code as well as all\ndependencies using exactly the same GHC flags.
  GHC inlining and specialization\noptimizations can make the code unpredictable if
  mixed flags are used. See the\n``--pedantic`` option of the ``run.sh`` script.\n\n``Benchmark
  Categories:`` We have two categories of benchmarks, one to measure\nthe performance
  of individual operations in isolation and the other to measure\nthe performance
  when multiple similar or different operations are composed\ntogether in a pipeline.\n\nBenchmarking
  Errors\n-------------------\n\nBenchmarking is a tricky business. Though the benchmarks
  have been carefully\ndesigned there may still be issues with the way benchmarking
  is being done or\nthe way they have been coded. If you find that something is being
  measured\nunfairly or incorrectly please bring it to our notice by raising an issue
  or\nsending an email.\n"
license-name: MIT
