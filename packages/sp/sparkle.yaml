homepage: http://github.com/tweag/sparkle#readme
changelog-type: markdown
hash: 64857056f014a7018177c6697dbfd11a8eba68d58451967b727e1a7baa4e29df
test-bench-deps: {}
maintainer: m@tweag.io
synopsis: Distributed Apache Spark applications in Haskell
changelog: ! "# Change Log\n\nAll notable changes to this project will be documented
  in this file.\n\nThe format is based on [Keep a Changelog](http://keepachangelog.com/).\n\n##
  [Unreleased]\n\n### Added\n\n### Changed\n\n## [0.7.4] - 2018-02-28\n\n### Changed\n\n*
  Fixed dynamic linking of sparkle when library dependencies don't set\n  the `RPATH`
  to `$ORIGIN`. PR #139\n* Linker flags `-rpath` and `-z origin` are no longer necessary.\n*
  Build with `jvm-batching`.\n\n## [0.7.3] - 2018-02-06\n\n### Added\n\n* More `RDD`
  method bindings: `randomSplit`, `mean`,\n  `zipWithUniqueId`, `reduceByKey`, `subtractByKey`.\n\n###
  Changed\n\n* Use inline-java for PairRDD bindings under the hood.\n* Updated sparkle
  to build with distributed-closure-0.4.0.\n\n## [0.7.2] - 2017-12-25\n\n### Added\n\n*
  More `RDD` method bindings: `sortBy`.\n\n## [0.7.1] - 2017-12-13\n\n### Fixed\n\n*
  Use StaticPointers for `PairRDD` as a workaround for [GHC bug\n  #14204][ghc-14204]
  occuring when mapping over a PairRDD (see [issue\n  #119][issue-119])\n\n## [0.7]
  - 2017-12-09\n\n## [0.6] - 2017-07-16\n\n### Added\n\n* Support shipping anonymous
  objects that appear in inline-java\n  quasiquotes. You'll need to configure your
  app to use the Kryo\n  serializer for this to work. See FAQ in README. This fixes
  #104.\n\n### Changed\n\n* Move `parallelize`, `textFile` and `binaryRecords` to
  `Context`\n  module.\n* Functions such as `sample` now use the [choice][hackage-choice]\n
  \ library to describe the semantics of boolean arguments in their\n  types.\n* Use
  inline-java for RDD bindings under the hood.\n\n## [0.5] - 2017-02-21\n\n### Added\n\n*
  Bind to expm1\n* Add bindings to dayofmonth, current_timestamp and current_date.\n*
  Add support for the dataframe condition expressions\n* Add bindings to withColumnRenamed,
  columns, printSchema, Column.expr.\n* Bind DataFrame distinct.\n* Add bindings for
  log and log1p for Columns.\n* Add binding to Column.cast.\n* Add bindings getList
  and array for columns.\n* Add bindings: schema for rows, Metadata type, javaRDD,
  range, Row\n  getters and constructors, StrucType constructors, createDataFrame,\n
  \ more DataType bindings.\n\n### Changed\n\n* Prevent Haskell exceptions from escaping
  apply.\n* Update sparkle to work with latest jni which uses ForeignPtr\n  for java
  references.\n* Move StructType and friends to modules StructField, DataType and
  Metadata.\n* Rename createRow, rowGet, rowSize, joinPairRDD to have the same names\n
  \ as the java methods.\n\n## [0.4]\n\n### Added\n\n* Support for reading/writing
  Parquet files.\n* More `RDD` method bindings: `repartition`, `treeAggregate`,\n
  \ `binaryRecords`, `aggregateByKey`, `mapPartitions`, `mapPartitionsWithIndex`.\n*
  More complete `DataFrame` support.\n* Intero support.\n* `stack ghci` support.\n*
  Support Template Haskell splices and `ANN` annotations that use\n  sparkle code.\n\n###
  Changed\n\n### Fixed\n\n* More reliable initialization of embedded shared library.\n*
  Cleanup temporary files properly.\n\n## [0.3] - 2016-12-27\n\n### Added\n\n* Dockerfile
  to build sparkle.\n* Compatibility with singletons-2.2.\n* Add the identity `Reify`/`Reflect`
  instances.\n* Change JNI bindings to use new `JNI.String` type, instead of\n  `ByteString`.
  This new type guarantees the invariants required by\n  the JNI API (null-termination
  in particular).\n\n### Changed\n\n* Remove `Reify`/`Reflect` instances for `Int`.
  Only instances for\n  sized types remain.\n\n### Fixed\n\n* Fix type in `Reify Int`
  making it incorrect.\n\n## [0.2.0] - 2016-12-13\n\n### Added\n\n* New binding: `getOrCreateSQLContext`.\n\n###
  Changed\n\n* `getOrCreate` renamed to `getOrCreateSparkContext`.\n\n## [0.1.0.1]
  - 2016-06-12\n\n### Added\n\n* More bindings to more `call*Method` JNI functions.\n\n###
  Changed\n\n* Use `getOrCreate` to get `SparkContext`.\n\n## [0.1.0] - 2016-04-25\n\n*
  Initial release\n\n[ghc-14204]: https://ghc.haskell.org/trac/ghc/ticket/14204\n[hackage-choice]:
  https://hackage.haskell.org/package/choice\n[issue-119]: https://github.com/tweag/sparkle/issues/119\n"
basic-deps:
  streaming: ! '>=0.1'
  zip-archive: ! '>=0.2'
  bytestring: ! '>=0.10'
  unix: -any
  base: ! '>=4.10.1.0 && <5'
  jni: ! '>=0.5.0 && <0.7'
  text: ! '>=1.2'
  filepath: ! '>=1.4'
  process: ! '>=1.2'
  constraints: -any
  jvm: ! '>=0.4.0.1 && <0.5'
  singletons: ! '>=2.0'
  distributed-closure: ! '>=0.3'
  binary: ! '>=0.7'
  regex-tdfa: ! '>=1.2'
  sparkle: -any
  jvm-streaming: ! '>=0.2'
  inline-java: ! '>=0.7.0 && <0.9'
  temporary: -any
  choice: ! '>=0.1'
  vector: ! '>=0.11'
all-versions:
- '0.1'
- '0.1.0.1'
- '0.2'
- '0.3'
- '0.4'
- '0.4.0.1'
- '0.4.0.2'
- '0.5.0.1'
- '0.6'
- '0.7'
- '0.7.1'
- '0.7.2'
- '0.7.2.1'
- '0.7.3'
- '0.7.4'
author: Tweag I/O
latest: '0.7.4'
description-type: markdown
description: ! "# sparkle: Apache Spark applications in Haskell\n\n[![CircleCI](https://circleci.com/gh/tweag/sparkle.svg?style=svg)](https://circleci.com/gh/tweag/sparkle)\n\n*sparkle
  [spär′kəl]:* a library for writing resilient analytics\napplications in Haskell
  that scale to thousands of nodes, using\n[Spark][spark] and the rest of the Apache
  ecosystem under the hood.\nSee [this blog post][hello-sparkle] for the details.\n\n**This
  is an early tech preview, not production ready.**\n\n[spark]: http://spark.apache.org/\n[hello-sparkle]:
  http://www.tweag.io/posts/2016-02-25-hello-sparkle.html\n\n## Getting started\n\nThe
  tl;dr using the `hello` app as an example on your local machine:\n\n```\n$ stack
  build hello\n$ stack exec -- sparkle package sparkle-example-hello\n$ stack exec
  -- spark-submit --master 'local[1]' --packages com.amazonaws:aws-java-sdk:1.11.253,org.apache.hadoop:hadoop-aws:2.7.2,com.google.guava:guava:23.0
  sparkle-example-hello.jar\n```\n\n## How to use\n\nTo run a Spark application the
  process is as follows:\n\n1. **create** an application in the `apps/` folder, in-repo
  or as\n   a submodule;\n1. **add** your app to `stack.yaml`;\n1. **build** the app;\n1.
  **package** your app into a deployable JAR container;\n1. **submit** it to a local
  or cluster deployment of Spark.\n\n**If you run into issues, read the Troubleshooting
  section below\n  first.**\n\n### Build\n\n#### Linux\n\n**Requirements**\n\n* the
  [Stack][stack] build tool (version 1.2 or above);\n* either, the [Nix][nix] package
  manager,\n* or, OpenJDK, Gradle and Spark (version 1.6) installed from your distro.\n\nTo
  build:\n\n```\n$ stack build\n```\n\nYou can optionally get Stack to download Spark
  and Gradle in a local\nsandbox (using [Nix][nix]) for good build results reproducibility.\n**This
  is the recommended way to build sparkle.** Alternatively,\nyou'll need these installed
  through your OS distribution's package\nmanager for the next steps (and you'll need
  to tell Stack how to find\nthe JVM header files and shared libraries).\n\nTo use
  Nix, set the following in your `~/.stack/config.yaml` (or pass\n`--nix` to all Stack
  commands, see the [Stack manual][stack-nix] for\nmore):\n\n```yaml\nnix:\n  enable:
  true\n```\n\n#### Other platforms\n\nsparkle is not directly supported on non-Linux
  operating systems (e.g.\nMac OS X or Windows). But you can use Docker to run sparkle
  natively\ninside a container on those platforms. First,\n\n```\n$ stack docker pull\n```\n\nThen,
  just add `--docker` as an argument to *all* Stack commands, e.g.\n\n```\n$ stack
  --docker build\n```\n\nBy default, Stack uses the [tweag/sparkle][docker-build-img]
  build and\ntest Docker image, which includes everything that Nix does as in the\nLinux
  section. See the [Stack manual][stack-docker] for how to modify\nthe Docker settings.\n\n###
  Package\n\nTo package your app as a JAR directly consumable by Spark:\n\n```\n$
  stack exec -- sparkle package <app-executable-name>\n```\n\n### Submit\n\nFinally,
  to run your application, for example locally:\n\n```\n$ stack exec -- spark-submit
  --master 'local[1]' <app-executable-name>.jar\n```\n\nThe `<app-executable-name>`
  is any executable name as given in the\n`.cabal` file for your app. See apps in
  the [apps/](apps/) folder for\nexamples.\n\nSee [here][spark-submit] for other options,
  including launching\na [whole cluster from scratch on EC2][spark-ec2]. This\n[blog
  post][tweag-blog-haskell-paas] shows you how to get started on\nthe [Databricks
  hosted platform][databricks] and on\n[Amazon's Elastic MapReduce][aws-emr].\n\n[docker-build-img]:
  https://hub.docker.com/r/tweag/sparkle/\n[stack]: https://github.com/commercialhaskell/stack\n[stack-docker]:
  https://docs.haskellstack.org/en/stable/docker_integration/\n[stack-nix]: https://docs.haskellstack.org/en/stable/nix_integration/#configuration\n[spark-submit]:
  http://spark.apache.org/docs/1.6.2/submitting-applications.html\n[spark-ec2]: http://spark.apache.org/docs/1.6.2/ec2-scripts.html\n[nix]:
  http://nixos.org/nix\n[tweag-blog-haskell-paas]: http://www.tweag.io/posts/2016-06-20-haskell-compute-paas-with-sparkle.html\n[databricks]:
  https://databricks.com/\n[aws-emr]: https://aws.amazon.com/emr/\n\n## How it works\n\nsparkle
  is a tool for creating self-contained Spark applications in\nHaskell. Spark applications
  are typically distributed as JAR files, so\nthat's what sparkle creates. We embed
  Haskell native object code as\ncompiled by GHC in these JAR files, along with any
  shared library\nrequired by this object code to run. Spark dynamically loads this\nobject
  code into its address space at runtime and interacts with it\nvia the Java Native
  Interface (JNI).\n\n## Troubleshooting\n\n### `jvm` library or header files not
  found\n\nYou'll need to tell Stack where to find your local JVM installation.\nSomething
  like the following in your `~/.stack/config.yaml` should do\nthe trick, but check
  that the paths match up what's on your system:\n\n```\nextra-include-dirs: [/usr/lib/jvm/java-7-openjdk-amd64/include]\nextra-lib-dirs:
  [/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/amd64/server]\n```\n\nOr use `--nix`:
  since it won't use your globally installed JDK, it\nwill have no trouble finding
  its own locally installed one.\n\n### Can't build sparkle on OS X\n\nOS X is not
  a supported platform for now. There are several issues to\nmake sparkle work on
  OS X, tracked\n[in this ticket](https://github.com/tweag/sparkle/issues/12).\n\n###
  Gradle <= 2.12 incompatible with JDK 9\n\nIf you're using JDK 9, note that you'll
  need to either downgrade to\nJDK 8 or update your Gradle version, since Gradle versions
  up to and\nincluding 2.12 are not compatible with JDK 9.\n\n### Anonymous classes
  in inline-java quasiquotes fail to deserialize\n\nWhen using inline-java, it is
  recommended to use the Kryo serializer,\nwhich is currently not the default in Spark
  but is faster anyways. If\nyou don't use the Kryo serializer, objects of anonymous
  class, which\narise e.g. when using Java 8 function literals,\n\n```haskell\nfoo
  :: RDD Int -> IO (RDD Bool)\nfoo rdd = [java| $rdd.map((Integer x) -> x.equals(0))
  |]\n```\n\nwon't be deserialized properly in multi-node setups. To avoid this\nproblem,
  switch to the Kryo serializer by setting the following\nconfiguration properties
  in your `SparkConf`:\n\n```haskell\ndo conf <- newSparkConf \"some spark app\"\n
  \  confSet conf \"spark.serializer\" \"org.apache.spark.serializer.KryoSerializer\"\n
  \  confSet conf \"spark.kryo.registrator\" \"io.tweag.sparkle.kryo.InlineJavaRegistrator\"\n```\n\nSee
  [#104](https://github.com/tweag/sparkle/issues/104) for more\ndetails.\n\n### java.lang.UnsatisfiedLinkError:
  /tmp/sparkle-app...: failed to map segment from shared object\n\nSparkle unzips
  the Haskell binary program in a temporary location on\nthe filesystem and then loads
  it from there. For loading to succeed, the\ntemporary location must not be mounted
  with the `noexec` option.\nAlternatively, the temporary location can be changed
  with\n```\nspark-submit --driver-java-options=\"-Djava.io.tmpdir=...\" \\\n             --conf
  \"spark.executor.extraJavaOptions=-Djava.io.tmpdir=...\"\n```\n\n### java.io.IOException:
  No FileSystem for scheme: s3n\n\nSpark 2.2 requires explicitly specifying extra
  JAR files to `spark-submit`\nin order to work with AWS. To work around this, add
  an additional 'packages'\nargument when submitting the job:\n\n```\nspark-submit
  --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.2,com.google.guava:guava:12.0\n```\n\n##
  License\n\nCopyright (c) 2015-2016 EURL Tweag.\n\nAll rights reserved.\n\nsparkle
  is free software, and may be redistributed under the terms\nspecified in the [LICENSE](LICENSE)
  file.\n\n## Sponsors\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n[![Tweag
  I/O](http://i.imgur.com/0HK8X4y.png)](http://tweag.io)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n[![LeapYear](http://i.imgur.com/t9VxRHn.png)](http://leapyear.io)\n\nsparkle
  is maintained by [Tweag I/O](http://tweag.io/).\n\nHave questions? Need help? Tweet
  at\n[@tweagio](http://twitter.com/tweagio).\n"
license-name: BSD3
